{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# üß† LLM + GNN (with Edge Type Awareness)\n",
    "# ======================================\n",
    "\n",
    "# üì¶ Install Dependencies\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install torch-geometric --quiet\n",
    "!pip install transformers --quiet\n",
    "\n",
    "# =======================\n",
    "# üìö Import Libraries\n",
    "# =======================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import RGCNConv\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# =======================\n",
    "# üì• Load Cora Dataset\n",
    "# =======================\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "print(f\"Nodes: {data.num_nodes}, Edges: {data.num_edges // 2}, Classes: {dataset.num_classes}\")\n",
    "\n",
    "# =======================\n",
    "# üè∑Ô∏è Simulate Edge Types\n",
    "# =======================\n",
    "# Let's simulate edge types (e.g., 0 = 'cites', 1 = 'extends', 2 = 'contradicts')\n",
    "import random\n",
    "num_edge_types = 3\n",
    "edge_type = torch.tensor([random.randint(0, num_edge_types - 1) for _ in range(data.edge_index.size(1))])\n",
    "\n",
    "# =======================\n",
    "# ü§ñ Encode Node Texts with BERT\n",
    "# =======================\n",
    "texts = [f\"paper about topic {int(label)}\" for label in data.y.tolist()]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert.eval()\n",
    "\n",
    "def embed_texts(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=32)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "bert_embeds = embed_texts(texts)\n",
    "bert_embeds = torch.tensor(normalize(bert_embeds), dtype=torch.float)\n",
    "\n",
    "# =======================\n",
    "# üîß Define R-GCN Model\n",
    "# =======================\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_relations):\n",
    "        super().__init__()\n",
    "        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations)\n",
    "        self.conv2 = RGCNConv(hidden_channels, out_channels, num_relations)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_type))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return x\n",
    "\n",
    "# =======================\n",
    "# üèãÔ∏è Train R-GCN\n",
    "# =======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RGCN(in_channels=bert_embeds.size(1), hidden_channels=64, out_channels=dataset.num_classes, num_relations=num_edge_types).to(device)\n",
    "data = data.to(device)\n",
    "bert_embeds = bert_embeds.to(device)\n",
    "edge_type = edge_type.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(bert_embeds, data.edge_index, edge_type)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# =======================\n",
    "# üéØ Evaluate\n",
    "# =======================\n",
    "model.eval()\n",
    "pred = model(bert_embeds, data.edge_index, edge_type).argmax(dim=1)\n",
    "correct = int((pred[data.test_mask] == data.y[data.test_mask]).sum())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# =======================\n",
    "# üß† Simulate LLM-Inferred Node\n",
    "# =======================\n",
    "llm_text = \"Paper on transformer architectures applied to citation graphs\"\n",
    "inputs = tokenizer(llm_text, return_tensors='pt', truncation=True, padding=True)\n",
    "with torch.no_grad():\n",
    "    llm_embed = bert(**inputs).last_hidden_state[:, 0, :]\n",
    "llm_embed = F.normalize(llm_embed, p=2, dim=1).to(device)\n",
    "\n",
    "# =======================\n",
    "# üîé Link Prediction via Similarity\n",
    "# =======================\n",
    "with torch.no_grad():\n",
    "    out_embed = model(bert_embeds, data.edge_index, edge_type)\n",
    "    out_embed = F.normalize(out_embed, p=2, dim=1)\n",
    "    sim = torch.matmul(out_embed, llm_embed.T).squeeze()\n",
    "    topk = sim.topk(5).indices\n",
    "\n",
    "# =======================\n",
    "# üîó Add Node and Edges\n",
    "# =======================\n",
    "extended_x = torch.cat([bert_embeds, llm_embed], dim=0)\n",
    "new_node_idx = extended_x.size(0) - 1\n",
    "new_edges = torch.stack([\n",
    "    torch.full((5,), new_node_idx), topk\n",
    "], dim=0)\n",
    "rev_edges = torch.stack([\n",
    "    topk, torch.full((5,), new_node_idx)\n",
    "], dim=0)\n",
    "\n",
    "extended_edge_index = torch.cat([data.edge_index, new_edges, rev_edges], dim=1)\n",
    "new_edge_types = torch.tensor([0]*5 + [0]*5).to(device)  # assume 'cites'\n",
    "extended_edge_type = torch.cat([edge_type, new_edge_types], dim=0)\n",
    "\n",
    "# =======================\n",
    "# üï∏Ô∏è Visualize Subgraph\n",
    "# =======================\n",
    "import networkx as nx\n",
    "sub_nodes = topk.tolist() + [new_node_idx]\n",
    "src, tgt = extended_edge_index\n",
    "mask = [(int(s) in sub_nodes and int(t) in sub_nodes) for s, t in zip(src, tgt)]\n",
    "edge_sub = extended_edge_index[:, mask]\n",
    "\n",
    "G = nx.Graph()\n",
    "for i in sub_nodes:\n",
    "    G.add_node(i, label=\"LLM\" if i == new_node_idx else f\"Node {i}\")\n",
    "for i in range(edge_sub.size(1)):\n",
    "    u, v = int(edge_sub[0, i]), int(edge_sub[1, i])\n",
    "    G.add_edge(u, v)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "nx.draw(G, pos, with_labels=True, labels=nx.get_node_attributes(G, 'label'),\n",
    "        node_color=[\"red\" if n == new_node_idx else \"skyblue\" for n in G.nodes()], node_size=700)\n",
    "plt.title(\"Subgraph Around LLM-Generated Node\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
