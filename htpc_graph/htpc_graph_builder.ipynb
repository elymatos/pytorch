{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b436818-062e-40fc-8ee9-cbf8007011e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "\n",
    "import traceback\n",
    "\n",
    "class HTCPGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds a hierarchical AND-OR graph model for HTPC with 5 levels:\n",
    "    - L1: Token transitions with OR nodes for alternative paths\n",
    "    - L2: Bigram memory (AND nodes)\n",
    "    - L3: Phrase memory (AND nodes)\n",
    "    - L4: Phrase hierarchy (AND nodes)\n",
    "    - L5: Discourse patterns (AND nodes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, debug=False):\n",
    "        # Debug mode flag\n",
    "        self.debug = debug\n",
    "        # Initialize the graph structure\n",
    "        self.graph = {\n",
    "            \"nodes\": {},  # All nodes with their attributes\n",
    "            \"edges\": [],  # All edges with their attributes\n",
    "            \"metadata\": {\n",
    "                \"token_count\": 0,\n",
    "                \"or_count\": 0,\n",
    "                \"and_count\": 0,\n",
    "                \"levels\": {\n",
    "                    \"L1\": 0,  # Token level\n",
    "                    \"L2\": 0,  # Bigram level\n",
    "                    \"L3\": 0,  # Phrase level\n",
    "                    \"L4\": 0,  # Hierarchy level\n",
    "                    \"L5\": 0   # Discourse level\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # Thresholds for each level\n",
    "        self.thresholds = {\n",
    "            \"bigram\": 2,      # Minimum frequency to create a bigram\n",
    "            \"phrase\": 2,      # Minimum frequency to create a phrase\n",
    "            \"hierarchy\": 2,   # Minimum frequency to create a hierarchy\n",
    "            \"discourse\": 2    # Minimum frequency to create a discourse pattern\n",
    "        }\n",
    "        # Tracking counters\n",
    "        self.token_frequencies = Counter()\n",
    "        self.bigram_frequencies = Counter()\n",
    "        self.phrase_frequencies = Counter()\n",
    "        self.hierarchy_frequencies = Counter()\n",
    "        # Temporary storage for sequence processing\n",
    "        self.sequence_buffer = []\n",
    "        # Tracking node predecessors and successors for OR node creation\n",
    "        self.predecessors = defaultdict(set)  # nodes that come before a given node\n",
    "        self.successors = defaultdict(set)    # nodes that come after a given node\n",
    "        # Node counter for unique IDs\n",
    "        self.next_node_id = 0\n",
    "        # NetworkX graph for visualization\n",
    "        self.nx_graph = nx.DiGraph()\n",
    "        \n",
    "    def _debug(self, message):\n",
    "        \"\"\"Print debug message if debug mode is enabled\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"DEBUG: {message}\")\n",
    "        \n",
    "    def _create_node(self, node_type, level, value, components=None, frequency=1):\n",
    "        \"\"\"Create a new node in the graph with a unique ID\"\"\"\n",
    "        node_id = f\"node_{self.next_node_id}\"\n",
    "        self.next_node_id += 1\n",
    "        \n",
    "        node = {\n",
    "            \"id\": node_id,\n",
    "            \"type\": node_type,  # \"AND\" or \"OR\"\n",
    "            \"level\": level,     # L1, L2, L3, L4, L5\n",
    "            \"value\": value,     # The actual content (token, phrase, etc.)\n",
    "            \"frequency\": frequency\n",
    "        }\n",
    "        \n",
    "        if components:\n",
    "            node[\"components\"] = components\n",
    "            \n",
    "        self.graph[\"nodes\"][node_id] = node\n",
    "        \n",
    "        # Update metadata\n",
    "        if node_type == \"AND\":\n",
    "            self.graph[\"metadata\"][\"and_count\"] += 1\n",
    "        else:  # \"OR\"\n",
    "            self.graph[\"metadata\"][\"or_count\"] += 1\n",
    "            \n",
    "        self.graph[\"metadata\"][\"levels\"][level] += 1\n",
    "        \n",
    "        # Add to NetworkX graph for visualization\n",
    "        color = self._get_node_color(node_type, level)\n",
    "        shape = \"box\" if node_type == \"AND\" else \"ellipse\"\n",
    "        self.nx_graph.add_node(node_id, \n",
    "                              label=str(value), \n",
    "                              type=node_type,\n",
    "                              level=level,\n",
    "                              color=color,\n",
    "                              shape=shape)\n",
    "        \n",
    "        return node_id\n",
    "    \n",
    "    def _create_edge(self, source_id, target_id, edge_type, weight=1):\n",
    "        \"\"\"Create a new edge in the graph\"\"\"\n",
    "        edge = {\n",
    "            \"source\": source_id,\n",
    "            \"target\": target_id,\n",
    "            \"type\": edge_type,\n",
    "            \"weight\": weight\n",
    "        }\n",
    "        \n",
    "        # Check if edge already exists\n",
    "        for existing_edge in self.graph[\"edges\"]:\n",
    "            if (existing_edge[\"source\"] == source_id and \n",
    "                existing_edge[\"target\"] == target_id and\n",
    "                existing_edge[\"type\"] == edge_type):\n",
    "                # Update weight of existing edge\n",
    "                existing_edge[\"weight\"] += weight\n",
    "                # Update NetworkX edge\n",
    "                self.nx_graph[source_id][target_id][\"weight\"] = existing_edge[\"weight\"]\n",
    "                return\n",
    "        \n",
    "        # Add new edge\n",
    "        self.graph[\"edges\"].append(edge)\n",
    "        \n",
    "        # Add to NetworkX graph\n",
    "        self.nx_graph.add_edge(source_id, target_id, \n",
    "                              type=edge_type, \n",
    "                              weight=weight)\n",
    "    \n",
    "    def _get_node_color(self, node_type, level):\n",
    "        \"\"\"Get node color based on type and level for visualization\"\"\"\n",
    "        # Base colors\n",
    "        and_color = [0.2, 0.6, 0.9, 1.0]  # Blue\n",
    "        or_color = [0.9, 0.4, 0.2, 1.0]   # Orange\n",
    "        \n",
    "        # Adjust color intensity based on level\n",
    "        level_num = int(level[1])  # Extract number from L1, L2, etc.\n",
    "        intensity = 0.5 + (level_num * 0.1)  # Darker for higher levels\n",
    "        \n",
    "        if node_type == \"AND\":\n",
    "            color = [c * intensity for c in and_color[:-1]] + [and_color[-1]]\n",
    "        else:  # \"OR\"\n",
    "            color = [c * intensity for c in or_color[:-1]] + [or_color[-1]]\n",
    "            \n",
    "        return f\"rgba({color[0]*255:.0f}, {color[1]*255:.0f}, {color[2]*255:.0f}, {color[3]:.1f})\"\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Convert text to tokens, handling basic punctuation\"\"\"\n",
    "        # Remove excess whitespace and convert to lowercase\n",
    "        text = text.strip().lower()\n",
    "        # Simple tokenization: split on whitespace and keep punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[.,!?;]', text)\n",
    "        return tokens\n",
    "    \n",
    "    def process_sequence(self, sequence):\n",
    "        \"\"\"Process a single sequence (sentence) and update the graph\"\"\"\n",
    "        try:\n",
    "            tokens = self.tokenize(sequence)\n",
    "            if not tokens:\n",
    "                return\n",
    "            \n",
    "            # Store the sequence for higher-level processing\n",
    "            self.sequence_buffer.append(tokens)\n",
    "            \n",
    "            # Process L1: Token transitions\n",
    "            self._process_tokens(tokens)\n",
    "            \n",
    "            # Process higher levels if we have enough sequences\n",
    "            if len(self.sequence_buffer) >= 5:  # Wait until we have enough context\n",
    "                self._build_higher_levels()\n",
    "                # Keep only the most recent sequences for sliding window\n",
    "                self.sequence_buffer = self.sequence_buffer[-5:]\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error processing sequence: {sequence}\")\n",
    "                print(f\"Error details: {e}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def _process_tokens(self, tokens):\n",
    "        \"\"\"Process token-level (L1) structures including OR nodes for alternatives\"\"\"\n",
    "        try:\n",
    "            # Track token nodes in this sequence\n",
    "            sequence_nodes = []\n",
    "            \n",
    "            # First, ensure all tokens have nodes\n",
    "            token_nodes = {}\n",
    "            for token in tokens:\n",
    "                try:\n",
    "                    # Update token frequency\n",
    "                    self.token_frequencies[token] += 1\n",
    "                    \n",
    "                    # Check if token already has a node\n",
    "                    token_node_id = None\n",
    "                    for node_id, node in self.graph[\"nodes\"].items():\n",
    "                        if node[\"level\"] == \"L1\" and node[\"type\"] == \"AND\" and node[\"value\"] == token:\n",
    "                            token_node_id = node_id\n",
    "                            node[\"frequency\"] += 1\n",
    "                            break\n",
    "                    \n",
    "                    # Create token node if needed (token nodes are AND nodes)\n",
    "                    if not token_node_id:\n",
    "                        token_node_id = self._create_node(\"AND\", \"L1\", token)\n",
    "                    \n",
    "                    token_nodes[token] = token_node_id\n",
    "                    sequence_nodes.append(token_node_id)\n",
    "                except Exception as e:\n",
    "                    if self.debug:\n",
    "                        print(f\"Error processing token '{token}': {e}\")\n",
    "                        continue  # Skip this token but continue with others\n",
    "            \n",
    "            # Process transitions between tokens\n",
    "            for i in range(len(tokens) - 1):\n",
    "                try:\n",
    "                    current_token = tokens[i]\n",
    "                    next_token = tokens[i + 1]\n",
    "                    \n",
    "                    if current_token not in token_nodes or next_token not in token_nodes:\n",
    "                        continue  # Skip if either token was not processed\n",
    "                        \n",
    "                    current_node_id = token_nodes[current_token]\n",
    "                    next_node_id = token_nodes[next_token]\n",
    "                    \n",
    "                    # Update tracking for OR node creation\n",
    "                    self.successors[current_node_id].add(next_node_id)\n",
    "                    self.predecessors[next_node_id].add(current_node_id)\n",
    "                    \n",
    "                    # Create direct edge (will be replaced by OR nodes later)\n",
    "                    self._create_edge(current_node_id, next_node_id, \"sequence\")\n",
    "                    \n",
    "                    # Track bigram frequency\n",
    "                    bigram_key = f\"{current_token}_{next_token}\"\n",
    "                    self.bigram_frequencies[bigram_key] += 1\n",
    "                except Exception as e:\n",
    "                    if self.debug:\n",
    "                        print(f\"Error processing transition {tokens[i]} -> {tokens[i+1]}: {e}\")\n",
    "                        continue  # Skip this transition but continue with others\n",
    "            \n",
    "            # Create OR nodes for convergence and divergence points\n",
    "            self._create_or_nodes()\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error in _process_tokens: {e}\")\n",
    "                traceback.print_exc()\n",
    "        \n",
    "    def _create_or_nodes(self):\n",
    "        \"\"\"Create OR nodes for points with multiple predecessors or successors\"\"\"\n",
    "        try:\n",
    "            # Handle convergence points (multiple paths leading to the same node)\n",
    "            for node_id, pred_set in self.predecessors.items():\n",
    "                if len(pred_set) > 1:  # Multiple predecessors\n",
    "                    try:\n",
    "                        # Create an OR node for this convergence point\n",
    "                        or_node_id = self._create_node(\"OR\", \"L1\", f\"OR_in_{node_id}\", components=list(pred_set))\n",
    "                        \n",
    "                        # Connect predecessors to OR node instead of directly to the target\n",
    "                        for pred_id in pred_set:\n",
    "                            # Remove direct edge\n",
    "                            edges_to_remove = []\n",
    "                            for i, edge in enumerate(self.graph[\"edges\"]):\n",
    "                                if edge[\"source\"] == pred_id and edge[\"target\"] == node_id:\n",
    "                                    edges_to_remove.append(i)\n",
    "                                    # Add edge from predecessor to OR node\n",
    "                                    self._create_edge(pred_id, or_node_id, \"alternative\")\n",
    "                            \n",
    "                            # Remove edges in reverse order to avoid index issues\n",
    "                            for i in sorted(edges_to_remove, reverse=True):\n",
    "                                if i < len(self.graph[\"edges\"]):  # Safety check\n",
    "                                    del self.graph[\"edges\"][i]\n",
    "                                \n",
    "                            # Remove from NetworkX graph if it exists\n",
    "                            if self.nx_graph.has_edge(pred_id, node_id):\n",
    "                                self.nx_graph.remove_edge(pred_id, node_id)\n",
    "                        \n",
    "                        # Add edge from OR node to target\n",
    "                        self._create_edge(or_node_id, node_id, \"sequence\")\n",
    "                    except Exception as e:\n",
    "                        if self.debug:\n",
    "                            print(f\"Error creating convergence OR node for {node_id}: {e}\")\n",
    "            \n",
    "            # Handle divergence points (node with multiple possible next nodes)\n",
    "            for node_id, succ_set in self.successors.items():\n",
    "                if len(succ_set) > 1:  # Multiple successors\n",
    "                    try:\n",
    "                        # Create an OR node for this divergence point\n",
    "                        or_node_id = self._create_node(\"OR\", \"L1\", f\"OR_out_{node_id}\", components=list(succ_set))\n",
    "                        \n",
    "                        # Connect OR node to successors instead of directly from the source\n",
    "                        for succ_id in succ_set:\n",
    "                            # Remove direct edge\n",
    "                            edges_to_remove = []\n",
    "                            for i, edge in enumerate(self.graph[\"edges\"]):\n",
    "                                if edge[\"source\"] == node_id and edge[\"target\"] == succ_id:\n",
    "                                    edges_to_remove.append(i)\n",
    "                                    # Add edge from OR node to successor\n",
    "                                    self._create_edge(or_node_id, succ_id, \"alternative\")\n",
    "                            \n",
    "                            # Remove edges in reverse order to avoid index issues\n",
    "                            for i in sorted(edges_to_remove, reverse=True):\n",
    "                                if i < len(self.graph[\"edges\"]):  # Safety check\n",
    "                                    del self.graph[\"edges\"][i]\n",
    "                                \n",
    "                            # Remove from NetworkX graph if it exists\n",
    "                            if self.nx_graph.has_edge(node_id, succ_id):\n",
    "                                self.nx_graph.remove_edge(node_id, succ_id)\n",
    "                        \n",
    "                        # Add edge from source to OR node\n",
    "                        self._create_edge(node_id, or_node_id, \"sequence\")\n",
    "                    except Exception as e:\n",
    "                        if self.debug:\n",
    "                            print(f\"Error creating divergence OR node for {node_id}: {e}\")\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error in _create_or_nodes: {e}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def _build_higher_levels(self):\n",
    "        \"\"\"Build higher-level structures (L2-L5) based on collected data\"\"\"\n",
    "        # Process L2: Bigram memory (AND nodes)\n",
    "        self._build_bigrams()\n",
    "        \n",
    "        # Process L3: Phrase memory (AND nodes)\n",
    "        self._build_phrases()\n",
    "        \n",
    "        # Process L4: Phrase hierarchy (AND nodes)\n",
    "        self._build_hierarchies()\n",
    "        \n",
    "        # Process L5: Discourse patterns (AND nodes)\n",
    "        self._build_discourse_patterns()\n",
    "    \n",
    "    def _build_bigrams(self):\n",
    "        \"\"\"Build L2: Bigram nodes (AND nodes) from token transitions\"\"\"\n",
    "        for bigram, freq in self.bigram_frequencies.items():\n",
    "            if freq < self.thresholds[\"bigram\"]:\n",
    "                continue\n",
    "                \n",
    "            # Parse the bigram key back into tokens\n",
    "            # Handle case when bigram key might have multiple underscores\n",
    "            parts = bigram.split(\"_\")\n",
    "            if len(parts) < 2:\n",
    "                continue  # Skip if invalid format\n",
    "            token1 = parts[0]\n",
    "            token2 = parts[1]  # This takes the second token, ignoring any additional splits\n",
    "            \n",
    "            # Find token node IDs\n",
    "            token1_node_id = None\n",
    "            token2_node_id = None\n",
    "            \n",
    "            for node_id, node in self.graph[\"nodes\"].items():\n",
    "                if node[\"level\"] == \"L1\" and node[\"type\"] == \"AND\":\n",
    "                    if node[\"value\"] == token1:\n",
    "                        token1_node_id = node_id\n",
    "                    elif node[\"value\"] == token2:\n",
    "                        token2_node_id = node_id\n",
    "            \n",
    "            if not token1_node_id or not token2_node_id:\n",
    "                continue  # Skip if token nodes not found\n",
    "            \n",
    "            # Check if bigram already exists\n",
    "            bigram_node_id = None\n",
    "            for node_id, node in self.graph[\"nodes\"].items():\n",
    "                if (node[\"level\"] == \"L2\" and \n",
    "                    node[\"type\"] == \"AND\" and \n",
    "                    \"components\" in node and \n",
    "                    set(node[\"components\"]) == {token1_node_id, token2_node_id}):\n",
    "                    bigram_node_id = node_id\n",
    "                    node[\"frequency\"] += freq\n",
    "                    break\n",
    "            \n",
    "            # Create new bigram node if needed\n",
    "            if not bigram_node_id:\n",
    "                bigram_node_id = self._create_node(\n",
    "                    \"AND\", \n",
    "                    \"L2\", \n",
    "                    f\"{token1}_{token2}\",\n",
    "                    components=[token1_node_id, token2_node_id],\n",
    "                    frequency=freq\n",
    "                )\n",
    "                \n",
    "                # Connect token nodes to bigram node\n",
    "                self._create_edge(token1_node_id, bigram_node_id, \"composition\")\n",
    "                self._create_edge(token2_node_id, bigram_node_id, \"composition\")\n",
    "    \n",
    "    def _build_phrases(self):\n",
    "        \"\"\"Build L3: Phrase nodes (AND nodes) from sequences of tokens\"\"\"\n",
    "        try:\n",
    "            # Process each sequence in the buffer to find phrases\n",
    "            for tokens in self.sequence_buffer:\n",
    "                # Find all potential phrases (3+ tokens)\n",
    "                if len(tokens) < 3:\n",
    "                    continue\n",
    "                    \n",
    "                # Generate all possible phrases from the sequence\n",
    "                for i in range(len(tokens) - 2):\n",
    "                    for j in range(i + 2, min(i + 6, len(tokens))):  # Limit phrase length\n",
    "                        try:\n",
    "                            phrase_tokens = tokens[i:j+1]\n",
    "                            phrase_key = \"_\".join(phrase_tokens)\n",
    "                            \n",
    "                            # Update phrase frequency\n",
    "                            self.phrase_frequencies[phrase_key] += 1\n",
    "                            \n",
    "                            # Check if this meets our threshold\n",
    "                            if self.phrase_frequencies[phrase_key] >= self.thresholds[\"phrase\"]:\n",
    "                                # Find token node IDs in this phrase\n",
    "                                token_node_ids = []\n",
    "                                all_tokens_found = True\n",
    "                                \n",
    "                                for token in phrase_tokens:\n",
    "                                    token_found = False\n",
    "                                    for node_id, node in self.graph[\"nodes\"].items():\n",
    "                                        if (node[\"level\"] == \"L1\" and \n",
    "                                            node[\"type\"] == \"AND\" and \n",
    "                                            node[\"value\"] == token):\n",
    "                                            token_node_ids.append(node_id)\n",
    "                                            token_found = True\n",
    "                                            break\n",
    "                                    \n",
    "                                    if not token_found:\n",
    "                                        all_tokens_found = False\n",
    "                                        break\n",
    "                                \n",
    "                                if not all_tokens_found or len(token_node_ids) != len(phrase_tokens):\n",
    "                                    continue  # Skip if not all tokens found\n",
    "                                \n",
    "                                # Check if phrase already exists\n",
    "                                phrase_node_id = None\n",
    "                                for node_id, node in self.graph[\"nodes\"].items():\n",
    "                                    if (node[\"level\"] == \"L3\" and \n",
    "                                        node[\"type\"] == \"AND\" and \n",
    "                                        \"components\" in node and \n",
    "                                        len(node[\"components\"]) == len(token_node_ids) and\n",
    "                                        all(c1 == c2 for c1, c2 in zip(node[\"components\"], token_node_ids))):\n",
    "                                        phrase_node_id = node_id\n",
    "                                        node[\"frequency\"] += 1\n",
    "                                        break\n",
    "                                \n",
    "                                # Create new phrase node if needed\n",
    "                                if not phrase_node_id:\n",
    "                                    phrase_node_id = self._create_node(\n",
    "                                        \"AND\", \n",
    "                                        \"L3\", \n",
    "                                        phrase_key,\n",
    "                                        components=token_node_ids,\n",
    "                                        frequency=self.phrase_frequencies[phrase_key]\n",
    "                                    )\n",
    "                                    \n",
    "                                    # Connect token nodes to phrase node\n",
    "                                    for token_node_id in token_node_ids:\n",
    "                                        self._create_edge(token_node_id, phrase_node_id, \"composition\")\n",
    "                        except Exception as e:\n",
    "                            if self.debug:\n",
    "                                print(f\"Error processing phrase {phrase_tokens}: {e}\")\n",
    "                                continue  # Skip this phrase but continue with others\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                print(f\"Error in _build_phrases: {e}\")\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def _build_hierarchies(self):\n",
    "        \"\"\"Build L4: Hierarchy nodes (AND nodes) connecting phrases\"\"\"\n",
    "        # Create a map of phrases by sequence for co-occurrence analysis\n",
    "        sequence_phrases = defaultdict(list)\n",
    "        \n",
    "        # Find phrases in each sequence\n",
    "        for seq_idx, tokens in enumerate(self.sequence_buffer):\n",
    "            seq_str = \" \".join(tokens)\n",
    "            \n",
    "            # Check each phrase node to see if it appears in this sequence\n",
    "            for node_id, node in self.graph[\"nodes\"].items():\n",
    "                if node[\"level\"] == \"L3\" and node[\"type\"] == \"AND\":\n",
    "                    phrase = node[\"value\"].replace(\"_\", \" \")\n",
    "                    if phrase in seq_str:\n",
    "                        sequence_phrases[seq_idx].append(node_id)\n",
    "        \n",
    "        # Find co-occurring phrases\n",
    "        hierarchy_pairs = Counter()\n",
    "        \n",
    "        for seq_idx, phrase_nodes in sequence_phrases.items():\n",
    "            # Create pairs of co-occurring phrases\n",
    "            for i in range(len(phrase_nodes)):\n",
    "                for j in range(i+1, len(phrase_nodes)):\n",
    "                    pair_key = f\"{phrase_nodes[i]}_{phrase_nodes[j]}\"\n",
    "                    hierarchy_pairs[pair_key] += 1\n",
    "        \n",
    "        # Create hierarchy nodes for frequent co-occurrences\n",
    "        for pair_key, freq in hierarchy_pairs.items():\n",
    "            if freq < self.thresholds[\"hierarchy\"]:\n",
    "                continue\n",
    "                \n",
    "            phrase1_id, phrase2_id = pair_key.split(\"_\")\n",
    "            \n",
    "            # Check if hierarchy already exists\n",
    "            hierarchy_node_id = None\n",
    "            for node_id, node in self.graph[\"nodes\"].items():\n",
    "                if (node[\"level\"] == \"L4\" and \n",
    "                    node[\"type\"] == \"AND\" and \n",
    "                    \"components\" in node and \n",
    "                    set(node[\"components\"]) == {phrase1_id, phrase2_id}):\n",
    "                    hierarchy_node_id = node_id\n",
    "                    node[\"frequency\"] += freq\n",
    "                    break\n",
    "            \n",
    "            # Create new hierarchy node if needed\n",
    "            if not hierarchy_node_id:\n",
    "                phrase1_value = self.graph[\"nodes\"][phrase1_id][\"value\"]\n",
    "                phrase2_value = self.graph[\"nodes\"][phrase2_id][\"value\"]\n",
    "                \n",
    "                hierarchy_node_id = self._create_node(\n",
    "                    \"AND\", \n",
    "                    \"L4\", \n",
    "                    f\"H({phrase1_value},{phrase2_value})\",\n",
    "                    components=[phrase1_id, phrase2_id],\n",
    "                    frequency=freq\n",
    "                )\n",
    "                \n",
    "                # Connect phrase nodes to hierarchy node\n",
    "                self._create_edge(phrase1_id, hierarchy_node_id, \"composition\")\n",
    "                self._create_edge(phrase2_id, hierarchy_node_id, \"composition\")\n",
    "    \n",
    "    def _build_discourse_patterns(self):\n",
    "        \"\"\"Build L5: Discourse nodes (AND nodes) for patterns across sentences\"\"\"\n",
    "        # Track which hierarchies appear in which sequences\n",
    "        hierarchy_by_seq = defaultdict(list)\n",
    "        \n",
    "        # Find hierarchies in each sequence\n",
    "        for seq_idx, tokens in enumerate(self.sequence_buffer):\n",
    "            seq_text = \" \".join(tokens)\n",
    "            \n",
    "            # Check each hierarchy node\n",
    "            for node_id, node in self.graph[\"nodes\"].items():\n",
    "                if node[\"level\"] == \"L4\" and node[\"type\"] == \"AND\":\n",
    "                    # Get the phrases in this hierarchy\n",
    "                    if \"components\" not in node or len(node[\"components\"]) < 2:\n",
    "                        continue  # Skip if components missing or insufficient\n",
    "                    \n",
    "                    components = node[\"components\"]\n",
    "                    phrase1_id = components[0]\n",
    "                    phrase2_id = components[1]\n",
    "                    phrase1 = self.graph[\"nodes\"][phrase1_id][\"value\"].replace(\"_\", \" \")\n",
    "                    phrase2 = self.graph[\"nodes\"][phrase2_id][\"value\"].replace(\"_\", \" \")\n",
    "                    \n",
    "                    # Check if both phrases appear in the sequence\n",
    "                    if phrase1 in seq_text and phrase2 in seq_text:\n",
    "                        hierarchy_by_seq[seq_idx].append(node_id)\n",
    "        \n",
    "        # Find discourse patterns (hierarchies that appear in consecutive sequences)\n",
    "        discourse_patterns = Counter()\n",
    "        \n",
    "        for i in range(len(self.sequence_buffer) - 1):\n",
    "            for h1 in hierarchy_by_seq.get(i, []):\n",
    "                for h2 in hierarchy_by_seq.get(i+1, []):\n",
    "                    pattern_key = f\"{h1}_{h2}\"\n",
    "                    discourse_patterns[pattern_key] += 1\n",
    "        \n",
    "        # Create discourse nodes for frequent patterns\n",
    "        for pattern_key, freq in discourse_patterns.items():\n",
    "            if freq < self.thresholds[\"discourse\"]:\n",
    "                continue\n",
    "                \n",
    "            # Safely handle pattern keys with multiple underscores\n",
    "            parts = pattern_key.split(\"_\")\n",
    "            if len(parts) < 2:\n",
    "                continue  # Skip if invalid format\n",
    "            h1_id = parts[0]\n",
    "            h2_id = \"_\".join(parts[1:])  # Join the rest as the second part\n",
    "            \n",
    "            # Check if discourse pattern already exists\n",
    "            discourse_node_id = None\n",
    "            for node_id, node in self.graph[\"nodes\"].items():\n",
    "                if (node[\"level\"] == \"L5\" and \n",
    "                    node[\"type\"] == \"AND\" and \n",
    "                    \"components\" in node and \n",
    "                    set(node[\"components\"]) == {h1_id, h2_id}):\n",
    "                    discourse_node_id = node_id\n",
    "                    node[\"frequency\"] += freq\n",
    "                    break\n",
    "            \n",
    "            # Create new discourse node if needed\n",
    "            if not discourse_node_id:\n",
    "                h1_value = self.graph[\"nodes\"][h1_id][\"value\"]\n",
    "                h2_value = self.graph[\"nodes\"][h2_id][\"value\"]\n",
    "                \n",
    "                discourse_node_id = self._create_node(\n",
    "                    \"AND\", \n",
    "                    \"L5\", \n",
    "                    f\"D({h1_value}â†’{h2_value})\",\n",
    "                    components=[h1_id, h2_id],\n",
    "                    frequency=freq\n",
    "                )\n",
    "                \n",
    "                # Connect hierarchy nodes to discourse node\n",
    "                self._create_edge(h1_id, discourse_node_id, \"composition\")\n",
    "                self._create_edge(h2_id, discourse_node_id, \"composition\")\n",
    "    \n",
    "    def update_metadata(self):\n",
    "        \"\"\"Update the metadata with current counts\"\"\"\n",
    "        self.graph[\"metadata\"][\"token_count\"] = sum(1 for node in self.graph[\"nodes\"].values() \n",
    "                                                if node[\"level\"] == \"L1\" and node[\"type\"] == \"AND\")\n",
    "    \n",
    "    def build_from_file(self, filepath):\n",
    "        \"\"\"Build the graph model from sequences in a text file\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                for line_num, line in enumerate(file, 1):\n",
    "                    line = line.strip()\n",
    "                    if line:  # Skip empty lines\n",
    "                        try:\n",
    "                            self.process_sequence(line)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing line {line_num}: {line}\")\n",
    "                            print(f\"Error details: {e}\")\n",
    "                            # Continue with next line instead of failing completely\n",
    "            \n",
    "            try:\n",
    "                # Ensure we process any remaining sequences\n",
    "                self._build_higher_levels()\n",
    "            except Exception as e:\n",
    "                print(f\"Error building higher levels: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                # Continue to metadata update\n",
    "            \n",
    "            # Update metadata\n",
    "            self.update_metadata()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def save_to_json(self, output_path):\n",
    "        \"\"\"Save the graph model to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(self.graph, file, indent=2)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving JSON: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def visualize(self, output_path=None, max_nodes=100, show=True):\n",
    "        \"\"\"\n",
    "        Visualize the graph using NetworkX and matplotlib\n",
    "        \n",
    "        Args:\n",
    "            output_path: Path to save the visualization image (optional)\n",
    "            max_nodes: Maximum number of nodes to display (for readability)\n",
    "            show: Whether to display the graph\n",
    "        \"\"\"\n",
    "        if len(self.nx_graph) == 0:\n",
    "            print(\"Graph is empty, nothing to visualize.\")\n",
    "            return False\n",
    "            \n",
    "        # Limit display to max_nodes for readability\n",
    "        if len(self.nx_graph) > max_nodes:\n",
    "            print(f\"Graph has {len(self.nx_graph)} nodes, limiting visualization to {max_nodes} nodes.\")\n",
    "            \n",
    "            # Create a subgraph with important nodes\n",
    "            subgraph_nodes = []\n",
    "            \n",
    "            # Include higher level nodes first\n",
    "            for level in [\"L5\", \"L4\", \"L3\", \"L2\", \"L1\"]:\n",
    "                level_nodes = [n for n, d in self.nx_graph.nodes(data=True) \n",
    "                              if d.get(\"level\") == level]\n",
    "                subgraph_nodes.extend(level_nodes)\n",
    "                if len(subgraph_nodes) >= max_nodes:\n",
    "                    break\n",
    "            \n",
    "            # Limit to max_nodes\n",
    "            subgraph_nodes = subgraph_nodes[:max_nodes]\n",
    "            \n",
    "            # Create subgraph\n",
    "            subgraph = self.nx_graph.subgraph(subgraph_nodes)\n",
    "        else:\n",
    "            subgraph = self.nx_graph\n",
    "        \n",
    "        # Create position layout\n",
    "        pos = nx.spring_layout(subgraph, k=0.3, iterations=50)\n",
    "        \n",
    "        # Setup figure\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Draw nodes\n",
    "        node_colors = []\n",
    "        node_sizes = []\n",
    "        node_shapes = []\n",
    "        \n",
    "        for node in subgraph.nodes():\n",
    "            node_data = subgraph.nodes[node]\n",
    "            level = node_data.get(\"level\", \"L1\")\n",
    "            node_type = node_data.get(\"type\", \"AND\")\n",
    "            \n",
    "            # Determine node size based on level\n",
    "            level_num = int(level[1])\n",
    "            size = 300 + (level_num * 100)  # Larger nodes for higher levels\n",
    "            node_sizes.append(size)\n",
    "            \n",
    "            # Determine node color\n",
    "            if node_type == \"AND\":\n",
    "                color = (0.2, 0.6, 0.9, 1.0)  # Blue for AND\n",
    "            else:\n",
    "                color = (0.9, 0.4, 0.2, 1.0)  # Orange for OR\n",
    "            \n",
    "            # Adjust color intensity for level\n",
    "            intensity = 0.5 + (level_num * 0.1)\n",
    "            color = (color[0] * intensity, color[1] * intensity, color[2] * intensity, color[3])\n",
    "            node_colors.append(color)\n",
    "            \n",
    "            # Determine shape\n",
    "            shape = \"s\" if node_type == \"AND\" else \"o\"  # Square for AND, Circle for OR\n",
    "            node_shapes.append(shape)\n",
    "        \n",
    "        # Draw edges\n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "        \n",
    "        for u, v, data in subgraph.edges(data=True):\n",
    "            edge_type = data.get(\"type\", \"sequence\")\n",
    "            \n",
    "            if edge_type == \"composition\":\n",
    "                color = \"green\"\n",
    "            elif edge_type == \"alternative\":\n",
    "                color = \"red\"\n",
    "            else:  # sequence\n",
    "                color = \"gray\"\n",
    "                \n",
    "            edge_colors.append(color)\n",
    "            \n",
    "            # Width based on weight\n",
    "            width = data.get(\"weight\", 1) * 0.5\n",
    "            edge_widths.append(min(width, 3.0))  # Cap width for readability\n",
    "        \n",
    "        # Draw the graph\n",
    "        for i, node in enumerate(subgraph.nodes()):\n",
    "            nx.draw_networkx_nodes(\n",
    "                subgraph, pos,\n",
    "                nodelist=[node],\n",
    "                node_color=[node_colors[i]],\n",
    "                node_size=node_sizes[i],\n",
    "                node_shape=node_shapes[i],\n",
    "                alpha=0.8\n",
    "            )\n",
    "        \n",
    "        nx.draw_networkx_edges(\n",
    "            subgraph, pos,\n",
    "            width=edge_widths,\n",
    "            edge_color=edge_colors,\n",
    "            arrows=True,\n",
    "            arrowsize=10,\n",
    "            connectionstyle=\"arc3,rad=0.1\"\n",
    "        )\n",
    "        \n",
    "        # Draw labels\n",
    "        nx.draw_networkx_labels(\n",
    "            subgraph, pos,\n",
    "            labels={n: d.get(\"label\", n) for n, d in subgraph.nodes(data=True)},\n",
    "            font_size=8,\n",
    "            font_color=\"black\"\n",
    "        )\n",
    "        \n",
    "        # Add legend\n",
    "        and_patch = plt.Line2D([0], [0], marker='s', color='w', \n",
    "                              markerfacecolor=(0.2, 0.6, 0.9), markersize=10, label='AND')\n",
    "        or_patch = plt.Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor=(0.9, 0.4, 0.2), markersize=10, label='OR')\n",
    "        \n",
    "        l1_line = plt.Line2D([0], [0], color='w', marker='o', \n",
    "                            markerfacecolor=(0.2, 0.6, 0.9, 0.6), markersize=10, label='L1')\n",
    "        l2_line = plt.Line2D([0], [0], color='w', marker='o', \n",
    "                            markerfacecolor=(0.2, 0.6, 0.9, 0.7), markersize=12, label='L2')\n",
    "        l3_line = plt.Line2D([0], [0], color='w', marker='o', \n",
    "                            markerfacecolor=(0.2, 0.6, 0.9, 0.8), markersize=14, label='L3')\n",
    "        l4_line = plt.Line2D([0], [0], color='w', marker='o', \n",
    "                            markerfacecolor=(0.2, 0.6, 0.9, 0.9), markersize=16, label='L4')\n",
    "        l5_line = plt.Line2D([0], [0], color='w', marker='o', \n",
    "                            markerfacecolor=(0.2, 0.6, 0.9, 1.0), markersize=18, label='L5')\n",
    "        \n",
    "        seq_line = plt.Line2D([0], [0], color='gray', lw=2, label='Sequence')\n",
    "        comp_line = plt.Line2D([0], [0], color='green', lw=2, label='Composition')\n",
    "        alt_line = plt.Line2D([0], [0], color='red', lw=2, label='Alternative')\n",
    "        \n",
    "        plt.legend(handles=[and_patch, or_patch, l1_line, l2_line, l3_line, l4_line, l5_line,\n",
    "                           seq_line, comp_line, alt_line], loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        \n",
    "        plt.title(\"HTPC AND-OR Graph Visualization\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save if path provided\n",
    "        if output_path:\n",
    "            plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "            print(f\"Visualization saved to {output_path}\")\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8c80c-e0d4-4500-af0c-315e01d00ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.PyTorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
