{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b436818-062e-40fc-8ee9-cbf8007011e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class HTCPGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds a hierarchical graph model for HTPC with 5 levels:\n",
    "    - L1: Token transitions\n",
    "    - L2: Bigram memory\n",
    "    - L3: Phrase memory\n",
    "    - L4: Phrase hierarchy\n",
    "    - L5: Discourse patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize the graph structure\n",
    "        self.graph = {\n",
    "            \"nodes\": {\n",
    "                \"token\": {},      # L1: Token nodes\n",
    "                \"bigram\": {},     # L2: Bigram nodes\n",
    "                \"phrase\": {},     # L3: Phrase nodes\n",
    "                \"hierarchy\": {},  # L4: Hierarchy nodes\n",
    "                \"discourse\": {}   # L5: Discourse nodes\n",
    "            },\n",
    "            \"edges\": {\n",
    "                \"sequence\": [],   # Token-to-token connections\n",
    "                \"composition\": [], # Connections between levels\n",
    "                \"context\": []     # Contextual relationships\n",
    "            },\n",
    "            \"metadata\": {\n",
    "                \"token_count\": 0,\n",
    "                \"bigram_count\": 0,\n",
    "                \"phrase_count\": 0,\n",
    "                \"hierarchy_count\": 0,\n",
    "                \"discourse_count\": 0\n",
    "            }\n",
    "        }\n",
    "        # Thresholds for each level\n",
    "        self.thresholds = {\n",
    "            \"bigram\": 2,      # Minimum frequency to create a bigram\n",
    "            \"phrase\": 2,      # Minimum frequency to create a phrase\n",
    "            \"hierarchy\": 2,   # Minimum frequency to create a hierarchy\n",
    "            \"discourse\": 2    # Minimum frequency to create a discourse pattern\n",
    "        }\n",
    "        # Tracking counters\n",
    "        self.token_frequencies = Counter()\n",
    "        self.bigram_frequencies = Counter()\n",
    "        self.phrase_frequencies = Counter()\n",
    "        self.hierarchy_frequencies = Counter()\n",
    "        # Temporary storage for sequence processing\n",
    "        self.sequence_buffer = []\n",
    "        # Node ID counters\n",
    "        self.next_node_id = {\n",
    "            \"token\": 0,\n",
    "            \"bigram\": 0,\n",
    "            \"phrase\": 0,\n",
    "            \"hierarchy\": 0,\n",
    "            \"discourse\": 0\n",
    "        }\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Convert text to tokens, handling basic punctuation\"\"\"\n",
    "        # Remove excess whitespace and convert to lowercase\n",
    "        text = text.strip().lower()\n",
    "        # Simple tokenization: split on whitespace and keep punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[.,!?;]', text)\n",
    "        return tokens\n",
    "    \n",
    "    def process_sequence(self, sequence):\n",
    "        \"\"\"Process a single sequence (sentence) and update the graph\"\"\"\n",
    "        tokens = self.tokenize(sequence)\n",
    "        if not tokens:\n",
    "            return\n",
    "        \n",
    "        # Store the sequence for higher-level processing\n",
    "        self.sequence_buffer.append(tokens)\n",
    "        \n",
    "        # Process L1: Token transitions\n",
    "        self._process_tokens(tokens)\n",
    "        \n",
    "        # Process higher levels if we have enough sequences\n",
    "        if len(self.sequence_buffer) >= 5:  # Wait until we have enough context\n",
    "            self._build_higher_levels()\n",
    "            # Keep only the most recent sequences for sliding window\n",
    "            self.sequence_buffer = self.sequence_buffer[-5:]\n",
    "    \n",
    "    def _process_tokens(self, tokens):\n",
    "        \"\"\"Process token-level (L1) structures\"\"\"\n",
    "        # Add tokens to the graph\n",
    "        previous_token = None\n",
    "        for token in tokens:\n",
    "            # Update token frequency\n",
    "            self.token_frequencies[token] += 1\n",
    "            \n",
    "            # Add token node if it doesn't exist\n",
    "            if token not in self.graph[\"nodes\"][\"token\"]:\n",
    "                token_id = f\"t_{self.next_node_id['token']}\"\n",
    "                self.next_node_id[\"token\"] += 1\n",
    "                self.graph[\"nodes\"][\"token\"][token] = {\n",
    "                    \"id\": token_id,\n",
    "                    \"value\": token,\n",
    "                    \"frequency\": 1\n",
    "                }\n",
    "            else:\n",
    "                # Update existing token frequency\n",
    "                self.graph[\"nodes\"][\"token\"][token][\"frequency\"] += 1\n",
    "                token_id = self.graph[\"nodes\"][\"token\"][token][\"id\"]\n",
    "            \n",
    "            # Create sequence edge if we have a previous token\n",
    "            if previous_token:\n",
    "                # Create a unique key for this token transition\n",
    "                transition_key = f\"{previous_token}_{token}\"\n",
    "                \n",
    "                # Track bigram frequency for L2\n",
    "                self.bigram_frequencies[transition_key] += 1\n",
    "                \n",
    "                # Add sequence edge\n",
    "                edge_exists = False\n",
    "                for edge in self.graph[\"edges\"][\"sequence\"]:\n",
    "                    if edge[\"source\"] == self.graph[\"nodes\"][\"token\"][previous_token][\"id\"] and \\\n",
    "                       edge[\"target\"] == token_id:\n",
    "                        edge[\"weight\"] += 1\n",
    "                        edge_exists = True\n",
    "                        break\n",
    "                \n",
    "                if not edge_exists:\n",
    "                    self.graph[\"edges\"][\"sequence\"].append({\n",
    "                        \"source\": self.graph[\"nodes\"][\"token\"][previous_token][\"id\"],\n",
    "                        \"target\": token_id,\n",
    "                        \"type\": \"sequence\",\n",
    "                        \"weight\": 1\n",
    "                    })\n",
    "            \n",
    "            previous_token = token\n",
    "    \n",
    "    def _build_higher_levels(self):\n",
    "        \"\"\"Build higher-level structures (L2-L5) based on collected data\"\"\"\n",
    "        # Process L2: Bigram memory\n",
    "        self._build_bigrams()\n",
    "        \n",
    "        # Process L3: Phrase memory\n",
    "        self._build_phrases()\n",
    "        \n",
    "        # Process L4: Phrase hierarchy\n",
    "        self._build_hierarchies()\n",
    "        \n",
    "        # Process L5: Discourse patterns\n",
    "        self._build_discourse_patterns()\n",
    "    \n",
    "    def _build_bigrams(self):\n",
    "        \"\"\"Build L2: Bigram nodes from token transitions\"\"\"\n",
    "        for bigram, freq in self.bigram_frequencies.items():\n",
    "            if freq < self.thresholds[\"bigram\"]:\n",
    "                continue\n",
    "                \n",
    "            # Parse the bigram key back into tokens\n",
    "            token1, token2 = bigram.split(\"_\")\n",
    "            \n",
    "            # Create bigram node if it doesn't exist\n",
    "            if bigram not in self.graph[\"nodes\"][\"bigram\"]:\n",
    "                bigram_id = f\"b_{self.next_node_id['bigram']}\"\n",
    "                self.next_node_id[\"bigram\"] += 1\n",
    "                self.graph[\"nodes\"][\"bigram\"][bigram] = {\n",
    "                    \"id\": bigram_id,\n",
    "                    \"tokens\": [token1, token2],\n",
    "                    \"frequency\": freq\n",
    "                }\n",
    "                \n",
    "                # Add composition edges connecting tokens to this bigram\n",
    "                self.graph[\"edges\"][\"composition\"].append({\n",
    "                    \"source\": self.graph[\"nodes\"][\"token\"][token1][\"id\"],\n",
    "                    \"target\": bigram_id,\n",
    "                    \"type\": \"composition\",\n",
    "                    \"level\": \"L1_to_L2\"\n",
    "                })\n",
    "                self.graph[\"edges\"][\"composition\"].append({\n",
    "                    \"source\": self.graph[\"nodes\"][\"token\"][token2][\"id\"],\n",
    "                    \"target\": bigram_id,\n",
    "                    \"type\": \"composition\",\n",
    "                    \"level\": \"L1_to_L2\"\n",
    "                })\n",
    "            else:\n",
    "                # Update frequency if bigram already exists\n",
    "                self.graph[\"nodes\"][\"bigram\"][bigram][\"frequency\"] += freq\n",
    "    \n",
    "    def _build_phrases(self):\n",
    "        \"\"\"Build L3: Phrase nodes from sequences of bigrams\"\"\"\n",
    "        # Process each sequence in the buffer to find phrases\n",
    "        for tokens in self.sequence_buffer:\n",
    "            # Find all potential phrases (3+ tokens)\n",
    "            if len(tokens) < 3:\n",
    "                continue\n",
    "                \n",
    "            # Generate all possible phrases from the sequence\n",
    "            for i in range(len(tokens) - 2):\n",
    "                for j in range(i + 2, min(i + 6, len(tokens))):  # Limit phrase length\n",
    "                    phrase_tokens = tokens[i:j+1]\n",
    "                    phrase_key = \"_\".join(phrase_tokens)\n",
    "                    \n",
    "                    # Update phrase frequency\n",
    "                    self.phrase_frequencies[phrase_key] += 1\n",
    "                    \n",
    "                    # Check if this meets our threshold\n",
    "                    if self.phrase_frequencies[phrase_key] >= self.thresholds[\"phrase\"]:\n",
    "                        # Create or update phrase node\n",
    "                        if phrase_key not in self.graph[\"nodes\"][\"phrase\"]:\n",
    "                            phrase_id = f\"p_{self.next_node_id['phrase']}\"\n",
    "                            self.next_node_id[\"phrase\"] += 1\n",
    "                            self.graph[\"nodes\"][\"phrase\"][phrase_key] = {\n",
    "                                \"id\": phrase_id,\n",
    "                                \"tokens\": phrase_tokens,\n",
    "                                \"frequency\": self.phrase_frequencies[phrase_key]\n",
    "                            }\n",
    "                            \n",
    "                            # Connect to component bigrams\n",
    "                            for k in range(len(phrase_tokens) - 1):\n",
    "                                bigram_key = f\"{phrase_tokens[k]}_{phrase_tokens[k+1]}\"\n",
    "                                if bigram_key in self.graph[\"nodes\"][\"bigram\"]:\n",
    "                                    self.graph[\"edges\"][\"composition\"].append({\n",
    "                                        \"source\": self.graph[\"nodes\"][\"bigram\"][bigram_key][\"id\"],\n",
    "                                        \"target\": phrase_id,\n",
    "                                        \"type\": \"composition\",\n",
    "                                        \"level\": \"L2_to_L3\"\n",
    "                                    })\n",
    "                        else:\n",
    "                            # Update existing phrase\n",
    "                            self.graph[\"nodes\"][\"phrase\"][phrase_key][\"frequency\"] = \\\n",
    "                                self.phrase_frequencies[phrase_key]\n",
    "    \n",
    "    def _build_hierarchies(self):\n",
    "        \"\"\"Build L4: Hierarchy nodes representing relationships between phrases\"\"\"\n",
    "        # Look for phrases that frequently appear in the same sentence\n",
    "        phrase_co_occurrences = Counter()\n",
    "        \n",
    "        for tokens in self.sequence_buffer:\n",
    "            # Find all phrases in this sequence\n",
    "            sequence_phrases = []\n",
    "            for phrase_key in self.graph[\"nodes\"][\"phrase\"]:\n",
    "                phrase_tokens = self.graph[\"nodes\"][\"phrase\"][phrase_key][\"tokens\"]\n",
    "                # Check if this phrase is in the sequence\n",
    "                phrase_str = \" \".join(phrase_tokens)\n",
    "                sequence_str = \" \".join(tokens)\n",
    "                if phrase_str in sequence_str:\n",
    "                    sequence_phrases.append(phrase_key)\n",
    "            \n",
    "            # Record co-occurrences of phrases\n",
    "            for i in range(len(sequence_phrases)):\n",
    "                for j in range(i+1, len(sequence_phrases)):\n",
    "                    hierarchy_key = f\"{sequence_phrases[i]}|{sequence_phrases[j]}\"\n",
    "                    phrase_co_occurrences[hierarchy_key] += 1\n",
    "        \n",
    "        # Create hierarchy nodes for frequent co-occurrences\n",
    "        for hierarchy_key, freq in phrase_co_occurrences.items():\n",
    "            if freq < self.thresholds[\"hierarchy\"]:\n",
    "                continue\n",
    "                \n",
    "            # Update our tracking counter\n",
    "            self.hierarchy_frequencies[hierarchy_key] += freq\n",
    "            \n",
    "            if hierarchy_key not in self.graph[\"nodes\"][\"hierarchy\"]:\n",
    "                hierarchy_id = f\"h_{self.next_node_id['hierarchy']}\"\n",
    "                self.next_node_id[\"hierarchy\"] += 1\n",
    "                \n",
    "                phrase1, phrase2 = hierarchy_key.split(\"|\")\n",
    "                \n",
    "                self.graph[\"nodes\"][\"hierarchy\"][hierarchy_key] = {\n",
    "                    \"id\": hierarchy_id,\n",
    "                    \"phrases\": [phrase1, phrase2],\n",
    "                    \"frequency\": freq\n",
    "                }\n",
    "                \n",
    "                # Connect to component phrases\n",
    "                if phrase1 in self.graph[\"nodes\"][\"phrase\"]:\n",
    "                    self.graph[\"edges\"][\"composition\"].append({\n",
    "                        \"source\": self.graph[\"nodes\"][\"phrase\"][phrase1][\"id\"],\n",
    "                        \"target\": hierarchy_id,\n",
    "                        \"type\": \"composition\",\n",
    "                        \"level\": \"L3_to_L4\"\n",
    "                    })\n",
    "                \n",
    "                if phrase2 in self.graph[\"nodes\"][\"phrase\"]:\n",
    "                    self.graph[\"edges\"][\"composition\"].append({\n",
    "                        \"source\": self.graph[\"nodes\"][\"phrase\"][phrase2][\"id\"],\n",
    "                        \"target\": hierarchy_id,\n",
    "                        \"type\": \"composition\",\n",
    "                        \"level\": \"L3_to_L4\"\n",
    "                    })\n",
    "            else:\n",
    "                # Update existing hierarchy node\n",
    "                self.graph[\"nodes\"][\"hierarchy\"][hierarchy_key][\"frequency\"] = \\\n",
    "                    self.hierarchy_frequencies[hierarchy_key]\n",
    "    \n",
    "    def _build_discourse_patterns(self):\n",
    "        \"\"\"Build L5: Discourse nodes representing patterns across multiple sentences\"\"\"\n",
    "        # This is a simplified implementation - in a full system, you would\n",
    "        # analyze patterns across multiple sequences\n",
    "        \n",
    "        # For our purposes, we'll connect hierarchy nodes that appear across different\n",
    "        # sequences in our buffer\n",
    "        \n",
    "        # First, find which hierarchies appear in which buffer positions\n",
    "        hierarchy_positions = defaultdict(list)\n",
    "        \n",
    "        for i, tokens in enumerate(self.sequence_buffer):\n",
    "            sequence_str = \" \".join(tokens)\n",
    "            \n",
    "            # Check which hierarchies might be present in this sequence\n",
    "            for hierarchy_key in self.graph[\"nodes\"][\"hierarchy\"]:\n",
    "                hierarchy_node = self.graph[\"nodes\"][\"hierarchy\"][hierarchy_key]\n",
    "                phrase1, phrase2 = hierarchy_key.split(\"|\")\n",
    "                \n",
    "                # Check if both phrases appear in the sequence\n",
    "                if phrase1 in self.graph[\"nodes\"][\"phrase\"] and phrase2 in self.graph[\"nodes\"][\"phrase\"]:\n",
    "                    phrase1_tokens = self.graph[\"nodes\"][\"phrase\"][phrase1][\"tokens\"]\n",
    "                    phrase2_tokens = self.graph[\"nodes\"][\"phrase\"][phrase2][\"tokens\"]\n",
    "                    \n",
    "                    phrase1_str = \" \".join(phrase1_tokens)\n",
    "                    phrase2_str = \" \".join(phrase2_tokens)\n",
    "                    \n",
    "                    if phrase1_str in sequence_str and phrase2_str in sequence_str:\n",
    "                        hierarchy_positions[hierarchy_key].append(i)\n",
    "        \n",
    "        # Find patterns across sequences\n",
    "        discourse_patterns = Counter()\n",
    "        \n",
    "        for h1 in hierarchy_positions:\n",
    "            for h2 in hierarchy_positions:\n",
    "                if h1 != h2:\n",
    "                    # Look for sequential patterns\n",
    "                    for pos1 in hierarchy_positions[h1]:\n",
    "                        for pos2 in hierarchy_positions[h2]:\n",
    "                            if pos2 == pos1 + 1:  # Adjacent sequences\n",
    "                                discourse_key = f\"{h1}||{h2}\"\n",
    "                                discourse_patterns[discourse_key] += 1\n",
    "        \n",
    "        # Create discourse nodes for frequent patterns\n",
    "        for discourse_key, freq in discourse_patterns.items():\n",
    "            if freq < self.thresholds[\"discourse\"]:\n",
    "                continue\n",
    "                \n",
    "            if discourse_key not in self.graph[\"nodes\"][\"discourse\"]:\n",
    "                discourse_id = f\"d_{self.next_node_id['discourse']}\"\n",
    "                self.next_node_id[\"discourse\"] += 1\n",
    "                \n",
    "                h1, h2 = discourse_key.split(\"||\")\n",
    "                \n",
    "                self.graph[\"nodes\"][\"discourse\"][discourse_key] = {\n",
    "                    \"id\": discourse_id,\n",
    "                    \"hierarchies\": [h1, h2],\n",
    "                    \"frequency\": freq\n",
    "                }\n",
    "                \n",
    "                # Connect to component hierarchies\n",
    "                if h1 in self.graph[\"nodes\"][\"hierarchy\"]:\n",
    "                    self.graph[\"edges\"][\"composition\"].append({\n",
    "                        \"source\": self.graph[\"nodes\"][\"hierarchy\"][h1][\"id\"],\n",
    "                        \"target\": discourse_id,\n",
    "                        \"type\": \"composition\",\n",
    "                        \"level\": \"L4_to_L5\"\n",
    "                    })\n",
    "                \n",
    "                if h2 in self.graph[\"nodes\"][\"hierarchy\"]:\n",
    "                    self.graph[\"edges\"][\"composition\"].append({\n",
    "                        \"source\": self.graph[\"nodes\"][\"hierarchy\"][h2][\"id\"],\n",
    "                        \"target\": discourse_id,\n",
    "                        \"type\": \"composition\",\n",
    "                        \"level\": \"L4_to_L5\"\n",
    "                    })\n",
    "            else:\n",
    "                # Update existing discourse node\n",
    "                self.graph[\"nodes\"][\"discourse\"][discourse_key][\"frequency\"] += freq\n",
    "    \n",
    "    def update_metadata(self):\n",
    "        \"\"\"Update the metadata with current counts\"\"\"\n",
    "        self.graph[\"metadata\"][\"token_count\"] = len(self.graph[\"nodes\"][\"token\"])\n",
    "        self.graph[\"metadata\"][\"bigram_count\"] = len(self.graph[\"nodes\"][\"bigram\"])\n",
    "        self.graph[\"metadata\"][\"phrase_count\"] = len(self.graph[\"nodes\"][\"phrase\"])\n",
    "        self.graph[\"metadata\"][\"hierarchy_count\"] = len(self.graph[\"nodes\"][\"hierarchy\"])\n",
    "        self.graph[\"metadata\"][\"discourse_count\"] = len(self.graph[\"nodes\"][\"discourse\"])\n",
    "    \n",
    "    def build_from_file(self, filepath):\n",
    "        \"\"\"Build the graph model from sequences in a text file\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if line:  # Skip empty lines\n",
    "                        self.process_sequence(line)\n",
    "            \n",
    "            # Ensure we process any remaining sequences\n",
    "            self._build_higher_levels()\n",
    "            \n",
    "            # Update metadata\n",
    "            self.update_metadata()\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def save_to_json(self, output_path):\n",
    "        \"\"\"Save the graph model to a JSON file\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(self.graph, file, indent=2)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving JSON: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) < 3:\n",
    "        print(\"Usage: python htpc_graph_builder.py <input_file> <output_file>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    input_file = \"test_input.txt\"\n",
    "    output_file = \"test_model.json\"\n",
    "    \n",
    "    builder = HTCPGraphBuilder()\n",
    "    print(f\"Building graph from {input_file}...\")\n",
    "    \n",
    "    if builder.build_from_file(input_file):\n",
    "        print(\"Graph built successfully.\")\n",
    "        \n",
    "        if builder.save_to_json(output_file):\n",
    "            print(f\"Graph saved to {output_file}\")\n",
    "            print(f\"Stats: {builder.graph['metadata']}\")\n",
    "        else:\n",
    "            print(\"Failed to save graph.\")\n",
    "    else:\n",
    "        print(\"Failed to build graph.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.PyTorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
