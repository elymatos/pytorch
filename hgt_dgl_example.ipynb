{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# üìò Minimal HGT Example in DGL\n",
    "# ==============================\n",
    "\n",
    "# üì¶ Install DGL\n",
    "!pip install dgl -f https://data.dgl.ai/wheels/repo.html --quiet\n",
    "!pip install transformers --quiet\n",
    "\n",
    "# ==============================\n",
    "# üìö Imports\n",
    "# ==============================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.nn import HeteroGraphConv, HGTConv\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# ==============================\n",
    "# üîß Define Toy Heterogeneous Graph\n",
    "# ==============================\n",
    "# 3 node types: paper, author, venue\n",
    "# 3 edge types: writes, publishes_in, cites\n",
    "\n",
    "num_papers = 5\n",
    "num_authors = 3\n",
    "num_venues = 2\n",
    "\n",
    "graph_data = {\n",
    "    ('author', 'writes', 'paper'): ([0, 1, 2], [0, 1, 2]),\n",
    "    ('paper', 'published_in', 'venue'): ([0, 1, 2, 3], [0, 1, 0, 1]),\n",
    "    ('paper', 'cites', 'paper'): ([0, 1, 2], [1, 2, 3])\n",
    "}\n",
    "\n",
    "hg = dgl.heterograph(graph_data)\n",
    "\n",
    "# ==============================\n",
    "# üìê Node Feature Initialization\n",
    "# ==============================\n",
    "# We'll use BERT for paper abstracts, random for authors/venues\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert.eval()\n",
    "\n",
    "paper_abstracts = [\n",
    "    \"Graph neural networks for citation prediction\",\n",
    "    \"Attention mechanisms in transformers\",\n",
    "    \"LLMs for graph completion tasks\",\n",
    "    \"Knowledge graphs and neural architectures\",\n",
    "    \"Survey on graph transformers\"\n",
    "]\n",
    "\n",
    "def encode_bert(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "paper_feats = encode_bert(paper_abstracts)\n",
    "author_feats = torch.randn(num_authors, 768)\n",
    "venue_feats = torch.randn(num_venues, 768)\n",
    "\n",
    "hg.nodes['paper'].data['h'] = paper_feats\n",
    "hg.nodes['author'].data['h'] = author_feats\n",
    "hg.nodes['venue'].data['h'] = venue_feats\n",
    "\n",
    "# ==============================\n",
    "# üß† HGT Model Definition\n",
    "# ==============================\n",
    "class HGTModel(nn.Module):\n",
    "    def __init__(self, metadata, in_dim, hidden_dim, out_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(HGTConv(\n",
    "                in_dim if i == 0 else hidden_dim,\n",
    "                hidden_dim,\n",
    "                metadata,\n",
    "                num_heads=num_heads,\n",
    "                dropout=0.2,\n",
    "                use_norm=True\n",
    "            ))\n",
    "        self.linear = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h_dict = inputs\n",
    "        for layer in self.layers:\n",
    "            h_dict = layer(g, h_dict)\n",
    "        return {k: self.linear(v) for k, v in h_dict.items()}\n",
    "\n",
    "# ==============================\n",
    "# üîÅ Training Loop (Example Task)\n",
    "# ==============================\n",
    "# Toy task: classify paper topics into 3 categories\n",
    "labels = torch.tensor([0, 1, 2, 0, 1])\n",
    "train_mask = torch.tensor([1, 1, 1, 0, 0], dtype=torch.bool)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HGTModel(hg.metadata(), 768, 256, 3, num_heads=4, num_layers=2).to(device)\n",
    "hg = hg.to(device)\n",
    "inputs = {k: hg.nodes[k].data['h'].to(device) for k in hg.ntypes}\n",
    "labels = labels.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    out_dict = model(hg, inputs)\n",
    "    out = out_dict['paper']\n",
    "    loss = F.cross_entropy(out[train_mask], labels[train_mask])\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# ‚úÖ Inference on Paper Nodes\n",
    "# ==============================\n",
    "model.eval()\n",
    "out = model(hg, inputs)['paper']\n",
    "preds = out.argmax(dim=1)\n",
    "print(\"Predictions:\", preds.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
