{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a8a614-b66d-411d-b353-b232881353c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ematos/miniconda3/envs/fn4.pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# ðŸ“š Import Libraries\n",
    "# =======================\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d181fa-6165-4a95-85cf-0bc45e264aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'available devices: {torch.cuda.device_count()}')\n",
    "print(f'current device: { torch.cuda.current_device()}')\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0d618e-1828-45c0-91c7-ddbc377d6ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 38.76it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book/agentivally uses,book/agentivally causes,book/agentivally has as agentive cause,book/agentivally has as agentive experience,book/is\n"
     ]
    }
   ],
   "source": [
    "model_name = 'VMware/open-llama-7b-open-instruct'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map='sequential')\n",
    "\n",
    "prompt_template = \"\"\"Given your profound knowledge about word (lexical items), I'll give you a list of relations and a word. \n",
    "Please, for each relation you generate a list of words (lexical items) related to the given word. Generate just relation that make sense\n",
    "from a human point of view. Don't use metaphorical or mythological relations.\n",
    "This is the list of relations, separated by commas: is affected agentively by,agentivally causes,agentivally uses,has as agentive cause,has as agentive experience,is associated with,is caused agentively by,is caused by,is caused intentionally by,causes emotion,causes infection,causes naturally,causes reaction,causes,constitutively uses,contains,is created by,is derived from,is found in,has as attribute,has as building part,has as colour,has as effect,has as instrument,has as member,has as origin,has as part,has as source,has as typical location,has diagnostic by,has influence on,is idiosincrasy of,includes,is a instance of,is a follower of,is a kind of,is activity of,is made of,is regulated by,is the ability of,is the activity of,is the activity performed in,is the habit of,is the purpose of,is the workplace of,kinship,lives in,made of,is measured by,measures,is part of,precedes,produces naturally,quantifies,is generally related to,is resolved by,results from,is the result state of,results in,is subtype of,is a symbol of,is typical of,is used against,is used as,is used by,is used during,is used for,is valued by,is a vice of.\n",
    "Note, please, that not all word will use all relations. Give me just those that make sense from a human point of view.\n",
    "The following instruction presents the given word. Please generate the related words and put each one in one line with the answer in the format \"given word - related word\". \n",
    "Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\"\"\n",
    "\n",
    "prompt = \"book\"\n",
    "\n",
    "inputt = prompt_template.format(instruction= prompt)\n",
    "input_ids = tokenizer(inputt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "output1 = model.generate(input_ids, max_length=512)\n",
    "input_length = input_ids.shape[1]\n",
    "output1 = output1[:, input_length:]\n",
    "output = tokenizer.decode(output1[0])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb01b8-68e0-4064-bcf3-497ad5f98e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.PyTorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
