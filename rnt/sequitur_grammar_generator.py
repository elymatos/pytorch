#!/usr/bin/env python3
"""
SEQUITUR Grammar Generator with Rule Usage Tracking

This script reads sequences from an input file, processes them with scikit-sequitur
to create a grammar that represents all sequences, and writes the grammar to an output file
along with rule usage statistics for weighting connections in a relational network.

Usage:
    python sequitur_grammar_generator.py input_sequences.txt grammar.txt
"""

import sys
import os
import re
from sksequitur import parse
from collections import Counter


def read_sequences(filename):
    """Read sequences from a text file, one sequence per line."""
    sequences = []
    with open(filename, 'r') as f:
        for line in f:
            line = line.strip()
            if line:  # Skip empty lines
                sequences.append(line)
    return sequences


def process_sequences(sequences):
    """Process a list of sequences and create a combined grammar."""
    # Join all sequences with a special separator
    separator = "ยง"

    # Ensure separator isn't in any sequence
    for seq in sequences:
        if separator in seq:
            raise ValueError(
                f"Separator character '{separator}' found in input sequence. Choose a different separator.")

    # Join the sequences
    combined_text = separator.join(sequences)

    # Parse with scikit-sequitur
    grammar = parse(combined_text)

    return grammar, separator


def count_rule_usage(grammar_str):
    """
    Count how many times each rule appears in the grammar.
    Returns a dictionary mapping rule IDs to usage counts.
    """
    usage_counts = Counter()
    rule_pattern = re.compile(r'(\d+) -> (.*)')

    # Extract all rules
    rules = {}
    for line in grammar_str.strip().split('\n'):
        match = rule_pattern.match(line)
        if match:
            rule_id, expansion = match.groups()
            rules[rule_id] = expansion.split()

    # Count occurrences of each rule in all expansions
    for rule_id, expansion in rules.items():
        for symbol in expansion:
            if symbol in rules:  # It's a rule reference
                usage_counts[symbol] += 1

    return usage_counts


def analyze_grammar(grammar):
    """
    Analyze the grammar to extract rules and their usage statistics.
    Returns a tuple (rules_dict, usage_counts).
    """
    grammar_str = str(grammar)

    # Extract rules with their expansions
    rules = {}
    rule_pattern = re.compile(r'(\d+) -> (.*)')

    for line in grammar_str.strip().split('\n'):
        match = rule_pattern.match(line)
        if match:
            rule_id, expansion = match.groups()
            rules[rule_id] = expansion.split()

    # Count rule usage in expansion parts
    usage_counts = Counter()

    for rule_id, expansion in rules.items():
        for symbol in expansion:
            if symbol in rules:  # It's a rule reference
                usage_counts[symbol] += 1

    return rules, usage_counts


def write_grammar_with_usage(grammar, output_file, separator=None):
    """Write the grammar to an output file with rule usage statistics."""
    grammar_str = str(grammar)
    rules, usage_counts = analyze_grammar(grammar)

    # Find max usage count for normalization
    max_count = max(usage_counts.values()) if usage_counts else 1

    with open(output_file, 'w') as f:
        # Write header with info
        f.write("# Grammar generated by scikit-sequitur with rule usage counts\n")
        f.write("# Format: rule -> expansion (space-separated symbols) [usage_count, weight]\n")
        if separator:
            f.write(f"# Note: '{separator}' is used as a sequence separator\n")
        f.write("# Weights are normalized between 0.1 and 1.0 based on usage frequency\n")
        f.write("\n")

        # Write each rule with usage statistics
        rule_pattern = re.compile(r'(\d+) -> (.*)')

        for line in grammar_str.strip().split('\n'):
            match = rule_pattern.match(line)
            if match:
                rule_id, expansion = match.groups()

                # Get usage count for this rule
                count = usage_counts.get(rule_id, 0)

                # Calculate a weight between 0.1 and 1.0
                # Rules that are never referenced still get a minimum weight of 0.1
                weight = 0.1 + (0.9 * (count / max_count)) if max_count > 0 else 0.1

                # Write with usage statistics
                f.write(f"{line} [count={count}, weight={weight:.2f}]\n")

        # Add a section with summary statistics for easier reference
        f.write("\n# Usage Statistics Summary\n")
        f.write("# Format: rule_id: usage_count, normalized_weight\n")

        for rule_id in sorted(rules.keys(), key=lambda x: int(x)):
            count = usage_counts.get(rule_id, 0)
            weight = 0.1 + (0.9 * (count / max_count)) if max_count > 0 else 0.1
            f.write(f"# {rule_id}: {count}, {weight:.2f}\n")

        # Write raw rule expansion reference for easier parsing
        f.write("\n# Raw Rule Expansions\n")
        for rule_id, expansion in rules.items():
            f.write(f"# {rule_id}: {' '.join(expansion)}\n")


def main():
    # Check command line arguments
    if len(sys.argv) != 3:
        print("Usage: python sequitur_grammar_generator.py <input_file> <output_file>")
        return 1

    input_file = sys.argv[1]
    output_file = sys.argv[2]

    # Check if input file exists
    if not os.path.exists(input_file):
        print(f"Error: Input file '{input_file}' not found.")
        return 1

    try:
        # Read the sequences
        print(f"Reading sequences from '{input_file}'...")
        sequences = read_sequences(input_file)
        print(f"Read {len(sequences)} sequences.")

        # Process sequences
        print("Processing sequences with scikit-sequitur...")
        grammar, separator = process_sequences(sequences)

        # Analyze the grammar for rule usage
        print("Analyzing rule usage statistics...")

        # Write grammar with usage statistics to output file
        print(f"Writing grammar to '{output_file}'...")
        write_grammar_with_usage(grammar, output_file, separator)

        print("Done!")

    except Exception as e:
        print(f"Error: {e}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())