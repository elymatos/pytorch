{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ff446a",
   "metadata": {},
   "source": [
    "# üß† L1 Bigram Predictor with BERT Embeddings\n",
    "This notebook builds a prototype predictive coding layer for bigram prediction using BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6798b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a1e1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Load your text file (one sentence per line)\n",
    "file_path = \"sample.txt\"  # Replace with your path\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b922a522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bigrams: 71\n"
     ]
    }
   ],
   "source": [
    "# üîó Extract bigrams\n",
    "def extract_bigrams(sentences: List[str]) -> List[Tuple[str, str]]:\n",
    "    bigrams = []\n",
    "    for sent in sentences:\n",
    "        words = sent.strip().split()\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "        for i in range(len(words) - 1):\n",
    "            bigrams.append((words[i], words[i+1]))\n",
    "    return bigrams\n",
    "\n",
    "bigrams = extract_bigrams(sentences)\n",
    "print(\"Total bigrams:\", len(bigrams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9504a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ü§ñ Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dae474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid bigrams: 40\n"
     ]
    }
   ],
   "source": [
    "# üß† Encode bigrams with BERT (CLS token) - only if each word maps to one token\n",
    "valid_bigrams = []\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for w1, w2 in bigrams[:100]:  # Limit to first 100 for performance\n",
    "    t1 = tokenizer.tokenize(w1)\n",
    "    t2 = tokenizer.tokenize(w2)\n",
    "    if len(t1) == 1 and len(t2) == 1:\n",
    "        input_ids = tokenizer.encode(w1, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        with torch.no_grad():\n",
    "            output = bert_model(input_ids).last_hidden_state\n",
    "        emb_input = output[:, 0, :]  # CLS token\n",
    "        inputs.append(emb_input.squeeze(0))\n",
    "        target_id = tokenizer.convert_tokens_to_ids(t2[0])\n",
    "        targets.append(target_id)\n",
    "        valid_bigrams.append((w1, w2))\n",
    "\n",
    "X = torch.stack(inputs)\n",
    "y = torch.tensor(targets)\n",
    "print(\"Valid bigrams:\", len(valid_bigrams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d825a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Word</th>\n",
       "      <th>Target Word</th>\n",
       "      <th>Target Token ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o</td>\n",
       "      <td>homem</td>\n",
       "      <td>40066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>homem</td>\n",
       "      <td>tomou</td>\n",
       "      <td>88954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tomou</td>\n",
       "      <td>o</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o</td>\n",
       "      <td>caf√©</td>\n",
       "      <td>34551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>caf√©</td>\n",
       "      <td>da</td>\n",
       "      <td>10143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Input Word Target Word  Target Token ID\n",
       "0          o       homem            40066\n",
       "1      homem       tomou            88954\n",
       "2      tomou           o              183\n",
       "3          o        caf√©            34551\n",
       "4       caf√©          da            10143"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Input Word\": [w1 for w1, _ in valid_bigrams],\n",
    "    \"Target Word\": [w2 for _, w2 in valid_bigrams],\n",
    "    \"Target Token ID\": y.tolist()\n",
    "})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23d71483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÆ Define Bigram Predictor Model\n",
    "class BigramPredictor(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.predictor = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.unsqueeze(1)  # Add time dim: [B, T=1, D]\n",
    "        _, h_n = self.rnn(embeddings)\n",
    "        return self.predictor(h_n.squeeze(0))  # [B, vocab]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de1aa394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 11.6881\n",
      "Epoch 2: Loss = 11.4198\n",
      "Epoch 3: Loss = 11.1520\n",
      "Epoch 4: Loss = 10.8684\n",
      "Epoch 5: Loss = 10.5585\n",
      "Epoch 6: Loss = 10.2172\n",
      "Epoch 7: Loss = 9.8431\n",
      "Epoch 8: Loss = 9.4371\n",
      "Epoch 9: Loss = 9.0016\n",
      "Epoch 10: Loss = 8.5394\n",
      "Epoch 11: Loss = 8.0537\n",
      "Epoch 12: Loss = 7.5477\n",
      "Epoch 13: Loss = 7.0246\n",
      "Epoch 14: Loss = 6.4883\n",
      "Epoch 15: Loss = 5.9442\n",
      "Epoch 16: Loss = 5.3995\n",
      "Epoch 17: Loss = 4.8654\n",
      "Epoch 18: Loss = 4.3576\n",
      "Epoch 19: Loss = 3.8964\n",
      "Epoch 20: Loss = 3.5023\n",
      "Epoch 21: Loss = 3.1862\n",
      "Epoch 22: Loss = 2.9431\n",
      "Epoch 23: Loss = 2.7558\n",
      "Epoch 24: Loss = 2.6058\n",
      "Epoch 25: Loss = 2.4796\n",
      "Epoch 26: Loss = 2.3697\n",
      "Epoch 27: Loss = 2.2720\n",
      "Epoch 28: Loss = 2.1846\n",
      "Epoch 29: Loss = 2.1061\n",
      "Epoch 30: Loss = 2.0350\n",
      "Epoch 31: Loss = 1.9700\n",
      "Epoch 32: Loss = 1.9097\n",
      "Epoch 33: Loss = 1.8528\n",
      "Epoch 34: Loss = 1.7981\n",
      "Epoch 35: Loss = 1.7448\n",
      "Epoch 36: Loss = 1.6925\n",
      "Epoch 37: Loss = 1.6408\n",
      "Epoch 38: Loss = 1.5898\n",
      "Epoch 39: Loss = 1.5391\n",
      "Epoch 40: Loss = 1.4887\n",
      "Epoch 41: Loss = 1.4384\n",
      "Epoch 42: Loss = 1.3884\n",
      "Epoch 43: Loss = 1.3387\n",
      "Epoch 44: Loss = 1.2897\n",
      "Epoch 45: Loss = 1.2419\n",
      "Epoch 46: Loss = 1.1956\n",
      "Epoch 47: Loss = 1.1509\n",
      "Epoch 48: Loss = 1.1079\n",
      "Epoch 49: Loss = 1.0667\n",
      "Epoch 50: Loss = 1.0271\n",
      "Epoch 51: Loss = 0.9888\n",
      "Epoch 52: Loss = 0.9518\n",
      "Epoch 53: Loss = 0.9159\n",
      "Epoch 54: Loss = 0.8812\n",
      "Epoch 55: Loss = 0.8477\n",
      "Epoch 56: Loss = 0.8156\n",
      "Epoch 57: Loss = 0.7847\n",
      "Epoch 58: Loss = 0.7550\n",
      "Epoch 59: Loss = 0.7265\n",
      "Epoch 60: Loss = 0.6992\n",
      "Epoch 61: Loss = 0.6731\n",
      "Epoch 62: Loss = 0.6481\n",
      "Epoch 63: Loss = 0.6244\n",
      "Epoch 64: Loss = 0.6020\n",
      "Epoch 65: Loss = 0.5811\n",
      "Epoch 66: Loss = 0.5615\n",
      "Epoch 67: Loss = 0.5434\n",
      "Epoch 68: Loss = 0.5265\n",
      "Epoch 69: Loss = 0.5109\n",
      "Epoch 70: Loss = 0.4964\n",
      "Epoch 71: Loss = 0.4828\n",
      "Epoch 72: Loss = 0.4701\n",
      "Epoch 73: Loss = 0.4582\n",
      "Epoch 74: Loss = 0.4470\n",
      "Epoch 75: Loss = 0.4366\n",
      "Epoch 76: Loss = 0.4269\n",
      "Epoch 77: Loss = 0.4179\n",
      "Epoch 78: Loss = 0.4096\n",
      "Epoch 79: Loss = 0.4018\n",
      "Epoch 80: Loss = 0.3947\n",
      "Epoch 81: Loss = 0.3880\n",
      "Epoch 82: Loss = 0.3817\n",
      "Epoch 83: Loss = 0.3758\n",
      "Epoch 84: Loss = 0.3703\n",
      "Epoch 85: Loss = 0.3651\n",
      "Epoch 86: Loss = 0.3603\n",
      "Epoch 87: Loss = 0.3558\n",
      "Epoch 88: Loss = 0.3516\n",
      "Epoch 89: Loss = 0.3477\n",
      "Epoch 90: Loss = 0.3441\n",
      "Epoch 91: Loss = 0.3408\n",
      "Epoch 92: Loss = 0.3376\n",
      "Epoch 93: Loss = 0.3347\n",
      "Epoch 94: Loss = 0.3320\n",
      "Epoch 95: Loss = 0.3294\n",
      "Epoch 96: Loss = 0.3270\n",
      "Epoch 97: Loss = 0.3247\n",
      "Epoch 98: Loss = 0.3226\n",
      "Epoch 99: Loss = 0.3206\n",
      "Epoch 100: Loss = 0.3187\n",
      "Epoch 101: Loss = 0.3169\n",
      "Epoch 102: Loss = 0.3152\n",
      "Epoch 103: Loss = 0.3136\n",
      "Epoch 104: Loss = 0.3121\n",
      "Epoch 105: Loss = 0.3106\n",
      "Epoch 106: Loss = 0.3092\n",
      "Epoch 107: Loss = 0.3079\n",
      "Epoch 108: Loss = 0.3066\n",
      "Epoch 109: Loss = 0.3054\n",
      "Epoch 110: Loss = 0.3043\n",
      "Epoch 111: Loss = 0.3032\n",
      "Epoch 112: Loss = 0.3021\n",
      "Epoch 113: Loss = 0.3011\n",
      "Epoch 114: Loss = 0.3001\n",
      "Epoch 115: Loss = 0.2992\n",
      "Epoch 116: Loss = 0.2983\n",
      "Epoch 117: Loss = 0.2974\n",
      "Epoch 118: Loss = 0.2966\n",
      "Epoch 119: Loss = 0.2958\n",
      "Epoch 120: Loss = 0.2950\n",
      "Epoch 121: Loss = 0.2943\n",
      "Epoch 122: Loss = 0.2936\n",
      "Epoch 123: Loss = 0.2929\n",
      "Epoch 124: Loss = 0.2923\n",
      "Epoch 125: Loss = 0.2916\n",
      "Epoch 126: Loss = 0.2910\n",
      "Epoch 127: Loss = 0.2904\n",
      "Epoch 128: Loss = 0.2898\n",
      "Epoch 129: Loss = 0.2893\n",
      "Epoch 130: Loss = 0.2887\n",
      "Epoch 131: Loss = 0.2882\n",
      "Epoch 132: Loss = 0.2877\n",
      "Epoch 133: Loss = 0.2872\n",
      "Epoch 134: Loss = 0.2867\n",
      "Epoch 135: Loss = 0.2863\n",
      "Epoch 136: Loss = 0.2858\n",
      "Epoch 137: Loss = 0.2854\n",
      "Epoch 138: Loss = 0.2849\n",
      "Epoch 139: Loss = 0.2845\n",
      "Epoch 140: Loss = 0.2841\n",
      "Epoch 141: Loss = 0.2837\n",
      "Epoch 142: Loss = 0.2833\n",
      "Epoch 143: Loss = 0.2830\n",
      "Epoch 144: Loss = 0.2826\n",
      "Epoch 145: Loss = 0.2823\n",
      "Epoch 146: Loss = 0.2819\n",
      "Epoch 147: Loss = 0.2816\n",
      "Epoch 148: Loss = 0.2813\n",
      "Epoch 149: Loss = 0.2809\n",
      "Epoch 150: Loss = 0.2806\n",
      "Epoch 151: Loss = 0.2803\n",
      "Epoch 152: Loss = 0.2800\n",
      "Epoch 153: Loss = 0.2797\n",
      "Epoch 154: Loss = 0.2795\n",
      "Epoch 155: Loss = 0.2792\n",
      "Epoch 156: Loss = 0.2789\n",
      "Epoch 157: Loss = 0.2786\n",
      "Epoch 158: Loss = 0.2784\n",
      "Epoch 159: Loss = 0.2781\n",
      "Epoch 160: Loss = 0.2779\n",
      "Epoch 161: Loss = 0.2776\n",
      "Epoch 162: Loss = 0.2774\n",
      "Epoch 163: Loss = 0.2772\n",
      "Epoch 164: Loss = 0.2769\n",
      "Epoch 165: Loss = 0.2767\n",
      "Epoch 166: Loss = 0.2765\n",
      "Epoch 167: Loss = 0.2763\n",
      "Epoch 168: Loss = 0.2761\n",
      "Epoch 169: Loss = 0.2759\n",
      "Epoch 170: Loss = 0.2757\n",
      "Epoch 171: Loss = 0.2755\n",
      "Epoch 172: Loss = 0.2753\n",
      "Epoch 173: Loss = 0.2751\n",
      "Epoch 174: Loss = 0.2749\n",
      "Epoch 175: Loss = 0.2747\n",
      "Epoch 176: Loss = 0.2745\n",
      "Epoch 177: Loss = 0.2743\n",
      "Epoch 178: Loss = 0.2742\n",
      "Epoch 179: Loss = 0.2740\n",
      "Epoch 180: Loss = 0.2738\n",
      "Epoch 181: Loss = 0.2736\n",
      "Epoch 182: Loss = 0.2735\n",
      "Epoch 183: Loss = 0.2733\n",
      "Epoch 184: Loss = 0.2732\n",
      "Epoch 185: Loss = 0.2730\n",
      "Epoch 186: Loss = 0.2729\n",
      "Epoch 187: Loss = 0.2727\n",
      "Epoch 188: Loss = 0.2726\n",
      "Epoch 189: Loss = 0.2724\n",
      "Epoch 190: Loss = 0.2723\n",
      "Epoch 191: Loss = 0.2721\n",
      "Epoch 192: Loss = 0.2720\n",
      "Epoch 193: Loss = 0.2718\n",
      "Epoch 194: Loss = 0.2717\n",
      "Epoch 195: Loss = 0.2716\n",
      "Epoch 196: Loss = 0.2714\n",
      "Epoch 197: Loss = 0.2713\n",
      "Epoch 198: Loss = 0.2712\n",
      "Epoch 199: Loss = 0.2711\n",
      "Epoch 200: Loss = 0.2709\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è Train on the sample\n",
    "model = BigramPredictor(embed_dim=768, hidden_dim=256, vocab_size=tokenizer.vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(X)\n",
    "    loss = loss_fn(preds, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77194b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word: carro\n",
      "Predicted next word: novo\n"
     ]
    }
   ],
   "source": [
    "# üß™ Test the bigram predictor with a new word\n",
    "def predict_next_word(word: str):\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    if len(tokens) != 1:\n",
    "        print(f\"'{word}' cannot be tokenized as a single BERT token.\")\n",
    "        return\n",
    "    input_ids = tokenizer.encode(word, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    with torch.no_grad():\n",
    "        embedding = bert_model(input_ids).last_hidden_state[:, 0, :]  # CLS token\n",
    "        logits = model(embedding)\n",
    "        predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_id])[0]\n",
    "        print(f\"Input word: {word}\")\n",
    "        print(f\"Predicted next word: {predicted_token}\")\n",
    "\n",
    "# üîç Try it\n",
    "predict_next_word(\"carro\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.Pytorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
