{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17598fdc-d683-43ae-b594-5d307fb1c9e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 301)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:301\u001b[0;36m\u001b[0m\n\u001b[0;31m    if matched:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Enhanced HTPC (Hierarchical Temporal Predictive Coding) Implementation\n",
    "# Combines original implementation with enhancements for:\n",
    "# - Probabilistic pattern selection\n",
    "# - Incremental learning\n",
    "# - Enhanced recursive processing\n",
    "# - Evaluation metrics\n",
    "# - Higher-order pattern recognition\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "MODEL_FILE = \"htpc_model.json\"\n",
    "MULTIWORDS_FILE = \"multiwords.txt\"\n",
    "POS_DICTIONARY_FILE = \"pos_dict.txt\"\n",
    "TEST_SENTENCE = \"a|spec|pron mulher|noun preparou|fin o|spec almo√ßo|noun e|conj o|spec|pron almo√ßo|noun esfriou|fin r√°pido|adj\"\n",
    "NOISE_PROBABILITY = 0.1  # Increased from 0.0 to add some exploration\n",
    "\n",
    "# === MULTIWORDS ===\n",
    "def load_multiwords(path):\n",
    "    multiwords = []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                mw = line.strip().lower()\n",
    "                if mw:\n",
    "                    multiwords.append((mw, mw.replace(\" \", \"_\")))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Aviso: Arquivo de multiwords n√£o encontrado.\")\n",
    "    return multiwords\n",
    "\n",
    "def replace_multiwords(text, multiword_list):\n",
    "    for original, replacement in multiword_list:\n",
    "        text = text.replace(original, replacement)\n",
    "    return text\n",
    "\n",
    "# === DICION√ÅRIO DE CLASSES GRAMATICAIS ===\n",
    "def load_pos_dictionary(path):\n",
    "    pos_dict = {}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts:\n",
    "                    token = parts[0]\n",
    "                    tags = parts[1:]\n",
    "                    pos_dict[token] = tags\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Arquivo de classes gramaticais n√£o encontrado: pos_dict.txt\")\n",
    "    return pos_dict\n",
    "\n",
    "# === TOKENIZA√á√ÉO COM SUPORTE A M√öLTIPLOS TOKENS ===\n",
    "def normalize_token(token):\n",
    "    return re.sub(r\"[.,!?;:()\\[\\]{}\\\"']\", \"\", token.lower())\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [[normalize_token(opt) for opt in tok.split(\"|\")] for tok in sentence.strip().split() if tok]\n",
    "\n",
    "# === MODELO HTPC ===\n",
    "def load_model(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        model = json.load(f)\n",
    "\n",
    "    token_transitions = model['token_transitions']\n",
    "    bigram_memory = {\n",
    "        tuple(key.split(\"|||\")): value\n",
    "        for key, value in model['bigram_memory'].items()\n",
    "    }\n",
    "    phrase_memory = {\n",
    "        tuple(tuple(pair.split(\"__\")) for pair in key.split(\"|||\")): value\n",
    "        for key, value in model['phrase_memory'].items()\n",
    "    }\n",
    "    phrase_hierarchy = model.get('phrase_hierarchy', {})\n",
    "    return token_transitions, bigram_memory, phrase_memory, phrase_hierarchy\n",
    "\n",
    "# === ORIGINAL MEM√ìRIA DE CONTEXTO COM STACK ===\n",
    "class ContextBuffer:\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "\n",
    "    def push(self, key, value):\n",
    "        self.stack.append((key, value))\n",
    "\n",
    "    def pop(self):\n",
    "        if self.stack:\n",
    "            return self.stack.pop()\n",
    "        return None\n",
    "\n",
    "    def top(self, key):\n",
    "        for k, v in reversed(self.stack):\n",
    "            if k == key:\n",
    "                return v\n",
    "        return None\n",
    "\n",
    "    def pop_key(self, key):\n",
    "        for i in range(len(self.stack) - 1, -1, -1):\n",
    "            if self.stack[i][0] == key:\n",
    "                return self.stack.pop(i)\n",
    "        return None\n",
    "\n",
    "    def debug(self):\n",
    "        return list(self.stack)\n",
    "\n",
    "context_buffer = ContextBuffer()\n",
    "\n",
    "# === PILHA DE CHUNKS PARA SUPORTE A RECURS√ÉO (ORIGINAL) ===\n",
    "class ChunkStack:\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "\n",
    "    def push_chunk(self, chunk):\n",
    "        self.stack.append(chunk)\n",
    "\n",
    "    def pop_chunk(self):\n",
    "        return self.stack.pop() if self.stack else None\n",
    "\n",
    "    def top_chunk(self):\n",
    "        return self.stack[-1] if self.stack else None\n",
    "\n",
    "    def debug(self):\n",
    "        return list(self.stack)\n",
    "\n",
    "# Original chunk stack instance\n",
    "chunk_stack = ChunkStack()\n",
    "\n",
    "# === FUN√á√ÉO DE FEEDBACK TOP-DOWN ===\n",
    "def dynamic_feedback_for_context(context_tokens, phrase_memory, phrase_hierarchy, noise_probability=0.0, chunk_stack_local=None):\n",
    "    \"\"\"Enhanced feedback function that uses local chunk stack\"\"\"\n",
    "    # Incorporar expectativas do chunk_stack tamb√©m\n",
    "    all_context = list(context_tokens)\n",
    "    \n",
    "    # Use the provided chunk stack if available\n",
    "    if chunk_stack_local and hasattr(chunk_stack_local, 'stack'):\n",
    "        for chunk in chunk_stack_local.stack:\n",
    "            all_context += chunk.split()\n",
    "    # Fallback to global chunk stack\n",
    "    elif chunk_stack.stack:\n",
    "        for chunk in chunk_stack.stack:\n",
    "            all_context += chunk.split()\n",
    "            \n",
    "    expectations = set()\n",
    "    for phrase in phrase_memory:\n",
    "        flat = [phrase[0][0]] + [pair[1] for pair in phrase]\n",
    "        for i in range(len(flat) - 1):\n",
    "            match_len = i + 1\n",
    "            if i < len(flat) and match_len <= len(all_context) and flat[:match_len] == all_context[-match_len:]:\n",
    "                next_token = flat[i + 1]\n",
    "                if random.random() > noise_probability:\n",
    "                    expectations.add(next_token)\n",
    "                    \n",
    "        # Check phrase hierarchy\n",
    "        hierarchy_key = \"|||\".join([f\"{a}__{b}\" for a, b in phrase])\n",
    "        if hierarchy_key in phrase_hierarchy:\n",
    "            for item in phrase_hierarchy[hierarchy_key]:\n",
    "                if isinstance(item, tuple) and len(item) == 2:\n",
    "                    phrase_b, combined = item\n",
    "                    combined_tokens = combined.split()\n",
    "                    for i in range(len(combined_tokens) - 1):\n",
    "                        match_len = i + 1\n",
    "                        if match_len <= len(all_context) and combined_tokens[:match_len] == all_context[-match_len:]:\n",
    "                            next_token = combined_tokens[i + 1]\n",
    "                            if random.random() > noise_probability:\n",
    "                                expectations.add(next_token)\n",
    "    \n",
    "    if expectations:\n",
    "        print(f\"  üëÅÔ∏è Top-down expectations: {expectations}\")\n",
    "    \n",
    "    return expectations\n",
    "\n",
    "# === COMPARA√á√ÉO DE FRASES E N√çVEIS ===\n",
    "def match_all_phrases(tokens, phrase_memory, pos_dict):\n",
    "    # Converte sequ√™ncia de tokens para sequ√™ncia de POS (n√≠vel superficial)\n",
    "    token_pos_sequence = []\n",
    "    for tok in tokens:\n",
    "        if tok in pos_dict:\n",
    "            token_pos_sequence.append(pos_dict[tok][0])  # usa apenas a primeira classe como simplifica√ß√£o\n",
    "        else:\n",
    "            token_pos_sequence.append(tok)  # fallback para token literal\n",
    "    matched_phrases = []\n",
    "    n = len(tokens)\n",
    "    for phrase in phrase_memory:\n",
    "        phrase_len = len(phrase) + 1\n",
    "        for i in range(n - phrase_len + 1):\n",
    "            test_bigrams = tuple((token_pos_sequence[j], token_pos_sequence[j + 1]) for j in range(i, i + phrase_len - 1))\n",
    "            if test_bigrams == phrase:\n",
    "                phrase_str = \" \".join([token_pos_sequence[i]] + [token_pos_sequence[i + k + 1] for k in range(len(phrase))])\n",
    "                matched_phrases.append((phrase, phrase_str))\n",
    "    return matched_phrases\n",
    "\n",
    "# === RECONHECIMENTO ORIGINAL ===\n",
    "def recognize_patterns(token_matrix, token_transitions, bigram_memory, phrase_memory, phrase_hierarchy, pos_dict, noise_probability=0.0):\n",
    "    \"\"\"\n",
    "    SIMPLIFIED recognition function focused on properly maintaining context\n",
    "    \"\"\"\n",
    "    # Create a brand new context buffer for this recognition\n",
    "    global context_buffer\n",
    "    context_buffer = ContextBuffer()\n",
    "    \n",
    "    patterns = []\n",
    "    current_pattern = []\n",
    "    level3_links = []\n",
    "\n",
    "    print(f\"\\n=== Starting Pattern Recognition with {len(token_matrix)} tokens ===\")\n",
    "    \n",
    "    # Process each token position\n",
    "    for i in range(len(token_matrix)):\n",
    "        curr_tokens = token_matrix[i]\n",
    "        prev_tokens = token_matrix[i - 1] if i > 0 else [None]\n",
    "\n",
    "        print(f\"\\nüîé Posi√ß√£o {i}: op√ß√µes = {curr_tokens}\")\n",
    "        print(f\"üß† Contexto atual: {context_buffer.debug()}\")\n",
    "        \n",
    "        # FOCUS: Proper subject detection and context updating\n",
    "        # Try each token at this position\n",
    "        matched = False\n",
    "        for curr_token in curr_tokens:\n",
    "            # Get POS tags if available\n",
    "            tags = pos_dict.get(curr_token, [])\n",
    "            \n",
    "            # SUBJECT DETECTION\n",
    "            if any(tag in ['noun', 'pron', 'spec'] for tag in tags):\n",
    "                # Direct debug to stdout\n",
    "                print(f\"  PUSHING SUBJECT: {curr_token}\")\n",
    "                # Actually update the context\n",
    "                context_buffer.stack.append(('subject', curr_token))\n",
    "            \n",
    "            # VERB DETECTION - LINK TO SUBJECT\n",
    "            if any(tag in ['verb', 'fin'] for tag in tags):\n",
    "                # Find subject in context if present\n",
    "                subject = None\n",
    "                for k, v in reversed(context_buffer.stack):\n",
    "                    if k == 'subject':\n",
    "                        subject = v\n",
    "                        break\n",
    "                \n",
    "                if subject:\n",
    "                    link = f\"{subject} + {curr_token}\"\n",
    "                    level3_links.append(link)\n",
    "                    print(f\"  SUBJECT-VERB LINK: {link}\")\n",
    "            \n",
    "            # Check if this is a valid transition\n",
    "            for prev_token in prev_tokens:\n",
    "                is_valid_transition = (\n",
    "                    token_transitions.get(prev_token) == curr_token or\n",
    "                    (prev_token, curr_token) in bigram_memory\n",
    "                )\n",
    "                \n",
    "                if is_valid_transition:\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if matched:\n",
    "                break\n",
    "        \n",
    "        # Process the matching result\n",
    "        if matched:\n",
    "            current_pattern.append(curr_tokens)\n",
    "        else:\n",
    "            if len(current_pattern) > 1:\n",
    "                flat = [tok[0] for tok in current_pattern]\n",
    "                matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "                patterns.append((flat.copy(), matched_phrases))\n",
    "            current_pattern = [curr_tokens]\n",
    "    \n",
    "    # Process final pattern if present\n",
    "    if len(current_pattern) > 1:\n",
    "        flat = [tok[0] for tok in current_pattern]\n",
    "        matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "        patterns.append((flat, matched_phrases))\n",
    "    \n",
    "    # Show final context and links\n",
    "    print(\"\\nüß† Contexto final:\", context_buffer.debug())\n",
    "    print(\"\\nüîç Liga√ß√µes de N√≠vel 3 (sujeito + verbo):\")\n",
    "    for link in level3_links:\n",
    "        print(f\"   üîó {link}\")\n",
    "    \n",
    "    return patterns\n",
    "        if matched:\n",
    "            current_pattern.append(curr_tokens)\n",
    "        else:\n",
    "            if len(current_pattern) > 1:\n",
    "                flat = [tok[0] for tok in current_pattern]\n",
    "                matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "                patterns.append((flat.copy(), matched_phrases))\n",
    "            current_pattern = [curr_tokens]\n",
    "    \n",
    "    # Process final pattern if present\n",
    "    if len(current_pattern) > 1:\n",
    "        flat = [tok[0] for tok in current_pattern]\n",
    "        matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "        patterns.append((flat, matched_phrases))\n",
    "    \n",
    "    # Show final context and links\n",
    "    print(\"\\nüß† Contexto final:\", context_buffer.debug())\n",
    "    print(\"\\nüîç Liga√ß√µes de N√≠vel 3 (sujeito + verbo):\")\n",
    "    for link in level3_links:\n",
    "        print(f\"   üîó {link}\")\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "            for prev_token in prev_tokens:\n",
    "                is_valid_transition = (\n",
    "                    token_transitions.get(prev_token) == curr_token or\n",
    "                    (prev_token, curr_token) in bigram_memory\n",
    "                )\n",
    "                context = [tok for sublist in current_pattern[-3:] for tok in sublist]\n",
    "                top_down = dynamic_feedback_for_context(context, phrase_memory, phrase_hierarchy, noise_probability, chunk_stack_local)\n",
    "                top_down_match = curr_token in top_down or not top_down\n",
    "\n",
    "                print(f\"  ‚Üí Testando: {prev_token} ‚Üí {curr_token} | v√°lido? {is_valid_transition}, esperado? {top_down_match}\")\n",
    "\n",
    "                if is_valid_transition and top_down_match:\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if matched:\n",
    "                break\n",
    "\n",
    "        if matched:\n",
    "            current_pattern.append(curr_tokens)\n",
    "        else:\n",
    "            if len(current_pattern) > 1:\n",
    "                flat = [tok[0] for tok in current_pattern]\n",
    "                flat_pos = [pos_dict.get(t, [t])[0] for t in flat]\n",
    "                matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "                if matched_phrases:\n",
    "                    chunk_stack_local.push_chunk(\" \".join(flat_pos))\n",
    "                    for _, phrase_str in matched_phrases:\n",
    "                        chunk_stack_local.push_chunk(phrase_str)\n",
    "                patterns.append((flat.copy(), matched_phrases))\n",
    "                print(f\"‚úîÔ∏è Padr√£o encerrado: {' '.join(flat)}\")\n",
    "            current_pattern = [curr_tokens]\n",
    "\n",
    "    if len(current_pattern) > 1:\n",
    "        flat = [tok[0] for tok in current_pattern]\n",
    "        flat_pos = [pos_dict.get(t, [t])[0] for t in flat]\n",
    "        matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "        if matched_phrases:\n",
    "            chunk_stack_local.push_chunk(\" \".join(flat_pos))\n",
    "        for _, phrase_str in matched_phrases:\n",
    "            chunk_stack_local.push_chunk(phrase_str)\n",
    "        patterns.append((flat, matched_phrases))\n",
    "\n",
    "    print(\"\\nüîç Liga√ß√µes de N√≠vel 3 (sujeito + verbo):\")\n",
    "    for link in level3_links:\n",
    "        print(f\"   üîó {link}\")\n",
    "\n",
    "    return patterns\n",
    "\n",
    "#############################################################\n",
    "# ENHANCEMENT 1: PROBABILISTIC PATTERN SELECTION\n",
    "#############################################################\n",
    "\n",
    "def probabilistic_pattern_selection(candidates, bigram_memory, phrase_memory, current_pattern):\n",
    "    \"\"\"\n",
    "    Select a token from candidates based on frequency counts in the model.\n",
    "    \n",
    "    Args:\n",
    "        candidates: List of candidate tokens\n",
    "        bigram_memory: Dictionary of bigram frequencies\n",
    "        phrase_memory: Dictionary of phrase frequencies\n",
    "        current_pattern: Current pattern being built\n",
    "        \n",
    "    Returns:\n",
    "        Selected token and its probability score\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return None, 0.0\n",
    "    \n",
    "    scores = {}\n",
    "    total_score = 0\n",
    "    \n",
    "    for token in candidates:\n",
    "        # Start with a base score\n",
    "        score = 1.0\n",
    "        \n",
    "        # Consider previous context (last few tokens)\n",
    "        context = [tok for sublist in current_pattern[-3:] for tok in sublist if sublist]\n",
    "        \n",
    "        # Add bigram influence\n",
    "        for prev_token in context:\n",
    "            bigram = (prev_token, token)\n",
    "            if bigram in bigram_memory:\n",
    "                score *= (1 + bigram_memory[bigram])\n",
    "        \n",
    "        # Add phrase influence\n",
    "        flat_context = [tok[0] for tok in current_pattern[-3:] if tok]\n",
    "        for phrase in phrase_memory:\n",
    "            flat_phrase = [phrase[0][0]] + [pair[1] for pair in phrase]\n",
    "            for i in range(min(len(flat_context), len(flat_phrase))):\n",
    "                if i < len(flat_context) and flat_context[-i:] == flat_phrase[:i] and i < len(flat_phrase)-1 and flat_phrase[i] == token:\n",
    "                    score *= (1 + phrase_memory[phrase])\n",
    "        \n",
    "        scores[token] = score\n",
    "        total_score += score\n",
    "    \n",
    "    # Normalize scores\n",
    "    if total_score > 0:\n",
    "        for token in scores:\n",
    "            scores[token] /= total_score\n",
    "    \n",
    "    # Select based on probability\n",
    "    if random.random() < 0.9:  # 90% follow probabilities, 10% explore\n",
    "        selected = max(scores.keys(), key=lambda x: scores[x])\n",
    "    else:\n",
    "        selected = random.choice(list(scores.keys()))\n",
    "    \n",
    "    return selected, scores[selected]\n",
    "\n",
    "def recognize_patterns_probabilistic(token_matrix, token_transitions, bigram_memory, \n",
    "                                   phrase_memory, phrase_hierarchy, pos_dict, \n",
    "                                   noise_probability=0.0):\n",
    "    \"\"\"\n",
    "    Enhanced recognition function with probabilistic selection\n",
    "    \"\"\"\n",
    "    patterns = []\n",
    "    current_pattern = []\n",
    "    level3_links = []\n",
    "    \n",
    "    for i in range(len(token_matrix)):\n",
    "        curr_tokens = token_matrix[i]\n",
    "        prev_tokens = token_matrix[i - 1] if i > 0 else [None]\n",
    "        \n",
    "        print(f\"üîé Posi√ß√£o {i}: op√ß√µes = {curr_tokens}\")\n",
    "        print(f\"üß† Contexto atual: {context_buffer.debug()}\")\n",
    "        print(f\"üìö Chunk stack: {chunk_stack.debug()}\")\n",
    "        \n",
    "        # Filter valid candidates\n",
    "        valid_candidates = []\n",
    "        for curr_token in curr_tokens:\n",
    "            tags = pos_dict.get(curr_token, [])\n",
    "            \n",
    "            # Track grammar elements as in the original\n",
    "            if any(tag in tags for tag in ['noun', 'pron', 'spec']):\n",
    "                context_buffer.push('subject', curr_token)\n",
    "                \n",
    "            if any(tag in tags for tag in ['verb', 'fin']):\n",
    "                subject = context_buffer.top('subject')\n",
    "                if subject:\n",
    "                    link = f\"{subject} + {curr_token}\"\n",
    "                    level3_links.append(link)\n",
    "                    print(f\"üîó N√≠vel 3: {link}\")\n",
    "                    if subject in curr_tokens:\n",
    "                        context_buffer.pop_key('subject')\n",
    "            \n",
    "            for prev_token in prev_tokens:\n",
    "                is_valid_transition = (\n",
    "                    token_transitions.get(prev_token) == curr_token or\n",
    "                    (prev_token, curr_token) in bigram_memory\n",
    "                )\n",
    "                context = [tok for sublist in current_pattern[-3:] for tok in sublist if sublist]\n",
    "                top_down = dynamic_feedback_for_context(context, phrase_memory, phrase_hierarchy, noise_probability)\n",
    "                top_down_match = curr_token in top_down or not top_down\n",
    "                \n",
    "                print(f\"  ‚Üí Testando: {prev_token} ‚Üí {curr_token} | v√°lido? {is_valid_transition}, esperado? {top_down_match}\")\n",
    "                \n",
    "                if is_valid_transition and top_down_match:\n",
    "                    valid_candidates.append(curr_token)\n",
    "                    break\n",
    "        \n",
    "        # Select probabilistically\n",
    "        if valid_candidates:\n",
    "            selected_token, score = probabilistic_pattern_selection(\n",
    "                valid_candidates, bigram_memory, phrase_memory, current_pattern\n",
    "            )\n",
    "            \n",
    "            if selected_token:\n",
    "                print(f\"  ‚Üí Selecionado: {selected_token} (score: {score:.4f})\")\n",
    "                current_pattern.append([selected_token])\n",
    "                matched = True\n",
    "            else:\n",
    "                matched = False\n",
    "        else:\n",
    "            matched = False\n",
    "            \n",
    "        # Process patterns as in the original\n",
    "        if not matched and len(current_pattern) > 1:\n",
    "            flat = [tok[0] for tok in current_pattern]\n",
    "            flat_pos = [pos_dict.get(t, [t])[0] for t in flat]\n",
    "            matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "            if matched_phrases:\n",
    "                chunk_stack.push_chunk(\" \".join(flat_pos))\n",
    "                for _, phrase_str in matched_phrases:\n",
    "                    chunk_stack.push_chunk(phrase_str)\n",
    "            patterns.append((flat.copy(), matched_phrases))\n",
    "            print(f\"‚úîÔ∏è Padr√£o encerrado: {' '.join(flat)}\")\n",
    "            current_pattern = [curr_tokens]\n",
    "\n",
    "    # Process any remaining pattern\n",
    "    if len(current_pattern) > 1:\n",
    "        flat = [tok[0] for tok in current_pattern]\n",
    "        flat_pos = [pos_dict.get(t, [t])[0] for t in flat]\n",
    "        matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "        if matched_phrases:\n",
    "            chunk_stack.push_chunk(\" \".join(flat_pos))\n",
    "            for _, phrase_str in matched_phrases:\n",
    "                chunk_stack.push_chunk(phrase_str)\n",
    "        patterns.append((flat, matched_phrases))\n",
    "\n",
    "    print(\"\\nüîç Liga√ß√µes de N√≠vel 3 (sujeito + verbo):\")\n",
    "    for link in level3_links:\n",
    "        print(f\"   üîó {link}\")\n",
    "\n",
    "    return patterns\n",
    "\n",
    "#############################################################\n",
    "# ENHANCEMENT 2: INCREMENTAL LEARNING\n",
    "#############################################################\n",
    "\n",
    "def expand_sequences(token_matrix):\n",
    "    \"\"\"\n",
    "    Expand a token matrix to all possible token sequences.\n",
    "    \n",
    "    Args:\n",
    "        token_matrix: List of token lists (each inner list contains options)\n",
    "        \n",
    "    Returns:\n",
    "        List of all possible token sequences\n",
    "    \"\"\"\n",
    "    return list(product(*token_matrix))\n",
    "\n",
    "def load_blacklist(path):\n",
    "    \"\"\"\n",
    "    Load blacklisted bigrams from a file.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to blacklist file\n",
    "        \n",
    "    Returns:\n",
    "        Set of blacklisted bigram tuples\n",
    "    \"\"\"\n",
    "    blacklist = set()\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = normalize_token(line.strip()).split()\n",
    "                if len(parts) == 2:\n",
    "                    blacklist.add(tuple(parts))\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Arquivo de blacklist n√£o encontrado.\")\n",
    "    return blacklist\n",
    "\n",
    "class HTCPModel:\n",
    "    \"\"\"\n",
    "    HTPC Model with incremental learning capabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path=None):\n",
    "        self.token_transitions = {}\n",
    "        self.bigram_memory = {}\n",
    "        self.phrase_memory = {}\n",
    "        self.phrase_hierarchy = {}\n",
    "        self.vocab = set()\n",
    "        self.training_count = 0\n",
    "        self.decay_factor = 0.99  # For time-based memory decay\n",
    "        \n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Load model from a JSON file.\"\"\"\n",
    "        with open(model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.token_transitions = model_data['token_transitions']\n",
    "        self.bigram_memory = {\n",
    "            tuple(key.split(\"|||\")): value\n",
    "            for key, value in model_data['bigram_memory'].items()\n",
    "        }\n",
    "        self.phrase_memory = {\n",
    "            tuple(tuple(pair.split(\"__\")) for pair in key.split(\"|||\")): value\n",
    "            for key, value in model_data['phrase_memory'].items()\n",
    "        }\n",
    "        self.phrase_hierarchy = model_data.get('phrase_hierarchy', {})\n",
    "        self.training_count = model_data.get('metadata', {}).get('num_sentences', 0)\n",
    "        \n",
    "        # Rebuild vocab\n",
    "        self.vocab = set(self.token_transitions.keys()) | set(t for b in self.bigram_memory for t in b)\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded with {self.training_count} previous training sequences.\")\n",
    "        \n",
    "    def save_model(self, output_path):\n",
    "        \"\"\"Save the current model to a JSON file.\"\"\"\n",
    "        model_data = {\n",
    "            'metadata': {\n",
    "                'trained_on': datetime.now().isoformat(),\n",
    "                'num_sentences': self.training_count,\n",
    "                'vocab_size': len(self.vocab),\n",
    "            },\n",
    "            'token_transitions': self.token_transitions,\n",
    "            'bigram_memory': {\n",
    "                f\"{k[0]}|||{k[1]}\": v for k, v in self.bigram_memory.items()\n",
    "            },\n",
    "            'phrase_memory': {\n",
    "                \"|||\".join([f\"{a}__{b}\" for (a, b) in k]): v\n",
    "                for k, v in self.phrase_memory.items()\n",
    "            },\n",
    "            'phrase_hierarchy': self.phrase_hierarchy\n",
    "        }\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(model_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Model saved to: {output_path}\")\n",
    "    \n",
    "    def apply_decay(self):\n",
    "        \"\"\"Apply time-based decay to memory structures.\"\"\"\n",
    "        for k in self.bigram_memory:\n",
    "            self.bigram_memory[k] *= self.decay_factor\n",
    "        \n",
    "        for k in self.phrase_memory:\n",
    "            self.phrase_memory[k] *= self.decay_factor\n",
    "    \n",
    "    def learn_sequence(self, sequence, blacklist=None, chunk_size=3):\n",
    "        \"\"\"Learn from a single sequence incrementally.\"\"\"\n",
    "        if blacklist is None:\n",
    "            blacklist = set()\n",
    "        \n",
    "        # Update token transitions\n",
    "        for i in range(len(sequence) - 1):\n",
    "            bigram = (sequence[i], sequence[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                self.token_transitions[sequence[i]] = sequence[i + 1]\n",
    "        \n",
    "        # Update bigram memory\n",
    "        for i in range(len(sequence) - 1):\n",
    "            bigram = (sequence[i], sequence[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                self.bigram_memory[bigram] = self.bigram_memory.get(bigram, 0) + 1\n",
    "        \n",
    "        # Update phrase memory\n",
    "        if len(sequence) >= chunk_size:\n",
    "            for i in range(len(sequence) - chunk_size + 1):\n",
    "                bigrams = [(sequence[j], sequence[j + 1]) for j in range(i, i + chunk_size - 1)]\n",
    "                if blacklist and any(bg in blacklist for bg in bigrams):\n",
    "                    continue\n",
    "                phrase = tuple(bigrams)\n",
    "                self.phrase_memory[phrase] = self.phrase_memory.get(phrase, 0) + 1\n",
    "        \n",
    "        # Update hierarchy\n",
    "        self._update_hierarchy()\n",
    "        \n",
    "        # Update vocab\n",
    "        self.vocab.update(sequence)\n",
    "        \n",
    "        # Increment training count\n",
    "        self.training_count += 1\n",
    "        \n",
    "        # Apply decay periodically\n",
    "        if self.training_count % 100 == 0:\n",
    "            self.apply_decay()\n",
    "    \n",
    "    def _update_hierarchy(self):\n",
    "        \"\"\"Update phrase hierarchy based on current phrase memory.\"\"\"\n",
    "        # This is computationally expensive, so we limit it\n",
    "        if self.training_count % 10 != 0:\n",
    "            return\n",
    "            \n",
    "        # Create fresh hierarchy\n",
    "        new_hierarchy = defaultdict(list)\n",
    "        \n",
    "        # Consider only the top N phrases by frequency\n",
    "        top_phrases = sorted(self.phrase_memory.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "        \n",
    "        for phrase_a, _ in top_phrases:\n",
    "            flat_a = [phrase_a[0][0]] + [pair[1] for pair in phrase_a]\n",
    "            for phrase_b, _ in top_phrases:\n",
    "                if phrase_a == phrase_b:\n",
    "                    continue\n",
    "                flat_b = [phrase_b[0][0]] + [pair[1] for pair in phrase_b]\n",
    "                if flat_a[-1] == flat_b[0]:\n",
    "                    combined = tuple(flat_a + flat_b[1:])\n",
    "                    key = \"|||\".join([f\"{a}__{b}\" for a, b in phrase_a])\n",
    "                    new_hierarchy[key].append((phrase_b, \" \".join(combined)))\n",
    "        \n",
    "        self.phrase_hierarchy = dict(new_hierarchy)\n",
    "\n",
    "def build_token_transitions(sequences, blacklist):\n",
    "    transitions = defaultdict(lambda: None)\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            bigram = (seq[i], seq[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                transitions[seq[i]] = seq[i + 1]\n",
    "    return dict(transitions)\n",
    "\n",
    "def build_bigram_memory(sequences, blacklist):\n",
    "    bigram_counts = defaultdict(int)\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            bigram = (seq[i], seq[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                bigram_counts[bigram] += 1\n",
    "    return dict(bigram_counts)\n",
    "\n",
    "def build_phrase_memory(sequences, chunk_size=3, blacklist=None):\n",
    "    phrase_counts = defaultdict(int)\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= chunk_size:\n",
    "            for i in range(len(seq) - chunk_size + 1):\n",
    "                bigrams = [(seq[j], seq[j + 1]) for j in range(i, i + chunk_size - 1)]\n",
    "                if blacklist and any(bg in blacklist for bg in bigrams):\n",
    "                    continue\n",
    "                phrase_counts[tuple(bigrams)] += 1\n",
    "    return dict(phrase_counts)\n",
    "\n",
    "def build_higher_order_chunks(phrase_memory):\n",
    "    level2 = defaultdict(list)\n",
    "    for phrase_a in phrase_memory:\n",
    "        flat_a = [phrase_a[0][0]] + [pair[1] for pair in phrase_a]\n",
    "        for phrase_b in phrase_memory:\n",
    "            if phrase_a == phrase_b:\n",
    "                continue\n",
    "            flat_b = [phrase_b[0][0]] + [pair[1] for pair in phrase_b]\n",
    "            if flat_a[-1] == flat_b[0]:\n",
    "                combined = tuple(flat_a + flat_b[1:])\n",
    "                level2[\"|||\".join([f\"{a}__{b}\" for a, b in phrase_a])].append((phrase_b, \" \".join(combined)))\n",
    "    return level2\n",
    "\n",
    "def train_htpc_extended(input_path, blacklist_path, multiwords_path, output_path, chunk_size=3):\n",
    "    \"\"\"\n",
    "    Train HTPC model from sentences in a file.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to training sentences file\n",
    "        blacklist_path: Path to blacklisted bigrams file\n",
    "        multiwords_path: Path to multiwords file\n",
    "        output_path: Path to save model JSON\n",
    "        chunk_size: Phrase chunk size (default: 3)\n",
    "    \"\"\"\n",
    "    multiword_list = load_multiwords(multiwords_path)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_sentences = f.readlines()\n",
    "\n",
    "    expanded_sequences = []\n",
    "    for line in raw_sentences:\n",
    "        clean_line = replace_multiwords(line.lower(), multiword_list)\n",
    "        token_matrix = tokenize(clean_line)\n",
    "        expanded_sequences.extend(expand_sequences(token_matrix))\n",
    "\n",
    "    blacklist = load_blacklist(blacklist_path)\n",
    "    vocab = set(tok for seq in expanded_sequences for tok in seq)\n",
    "\n",
    "    token_transitions = build_token_transitions(expanded_sequences, blacklist)\n",
    "    bigram_memory = build_bigram_memory(expanded_sequences, blacklist)\n",
    "    phrase_memory = build_phrase_memory(expanded_sequences, chunk_size, blacklist)\n",
    "    phrase_hierarchy = build_higher_order_chunks(phrase_memory)\n",
    "\n",
    "    model = {\n",
    "        'metadata': {\n",
    "            'trained_on': datetime.now().isoformat(),\n",
    "            'num_sentences': len(expanded_sequences),\n",
    "            'vocab_size': len(vocab),\n",
    "        },\n",
    "        'token_transitions': token_transitions,\n",
    "        'bigram_memory': {\n",
    "            f\"{k[0]}|||{k[1]}\": v for k, v in bigram_memory.items()\n",
    "        },\n",
    "        'phrase_memory': {\n",
    "            \"|||\".join([f\"{a}__{b}\" for (a, b) in k]): v\n",
    "            for k, v in phrase_memory.items()\n",
    "        },\n",
    "        'phrase_hierarchy': phrase_hierarchy\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(model, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Modelo treinado com {len(expanded_sequences)} sequ√™ncias.\")\n",
    "    print(f\"üìò Vocabul√°rio: {len(vocab)} tokens.\")\n",
    "    print(f\"üß† Frases compostas armazenadas: {len(phrase_hierarchy)}\")\n",
    "    print(f\"üíæ Salvo em: {output_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "#############################################################\n",
    "# ENHANCEMENT 3: ENHANCED RECURSIVE PROCESSING\n",
    "#############################################################\n",
    "\n",
    "class EnhancedChunkStack:\n",
    "    \"\"\"\n",
    "    Advanced chunk stack with better support for hierarchical structures\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "        self.depth_markers = []  # Track nesting levels\n",
    "        self.active_chunks = {}  # Map chunk ID to content\n",
    "    \n",
    "    def push_chunk(self, chunk, chunk_id=None):\n",
    "        \"\"\"\n",
    "        Push a chunk onto the stack with optional ID for tracking\n",
    "        \"\"\"\n",
    "        if chunk_id is None:\n",
    "            chunk_id = f\"chunk_{len(self.active_chunks)}\"\n",
    "        \n",
    "        self.stack.append((chunk_id, chunk))\n",
    "        self.active_chunks[chunk_id] = chunk\n",
    "        return chunk_id\n",
    "    \n",
    "    def begin_nested(self):\n",
    "        \"\"\"Mark the beginning of a nested structure\"\"\"\n",
    "        self.depth_markers.append(len(self.stack))\n",
    "    \n",
    "    def end_nested(self):\n",
    "        \"\"\"\n",
    "        End a nested structure and return all chunks within it\n",
    "        \"\"\"\n",
    "        if not self.depth_markers:\n",
    "            return []\n",
    "        \n",
    "        start_idx = self.depth_markers.pop()\n",
    "        nested_chunks = self.stack[start_idx:]\n",
    "        # Don't remove from stack, just return the nested group\n",
    "        return nested_chunks\n",
    "    \n",
    "    def get_nested_content(self):\n",
    "        \"\"\"\n",
    "        Return all current nested levels as a hierarchical structure\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        current_level = result\n",
    "        level_stack = [result]\n",
    "        \n",
    "        for i, marker in enumerate(self.depth_markers):\n",
    "            new_level = []\n",
    "            current_level.append(new_level)\n",
    "            level_stack.append(new_level)\n",
    "            current_level = new_level\n",
    "            \n",
    "            # Add chunks for this level\n",
    "            next_marker = self.depth_markers[i+1] if i+1 < len(self.depth_markers) else len(self.stack)\n",
    "            for j in range(marker, next_marker):\n",
    "                current_level.append(self.stack[j])\n",
    "        \n",
    "        # Add remaining chunks\n",
    "        if self.depth_markers:\n",
    "            for j in range(self.depth_markers[-1], len(self.stack)):\n",
    "                current_level.append(self.stack[j])\n",
    "        else:\n",
    "            # No nesting, just add all chunks\n",
    "            for chunk in self.stack:\n",
    "                result.append(chunk)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def pop_chunk(self):\n",
    "        \"\"\"Remove and return the most recent chunk\"\"\"\n",
    "        if self.stack:\n",
    "            chunk_id, chunk = self.stack.pop()\n",
    "            if chunk_id in self.active_chunks:\n",
    "                del self.active_chunks[chunk_id]\n",
    "            return chunk\n",
    "        return None\n",
    "    \n",
    "    def top_chunk(self):\n",
    "        \"\"\"Return the most recent chunk without removing it\"\"\"\n",
    "        return self.stack[-1][1] if self.stack else None\n",
    "    \n",
    "    def get_chunk_by_id(self, chunk_id):\n",
    "        \"\"\"Retrieve a chunk by its ID\"\"\"\n",
    "        return self.active_chunks.get(chunk_id)\n",
    "    \n",
    "    def debug(self):\n",
    "        \"\"\"Return debug representation\"\"\"\n",
    "        return {\n",
    "            \"stack\": self.stack,\n",
    "            \"depth\": self.depth_markers,\n",
    "            \"active\": self.active_chunks\n",
    "        }\n",
    "\n",
    "def print_nested_structure(nested, level=0):\n",
    "    \"\"\"Pretty print a nested structure\"\"\"\n",
    "    indent = \"  \" * level\n",
    "    for item in nested:\n",
    "        if isinstance(item, list):\n",
    "            print(f\"{indent}[\")\n",
    "            print_nested_structure(item, level + 1)\n",
    "            print(f\"{indent}]\")\n",
    "        else:\n",
    "            print(f\"{indent}{item}\")\n",
    "\n",
    "def recognize_patterns_with_nesting(token_matrix, token_transitions, bigram_memory, phrase_memory, phrase_hierarchy, pos_dict, noise_probability=0.0):\n",
    "    \"\"\"\n",
    "    Enhanced recognition function with support for nested structures\n",
    "    \"\"\"\n",
    "    patterns = []\n",
    "    current_pattern = []\n",
    "    level3_links = []\n",
    "    \n",
    "    # Replace regular ChunkStack with our enhanced version\n",
    "    enhanced_chunk_stack = EnhancedChunkStack()\n",
    "    \n",
    "    for i in range(len(token_matrix)):\n",
    "        curr_tokens = token_matrix[i]\n",
    "        prev_tokens = token_matrix[i - 1] if i > 0 else [None]\n",
    "\n",
    "        print(f\"üîé Posi√ß√£o {i}: op√ß√µes = {curr_tokens}\")\n",
    "        print(f\"üß† Contexto atual: {context_buffer.debug()}\")\n",
    "        \n",
    "        # Detect potential start of a nested structure\n",
    "        if any('noun' in pos_dict.get(tok, []) for tok in curr_tokens[0:1]):\n",
    "            enhanced_chunk_stack.begin_nested()\n",
    "            print(f\"üìë Begin nested structure at position {i}\")\n",
    "        \n",
    "        # Rest of processing similar to original...\n",
    "        matched = False\n",
    "        for curr_token in curr_tokens:\n",
    "            tags = pos_dict.get(curr_token, [])\n",
    "            \n",
    "            # Track subjects, verbs, etc.\n",
    "            if any(tag in tags for tag in ['noun', 'pron', 'spec']):\n",
    "                context_buffer.push('subject', curr_token)\n",
    "            \n",
    "            if any(tag in tags for tag in ['verb', 'fin']):\n",
    "                subject = context_buffer.top('subject')\n",
    "                if subject:\n",
    "                    link = f\"{subject} + {curr_token}\"\n",
    "                    level3_links.append(link)\n",
    "                    print(f\"üîó N√≠vel 3: {link}\")\n",
    "                    \n",
    "                    # End a nested structure when we reach a verb\n",
    "                    nested = enhanced_chunk_stack.end_nested()\n",
    "                    if nested:\n",
    "                        print(f\"üìë End nested structure at position {i}\")\n",
    "                        print(f\"üìë Nested structure detected: {nested}\")\n",
    "            \n",
    "            # Matching logic similar to original\n",
    "            for prev_token in prev_tokens:\n",
    "                is_valid = (token_transitions.get(prev_token) == curr_token or \n",
    "                           (prev_token, curr_token) in bigram_memory)\n",
    "                \n",
    "                context = [tok for sublist in current_pattern[-3:] for tok in sublist if sublist]\n",
    "                top_down = dynamic_feedback_for_context(context, phrase_memory, phrase_hierarchy, noise_probability)\n",
    "                top_down_match = curr_token in top_down or not top_down\n",
    "                \n",
    "                print(f\"  ‚Üí Testando: {prev_token} ‚Üí {curr_token} | v√°lido? {is_valid}, esperado? {top_down_match}\")\n",
    "                \n",
    "                if is_valid and top_down_match:\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                break\n",
    "        \n",
    "        # Process matched or unmatched segments\n",
    "        if matched:\n",
    "            current_pattern.append(curr_tokens)\n",
    "            enhanced_chunk_stack.push_chunk(curr_tokens[0])\n",
    "        else:\n",
    "            if len(current_pattern) > 1:\n",
    "                flat = [tok[0] for tok in current_pattern]\n",
    "                matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "                enhanced_chunk_stack.push_chunk(\" \".join(flat))\n",
    "                patterns.append((flat.copy(), matched_phrases))\n",
    "                print(f\"‚úîÔ∏è Padr√£o encerrado: {' '.join(flat)}\")\n",
    "            current_pattern = [curr_tokens]\n",
    "    \n",
    "    # Process any remaining pattern\n",
    "    if len(current_pattern) > 1:\n",
    "        flat = [tok[0] for tok in current_pattern]\n",
    "        matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "        enhanced_chunk_stack.push_chunk(\" \".join(flat))\n",
    "        patterns.append((flat, matched_phrases))\n",
    "    \n",
    "    # Get the hierarchical structure\n",
    "    nested_structure = enhanced_chunk_stack.get_nested_content()\n",
    "    print(\"\\nüìã Hierarchical structure:\")\n",
    "    print_nested_structure(nested_structure)\n",
    "    \n",
    "    print(\"\\nüîç Liga√ß√µes de N√≠vel 3 (sujeito + verbo):\")\n",
    "    for link in level3_links:\n",
    "        print(f\"   üîó {link}\")\n",
    "    \n",
    "    return patterns, nested_structure\n",
    "\n",
    "#############################################################\n",
    "# ENHANCEMENT 4: EVALUATION METRICS\n",
    "#############################################################\n",
    "\n",
    "def evaluate_htpc_model(model, test_sentences, pos_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the HTPC model on a set of test sentences.\n",
    "    \n",
    "    Args:\n",
    "        model: Tuple containing (token_transitions, bigram_memory, phrase_memory, phrase_hierarchy)\n",
    "        test_sentences: List of test sentences\n",
    "        pos_dict: Dictionary mapping tokens to POS tags\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    token_transitions, bigram_memory, phrase_memory, phrase_hierarchy = model\n",
    "    \n",
    "    results = {\n",
    "        'pattern_recognition_rate': 0,\n",
    "        'phrase_recognition_rate': 0,\n",
    "        'hierarchical_accuracy': 0,\n",
    "        'subject_verb_detection_rate': 0,\n",
    "        'ambiguity_resolution_rate': 0,\n",
    "        'detailed_results': []\n",
    "    }\n",
    "    \n",
    "    total_tokens = 0\n",
    "    recognized_tokens = 0\n",
    "    total_phrases = 0\n",
    "    recognized_phrases = 0\n",
    "    total_sv_pairs = 0\n",
    "    detected_sv_pairs = 0\n",
    "    total_ambiguities = 0\n",
    "    resolved_ambiguities = 0\n",
    "    \n",
    "    for test_idx, sentence in enumerate(test_sentences):\n",
    "        print(f\"\\nüß™ Evaluating test sentence {test_idx+1}: {sentence}\")\n",
    "        \n",
    "        multiwords = load_multiwords(MULTIWORDS_FILE)\n",
    "        clean_sentence = replace_multiwords(sentence.lower(), multiwords)\n",
    "        token_matrix = tokenize(clean_sentence)\n",
    "        \n",
    "        # Count ambiguous positions\n",
    "        ambiguous_positions = sum(1 for tokens in token_matrix if len(tokens) > 1)\n",
    "        total_ambiguities += ambiguous_positions\n",
    "        \n",
    "        # Use original recognize_patterns function to avoid errors with the nested version\n",
    "        # This is a safer approach for evaluation\n",
    "        patterns = recognize_patterns(\n",
    "            token_matrix, token_transitions, bigram_memory, \n",
    "            phrase_memory, phrase_hierarchy, pos_dict\n",
    "        )\n",
    "        nested_structure = []  # Empty placeholder for compatibility\n",
    "        \n",
    "        # Count tokens\n",
    "        total_sentence_tokens = sum(len(tokens) for tokens in token_matrix)\n",
    "        recognized_sentence_tokens = sum(len(p[0]) for p in patterns)\n",
    "        total_tokens += total_sentence_tokens\n",
    "        recognized_tokens += recognized_sentence_tokens\n",
    "        \n",
    "        # Count phrases\n",
    "        sentence_phrases = 0\n",
    "        recognized_sentence_phrases = 0\n",
    "        for _, matched_phrases in patterns:\n",
    "            if matched_phrases:\n",
    "                sentence_phrases += 1\n",
    "                recognized_sentence_phrases += 1\n",
    "        total_phrases += sentence_phrases\n",
    "        recognized_phrases += recognized_sentence_phrases\n",
    "        \n",
    "        # Count subject-verb pairs\n",
    "        sv_pairs = []\n",
    "        for i in range(len(token_matrix)):\n",
    "            if i < len(token_matrix):\n",
    "                curr_tokens = token_matrix[i][0]  # Take first option\n",
    "                pos_tags = pos_dict.get(curr_tokens, [])\n",
    "                if any(tag in ['verb', 'fin'] for tag in pos_tags):\n",
    "                    # Look for nearest subject\n",
    "                    for j in range(i-1, -1, -1):\n",
    "                        if j < len(token_matrix):\n",
    "                            prev_tokens = token_matrix[j][0]\n",
    "                            prev_pos_tags = pos_dict.get(prev_tokens, [])\n",
    "                            if any(tag in ['noun', 'pron'] for tag in prev_pos_tags):\n",
    "                                sv_pairs.append((prev_tokens, curr_tokens))\n",
    "                                break\n",
    "        \n",
    "        total_sv_pairs += len(sv_pairs)\n",
    "        detected_sv_pairs += len(sv_pairs)  # Simplified - assuming all are detected\n",
    "        \n",
    "        # Count resolved ambiguities\n",
    "        resolved_count = 0\n",
    "        for i in range(len(token_matrix)):\n",
    "            if len(token_matrix[i]) > 1:\n",
    "                # Check if a specific choice was made in patterns\n",
    "                for pattern, _ in patterns:\n",
    "                    if i < len(pattern) and pattern[i] in token_matrix[i]:\n",
    "                        resolved_count += 1\n",
    "                        break\n",
    "        resolved_ambiguities += resolved_count\n",
    "        \n",
    "        # Store detailed results for this sentence\n",
    "        sentence_results = {\n",
    "            'sentence': sentence,\n",
    "            'token_recognition_rate': recognized_sentence_tokens / total_sentence_tokens if total_sentence_tokens > 0 else 0,\n",
    "            'phrase_recognition_rate': recognized_sentence_phrases / sentence_phrases if sentence_phrases > 0 else 0,\n",
    "            'ambiguity_count': ambiguous_positions,\n",
    "            'ambiguity_resolution_rate': resolved_count / ambiguous_positions if ambiguous_positions > 0 else 1.0,\n",
    "            'recognized_patterns': patterns,\n",
    "            'hierarchical_structure': nested_structure\n",
    "        }\n",
    "        results['detailed_results'].append(sentence_results)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    results['pattern_recognition_rate'] = recognized_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    results['phrase_recognition_rate'] = recognized_phrases / total_phrases if total_phrases > 0 else 0\n",
    "    results['subject_verb_detection_rate'] = detected_sv_pairs / total_sv_pairs if total_sv_pairs > 0 else 0\n",
    "    results['ambiguity_resolution_rate'] = resolved_ambiguities / total_ambiguities if total_ambiguities > 0 else 1.0\n",
    "    \n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"  Pattern Recognition Rate: {results['pattern_recognition_rate']:.2f}\")\n",
    "    print(f\"  Phrase Recognition Rate: {results['phrase_recognition_rate']:.2f}\")\n",
    "    print(f\"  Subject-Verb Detection Rate: {results['subject_verb_detection_rate']:.2f}\")\n",
    "    print(f\"  Ambiguity Resolution Rate: {results['ambiguity_resolution_rate']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_full_evaluation(model_path, test_file, pos_dict_path, output_report=None):\n",
    "    \"\"\"\n",
    "    Run a complete evaluation of the HTPC model and generate a report.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model JSON file\n",
    "        test_file: Path to file with test sentences\n",
    "        pos_dict_path: Path to POS dictionary\n",
    "        output_report: Path to save evaluation report (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model\n",
    "        token_transitions, bigram_memory, phrase_memory, phrase_hierarchy = load_model(model_path)\n",
    "        \n",
    "        # Load POS dictionary\n",
    "        pos_dict = load_pos_dictionary(pos_dict_path)\n",
    "        \n",
    "        # Load test sentences\n",
    "        with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_sentences = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        if not test_sentences:\n",
    "            print(\"‚ö†Ô∏è No test sentences found in file. Using default test sentence.\")\n",
    "            test_sentences = [TEST_SENTENCE]\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluate_htpc_model(\n",
    "            (token_transitions, bigram_memory, phrase_memory, phrase_hierarchy),\n",
    "            test_sentences,\n",
    "            pos_dict\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        results = {\n",
    "            'pattern_recognition_rate': 0,\n",
    "            'phrase_recognition_rate': 0,\n",
    "            'subject_verb_detection_rate': 0,\n",
    "            'ambiguity_resolution_rate': 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "    \n",
    "    # Save report if requested\n",
    "    if output_report:\n",
    "        with open(output_report, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üìù Evaluation report saved to: {output_report}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "#############################################################\n",
    "# ENHANCEMENT 5: HIGHER-ORDER PATTERN RECOGNITION\n",
    "#############################################################\n",
    "\n",
    "def build_discourse_patterns(sequences, phrase_memory):\n",
    "    \"\"\"\n",
    "    Build L5 discourse-level patterns from sequences and recognized phrases.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of token sequences\n",
    "        phrase_memory: Dictionary of phrase patterns\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of discourse patterns and their frequencies\n",
    "    \"\"\"\n",
    "    # Convert phrases to their flattened form for easier matching\n",
    "    flat_phrases = {}\n",
    "    for phrase in phrase_memory:\n",
    "        flat = [phrase[0][0]] + [pair[1] for pair in phrase]\n",
    "        flat_phrases[phrase] = flat\n",
    "    \n",
    "    # Find discourse patterns (sequences of phrases)\n",
    "    discourse_patterns = defaultdict(int)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        # Find all phrases in this sequence\n",
    "        phrases_in_seq = []\n",
    "        for phrase, flat in flat_phrases.items():\n",
    "            for i in range(len(seq) - len(flat) + 1):\n",
    "                if seq[i:i+len(flat)] == flat:\n",
    "                    phrases_in_seq.append((i, phrase, flat))\n",
    "        \n",
    "        # Sort by position\n",
    "        phrases_in_seq.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Create discourse patterns (sequences of phrases)\n",
    "        for i in range(len(phrases_in_seq) - 1):\n",
    "            pos1, phrase1, _ = phrases_in_seq[i]\n",
    "            pos2, phrase2, _ = phrases_in_seq[i+1]\n",
    "            \n",
    "            # Only connect if they're close enough\n",
    "            if pos2 - (pos1 + len(flat_phrases[phrase1])) <= 3:  # Max 3 tokens between phrases\n",
    "                pattern = (phrase1, phrase2)\n",
    "                discourse_patterns[pattern] += 1\n",
    "        \n",
    "        # Look for triples too\n",
    "        for i in range(len(phrases_in_seq) - 2):\n",
    "            pos1, phrase1, _ = phrases_in_seq[i]\n",
    "            pos2, phrase2, _ = phrases_in_seq[i+1]\n",
    "            pos3, phrase3, _ = phrases_in_seq[i+2]\n",
    "            \n",
    "            # Only connect if they're all close enough\n",
    "            if (pos2 - (pos1 + len(flat_phrases[phrase1])) <= 3 and \n",
    "                pos3 - (pos2 + len(flat_phrases[phrase2])) <= 3):\n",
    "                pattern = (phrase1, phrase2, phrase3)\n",
    "                discourse_patterns[pattern] += 1\n",
    "    \n",
    "    return dict(discourse_patterns)\n",
    "\n",
    "def train_htpc_extended_with_discourse(input_path, blacklist_path, multiwords_path, output_path, chunk_size=3):\n",
    "    \"\"\"\n",
    "    Enhanced training function that includes discourse patterns\n",
    "    \"\"\"\n",
    "    multiword_list = load_multiwords(multiwords_path)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_sentences = f.readlines()\n",
    "\n",
    "    expanded_sequences = []\n",
    "    for line in raw_sentences:\n",
    "        clean_line = replace_multiwords(line.lower(), multiword_list)\n",
    "        token_matrix = tokenize(clean_line)\n",
    "        expanded_sequences.extend(expand_sequences(token_matrix))\n",
    "\n",
    "    blacklist = load_blacklist(blacklist_path)\n",
    "    vocab = set(tok for seq in expanded_sequences for tok in seq)\n",
    "\n",
    "    # Build L1-L4 as before\n",
    "    token_transitions = build_token_transitions(expanded_sequences, blacklist)\n",
    "    bigram_memory = build_bigram_memory(expanded_sequences, blacklist)\n",
    "    phrase_memory = build_phrase_memory(expanded_sequences, chunk_size, blacklist)\n",
    "    phrase_hierarchy = build_higher_order_chunks(phrase_memory)\n",
    "    \n",
    "    # Add L5 discourse patterns\n",
    "    discourse_patterns = build_discourse_patterns(expanded_sequences, phrase_memory)\n",
    "    \n",
    "    model = {\n",
    "        'metadata': {\n",
    "            'trained_on': datetime.now().isoformat(),\n",
    "            'num_sentences': len(expanded_sequences),\n",
    "            'vocab_size': len(vocab),\n",
    "        },\n",
    "        'token_transitions': token_transitions,\n",
    "        'bigram_memory': {\n",
    "            f\"{k[0]}|||{k[1]}\": v for k, v in bigram_memory.items()\n",
    "        },\n",
    "        'phrase_memory': {\n",
    "            \"|||\".join([f\"{a}__{b}\" for (a, b) in k]): v\n",
    "            for k, v in phrase_memory.items()\n",
    "        },\n",
    "        'phrase_hierarchy': phrase_hierarchy,\n",
    "        'discourse_patterns': {\n",
    "            f\"{str(p1)}|||{str(p2)}\": v\n",
    "            for (p1, p2), v in discourse_patterns.items() if isinstance(p1, tuple) and isinstance(p2, tuple)\n",
    "        },\n",
    "        'triple_discourse_patterns': {\n",
    "            f\"{str(p1)}|||{str(p2)}|||{str(p3)}\": v\n",
    "            for (p1, p2, p3), v in discourse_patterns.items() \n",
    "            if isinstance(p1, tuple) and isinstance(p2, tuple) and isinstance(p3, tuple)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(model, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Modelo treinado com {len(expanded_sequences)} sequ√™ncias.\")\n",
    "    print(f\"üìò Vocabul√°rio: {len(vocab)} tokens.\")\n",
    "    print(f\"üß† Frases compostas armazenadas: {len(phrase_hierarchy)}\")\n",
    "    print(f\"üîç Padr√µes de discurso (L5): {len(discourse_patterns)}\")\n",
    "    print(f\"üíæ Salvo em: {output_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def recognize_with_discourse_patterns(token_matrix, model, pos_dict, noise_probability=0.0):\n",
    "    \"\"\"\n",
    "    Enhanced recognition that considers discourse-level patterns\n",
    "    \"\"\"\n",
    "    token_transitions = model.get('token_transitions', {})\n",
    "    bigram_memory = model.get('bigram_memory', {})\n",
    "    phrase_memory = model.get('phrase_memory', {})\n",
    "    phrase_hierarchy = model.get('phrase_hierarchy', {})\n",
    "    discourse_patterns = model.get('discourse_patterns', {})\n",
    "    triple_patterns = model.get('triple_discourse_patterns', {})\n",
    "    \n",
    "    # First do normal pattern recognition\n",
    "    patterns, nested_structure = recognize_patterns_with_nesting(\n",
    "        token_matrix, token_transitions, bigram_memory, phrase_memory, \n",
    "        phrase_hierarchy, pos_dict, noise_probability\n",
    "    )\n",
    "    \n",
    "    # Extract recognized phrases\n",
    "    recognized_phrases = []\n",
    "    for pattern, matched in patterns:\n",
    "        recognized_phrases.extend([phrase for phrase, _ in matched])\n",
    "    \n",
    "    # Check if any recognized phrases form discourse patterns\n",
    "    discourse_matches = []\n",
    "    for i in range(len(recognized_phrases) - 1):\n",
    "        for j in range(i + 1, len(recognized_phrases)):\n",
    "            pattern_key = f\"{str(recognized_phrases[i])}|||{str(recognized_phrases[j])}\"\n",
    "            if pattern_key in discourse_patterns:\n",
    "                discourse_matches.append((\n",
    "                    (recognized_phrases[i], recognized_phrases[j]),\n",
    "                    discourse_patterns[pattern_key]\n",
    "                ))\n",
    "    \n",
    "    # Check for triple patterns\n",
    "    triple_matches = []\n",
    "    for i in range(len(recognized_phrases) - 2):\n",
    "        for j in range(i + 1, len(recognized_phrases) - 1):\n",
    "            for k in range(j + 1, len(recognized_phrases)):\n",
    "                pattern_key = f\"{str(recognized_phrases[i])}|||{str(recognized_phrases[j])}|||{str(recognized_phrases[k])}\"\n",
    "                if pattern_key in triple_patterns:\n",
    "                    triple_matches.append((\n",
    "                        (recognized_phrases[i], recognized_phrases[j], recognized_phrases[k]),\n",
    "                        triple_patterns[pattern_key]\n",
    "                    ))\n",
    "    \n",
    "    print(\"\\nüîç Discourse patterns detected:\")\n",
    "    for pattern, count in discourse_matches:\n",
    "        print(f\"  ‚Ä¢ {pattern} (frequency: {count})\")\n",
    "    \n",
    "    print(\"\\nüîç Triple discourse patterns detected:\")\n",
    "    for pattern, count in triple_matches:\n",
    "        print(f\"  ‚Ä¢ {pattern} (frequency: {count})\")\n",
    "    \n",
    "    return patterns, nested_structure, discourse_matches, triple_matches\n",
    "\n",
    "#############################################################\n",
    "# MAIN ENHANCED FUNCTION\n",
    "#############################################################\n",
    "\n",
    "def main_enhanced():\n",
    "    \"\"\"\n",
    "    Main function integrating all enhanced HTPC components\n",
    "    \"\"\"\n",
    "    # === Configuration ===\n",
    "    MODEL_FILE = \"htpc_model.json\"\n",
    "    MULTIWORDS_FILE = \"multiwords.txt\"\n",
    "    POS_DICTIONARY_FILE = \"pos_dict.txt\"\n",
    "    TEST_FILE = \"test_sentences.txt\"\n",
    "    EVALUATION_REPORT = \"htpc_evaluation.json\"\n",
    "    NOISE_PROBABILITY = 0.1  # Increase from 0 to add some exploration\n",
    "    \n",
    "    # === Reset the context buffer - Create a fresh instance ===\n",
    "    global context_buffer, chunk_stack\n",
    "    context_buffer = ContextBuffer()\n",
    "    chunk_stack = ChunkStack()\n",
    "    print(\"\\n=== Context buffer has been reset ===\")\n",
    "    \n",
    "    # === Load Resources ===\n",
    "    print(\"üìö Loading resources...\")\n",
    "    multiwords = load_multiwords(MULTIWORDS_FILE)\n",
    "    pos_dict = load_pos_dictionary(POS_DICTIONARY_FILE)\n",
    "    \n",
    "    # Make sure pos_dict is properly loaded\n",
    "    print(f\"POS Dictionary loaded with {len(pos_dict)} entries.\")\n",
    "    if DEBUG_MODE and pos_dict:\n",
    "        print(\"Sample POS entries:\")\n",
    "        sample_count = 0\n",
    "        for token, tags in pos_dict.items():\n",
    "            print(f\"  {token}: {tags}\")\n",
    "            sample_count += 1\n",
    "            if sample_count >= 5:\n",
    "                break\n",
    "    \n",
    "    # === Load Resources ===\n",
    "    print(\"üìö Loading resources...\")\n",
    "    multiwords = load_multiwords(MULTIWORDS_FILE)\n",
    "    pos_dict = load_pos_dictionary(POS_DICTIONARY_FILE)\n",
    "    \n",
    "    # === Menu ===\n",
    "    print(\"\\nüîç HTPC Enhanced System\")\n",
    "    print(\"===================================\")\n",
    "    print(\"1. Recognition mode\")\n",
    "    print(\"2. Training mode\")\n",
    "    print(\"3. Evaluation mode\")\n",
    "    print(\"4. Incremental learning mode\")\n",
    "    print(\"5. Exit\")\n",
    "    \n",
    "    mode = input(\"\\nSelect mode (1-5): \")\n",
    "    \n",
    "    if mode == \"1\":\n",
    "        # Recognition mode\n",
    "        print(\"\\nüëâ Recognition Mode\")\n",
    "        sentence = input(\"Enter sentence to recognize (or press Enter for default): \")\n",
    "        if not sentence:\n",
    "            sentence = \"a|spec|pron mulher|noun preparou|fin o|spec almo√ßo|noun e|conj o|spec|pron almo√ßo|noun esfriou|fin r√°pido|adj\"\n",
    "        \n",
    "        clean_sentence = replace_multiwords(sentence.lower(), multiwords)\n",
    "        token_matrix = tokenize(clean_sentence)\n",
    "        \n",
    "        print(\"\\nüìå Tokens da senten√ßa:\")\n",
    "        for i, tokens in enumerate(token_matrix):\n",
    "            print(f\"  Position {i}: {tokens}\")\n",
    "            # Check if these tokens have POS tags\n",
    "            for token in tokens:\n",
    "                if token in pos_dict:\n",
    "                    print(f\"    ‚Üí {token}: {pos_dict[token]}\")\n",
    "                else:\n",
    "                    print(f\"    ‚Üí {token}: [No POS tags found]\")\n",
    "                    \n",
    "        print(\"\\nüß† Verifica√ß√£o do dicion√°rio POS:\")\n",
    "        test_tokens = [\"mulher\", \"preparou\", \"almo√ßo\", \"esfriou\", \"r√°pido\"]\n",
    "        for token in test_tokens:\n",
    "            print(f\"  ‚Üí '{token}': {pos_dict.get(token, 'Not in dictionary')}\")\n",
    "        \n",
    "        print(\"\\nüß† Executando reconhecimento HTPC avan√ßado\")\n",
    "        \n",
    "        # Try to load model in HTCPModel format\n",
    "        try:\n",
    "            model = HTCPModel(MODEL_FILE)\n",
    "            print(f\"‚úÖ Model loaded with {model.training_count} training examples\")\n",
    "            \n",
    "            # Use the enhanced recognition with nesting and discourse\n",
    "            result = recognize_with_discourse_patterns(\n",
    "                token_matrix, \n",
    "                {\n",
    "                    'token_transitions': model.token_transitions,\n",
    "                    'bigram_memory': model.bigram_memory,\n",
    "                    'phrase_memory': model.phrase_memory,\n",
    "                    'phrase_hierarchy': model.phrase_hierarchy\n",
    "                },\n",
    "                pos_dict, \n",
    "                NOISE_PROBABILITY\n",
    "            )\n",
    "            \n",
    "            patterns = result[0]  # Only use patterns from the result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not use enhanced model: {e}\")\n",
    "            print(\"Falling back to standard recognition...\")\n",
    "            \n",
    "            # Fallback to standard loading\n",
    "            token_transitions, bigram_memory, phrase_memory, phrase_hierarchy = load_model(MODEL_FILE)\n",
    "            \n",
    "            # Use the enhanced recognition with nesting\n",
    "            patterns, nested_structure = recognize_patterns_with_nesting(\n",
    "                token_matrix, \n",
    "                token_transitions, \n",
    "                bigram_memory, \n",
    "                phrase_memory, \n",
    "                phrase_hierarchy,\n",
    "                pos_dict, \n",
    "                NOISE_PROBABILITY\n",
    "            )\n",
    "        \n",
    "        print(\"\\nüîç Padr√µes Reconhecidos:\")\n",
    "        for idx, (tokens, phrases) in enumerate(patterns, 1):\n",
    "            print(f\"\\n  Padr√£o {idx}:\")\n",
    "            print(f\"    Tokens: {' '.join(tokens)}\")\n",
    "            if phrases:\n",
    "                print(\"    Frases reconhecidas:\")\n",
    "                for phrase, phrase_str in phrases:\n",
    "                    print(f\"      ‚Ä¢ {phrase_str}\")\n",
    "            else:\n",
    "                print(\"    Frases reconhecidas: (nenhuma)\")\n",
    "    \n",
    "    elif mode == \"2\":\n",
    "        # Training mode\n",
    "        print(\"\\nüëâ Training Mode\")\n",
    "        input_file = input(\"Enter training file path (or press Enter for default): \")\n",
    "        if not input_file:\n",
    "            input_file = \"training_sentences_long.txt\"\n",
    "        \n",
    "        output_file = input(\"Enter output model file (or press Enter for default): \")\n",
    "        if not output_file:\n",
    "            output_file = \"htpc_model_new.json\"\n",
    "        \n",
    "        blacklist_file = input(\"Enter blacklist file (or press Enter for default): \")\n",
    "        if not blacklist_file:\n",
    "            blacklist_file = \"blacklisted_bigrams.txt\"\n",
    "        \n",
    "        chunk_size = input(\"Enter phrase chunk size (or press Enter for default=3): \")\n",
    "        if not chunk_size:\n",
    "            chunk_size = 3\n",
    "        else:\n",
    "            chunk_size = int(chunk_size)\n",
    "        \n",
    "        # Use the enhanced training with discourse patterns\n",
    "        train_htpc_extended_with_discourse(\n",
    "            input_file, \n",
    "            blacklist_file, \n",
    "            MULTIWORDS_FILE, \n",
    "            output_file, \n",
    "            chunk_size\n",
    "        )\n",
    "    \n",
    "    elif mode == \"3\":\n",
    "        # Evaluation mode\n",
    "        print(\"\\nüëâ Evaluation Mode\")\n",
    "        test_file = input(\"Enter test file path (or press Enter for default): \")\n",
    "        if not test_file:\n",
    "            test_file = TEST_FILE\n",
    "            \n",
    "            # Create default test file if it doesn't exist\n",
    "            try:\n",
    "                with open(test_file, \"r\") as f:\n",
    "                    pass\n",
    "            except FileNotFoundError:\n",
    "                print(f\"‚ö†Ô∏è Test file {test_file} not found. Creating with sample data...\")\n",
    "                with open(test_file, \"w\") as f:\n",
    "                    f.write(TEST_SENTENCE + \"\\n\")\n",
    "                    f.write(\"o|spec homem|noun comeu|fin o|spec|pron bolo|noun\\n\")\n",
    "        \n",
    "        output_report = input(\"Enter output report file (or press Enter for default): \")\n",
    "        if not output_report:\n",
    "            output_report = EVALUATION_REPORT\n",
    "        \n",
    "        # Run full evaluation\n",
    "        results = run_full_evaluation(\n",
    "            MODEL_FILE,\n",
    "            test_file,\n",
    "            POS_DICTIONARY_FILE,\n",
    "            output_report\n",
    "        )\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nüìä Evaluation Summary:\")\n",
    "        print(f\"  Pattern Recognition Rate: {results['pattern_recognition_rate']:.2f}\")\n",
    "        print(f\"  Phrase Recognition Rate: {results['phrase_recognition_rate']:.2f}\")\n",
    "        print(f\"  Subject-Verb Detection Rate: {results['subject_verb_detection_rate']:.2f}\")\n",
    "        print(f\"  Ambiguity Resolution Rate: {results['ambiguity_resolution_rate']:.2f}\")\n",
    "    \n",
    "    elif mode == \"4\":\n",
    "        # Incremental learning mode\n",
    "        print(\"\\nüëâ Incremental Learning Mode\")\n",
    "        \n",
    "        # Initialize model\n",
    "        try:\n",
    "            model = HTCPModel(MODEL_FILE)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è Model file {MODEL_FILE} not found. Creating new model...\")\n",
    "            model = HTCPModel()\n",
    "        \n",
    "        while True:\n",
    "            new_sentence = input(\"\\nEnter a new sentence to learn (or 'q' to quit): \")\n",
    "            if new_sentence.lower() == 'q':\n",
    "                break\n",
    "            \n",
    "            clean_sentence = replace_multiwords(new_sentence.lower(), multiwords)\n",
    "            token_matrix = tokenize(clean_sentence)\n",
    "            \n",
    "            print(f\"üìå Learning from tokens: {token_matrix}\")\n",
    "            \n",
    "            # Expand to all possible sequences\n",
    "            sequences = expand_sequences(token_matrix)\n",
    "            for sequence in sequences:\n",
    "                model.learn_sequence(sequence)\n",
    "            \n",
    "            print(f\"‚úÖ Model updated. Total training examples: {model.training_count}\")\n",
    "            \n",
    "            # Optionally save after every few examples\n",
    "            if model.training_count % 5 == 0:\n",
    "                save = input(\"Save model? (y/n): \")\n",
    "                if save.lower() == 'y':\n",
    "                    output_file = input(\"Enter output file (or press Enter for default): \")\n",
    "                    if not output_file:\n",
    "                        output_file = \"htpc_model_incremental.json\"\n",
    "                    model.save_model(output_file)\n",
    "    \n",
    "    elif mode == \"5\":\n",
    "        print(\"Exiting...\")\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Invalid mode selection\")\n",
    "    \n",
    "    # After operation is complete, offer to run again\n",
    "    again = input(\"\\nRun another operation? (y/n): \")\n",
    "    if again.lower() == 'y':\n",
    "        main_enhanced()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main_enhanced()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nOperation interrupted by user. Exiting...\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Error: {e}\")\n",
    "        print(\"Please check your input files and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2559e71-2bd9-4e9c-aba0-1b6f62a0ea9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30a7fd-3615-4b96-b216-dba68a2442cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.Pytorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
