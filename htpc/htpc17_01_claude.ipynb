{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17598fdc-d683-43ae-b594-5d307fb1c9e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 953)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:953\u001b[0;36m\u001b[0m\n\u001b[0;31m    in range(i + 1, len(recognized_phrases)):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Enhanced HTPC (Hierarchical Temporal Predictive Coding) Implementation\n",
    "# Fixed version with proper indentation\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# === CONFIGURA√á√ÉO ===\n",
    "MODEL_FILE = \"htpc_model.json\"\n",
    "MULTIWORDS_FILE = \"multiwords.txt\"\n",
    "POS_DICTIONARY_FILE = \"pos_dict.txt\"\n",
    "TEST_SENTENCE = \"a|spec|pron mulher|noun preparou|fin o|spec almo√ßo|noun e|conj o|spec|pron almo√ßo|noun esfriou|fin r√°pido|adj\"\n",
    "NOISE_PROBABILITY = 0.1  # Increased from 0.0 to add some exploration\n",
    "DEBUG_MODE = True  # Enable detailed debugging\n",
    "\n",
    "# === MULTIWORDS ===\n",
    "def load_multiwords(path):\n",
    "    multiwords = []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                mw = line.strip().lower()\n",
    "                if mw:\n",
    "                    multiwords.append((mw, mw.replace(\" \", \"_\")))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Aviso: Arquivo de multiwords n√£o encontrado.\")\n",
    "    return multiwords\n",
    "\n",
    "def replace_multiwords(text, multiword_list):\n",
    "    for original, replacement in multiword_list:\n",
    "        text = text.replace(original, replacement)\n",
    "    return text\n",
    "\n",
    "# === DICION√ÅRIO DE CLASSES GRAMATICAIS ===\n",
    "def load_pos_dictionary(path):\n",
    "    pos_dict = {}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if parts:\n",
    "                    token = parts[0]\n",
    "                    tags = parts[1:]\n",
    "                    pos_dict[token] = tags\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Arquivo de classes gramaticais n√£o encontrado: pos_dict.txt\")\n",
    "    return pos_dict\n",
    "\n",
    "# === TOKENIZA√á√ÉO COM SUPORTE A M√öLTIPLOS TOKENS ===\n",
    "def normalize_token(token):\n",
    "    return re.sub(r\"[.,!?;:()\\[\\]{}\\\"']\", \"\", token.lower())\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [[normalize_token(opt) for opt in tok.split(\"|\")] for tok in sentence.strip().split() if tok]\n",
    "\n",
    "# === MODELO HTPC ===\n",
    "def load_model(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        model = json.load(f)\n",
    "\n",
    "    token_transitions = model['token_transitions']\n",
    "    bigram_memory = {\n",
    "        tuple(key.split(\"|||\")): value\n",
    "        for key, value in model['bigram_memory'].items()\n",
    "    }\n",
    "    phrase_memory = {\n",
    "        tuple(tuple(pair.split(\"__\")) for pair in key.split(\"|||\")): value\n",
    "        for key, value in model['phrase_memory'].items()\n",
    "    }\n",
    "    phrase_hierarchy = model.get('phrase_hierarchy', {})\n",
    "    return token_transitions, bigram_memory, phrase_memory, phrase_hierarchy\n",
    "\n",
    "# === MEM√ìRIA DE CONTEXTO COM STACK ===\n",
    "class ContextBuffer:\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "        print(\"Created new ContextBuffer instance\")\n",
    "\n",
    "    def push(self, key, value):\n",
    "        self.stack.append((key, value))\n",
    "        print(f\"++ CONTEXT PUSH: {key}={value}\")\n",
    "\n",
    "    def pop(self):\n",
    "        if self.stack:\n",
    "            result = self.stack.pop()\n",
    "            print(f\"-- CONTEXT POP: {result}\")\n",
    "            return result\n",
    "        print(\"!! CONTEXT POP: Empty stack\")\n",
    "        return None\n",
    "\n",
    "    def top(self, key):\n",
    "        for k, v in reversed(self.stack):\n",
    "            if k == key:\n",
    "                print(f\"?? CONTEXT LOOKUP: Found {key}={v}\")\n",
    "                return v\n",
    "        print(f\"?? CONTEXT LOOKUP: {key} not found\")\n",
    "        return None\n",
    "\n",
    "    def pop_key(self, key):\n",
    "        for i in range(len(self.stack) - 1, -1, -1):\n",
    "            if self.stack[i][0] == key:\n",
    "                result = self.stack.pop(i)\n",
    "                print(f\"-- CONTEXT POP_KEY: {key}={result[1]}\")\n",
    "                return result\n",
    "        print(f\"!! CONTEXT POP_KEY: {key} not found\")\n",
    "        return None\n",
    "\n",
    "    def debug(self):\n",
    "        result = [f\"{k}={v}\" for k, v in self.stack]\n",
    "        print(f\"CONTEXT DUMP: {result}\")\n",
    "        return result\n",
    "\n",
    "# === PILHA DE CHUNKS PARA SUPORTE A RECURS√ÉO (ORIGINAL) ===\n",
    "class ChunkStack:\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "\n",
    "    def push_chunk(self, chunk):\n",
    "        self.stack.append(chunk)\n",
    "\n",
    "    def pop_chunk(self):\n",
    "        return self.stack.pop() if self.stack else None\n",
    "\n",
    "    def top_chunk(self):\n",
    "        return self.stack[-1] if self.stack else None\n",
    "\n",
    "    def debug(self):\n",
    "        return list(self.stack)\n",
    "\n",
    "# Global instances\n",
    "context_buffer = ContextBuffer()\n",
    "chunk_stack = ChunkStack()\n",
    "\n",
    "# === FUN√á√ÉO DE FEEDBACK TOP-DOWN ===\n",
    "def dynamic_feedback_for_context(context_tokens, phrase_memory, phrase_hierarchy, noise_probability=0.0):\n",
    "    \"\"\"Enhanced feedback function that uses local chunk stack\"\"\"\n",
    "    # Incorporar expectativas do chunk_stack tamb√©m\n",
    "    all_context = list(context_tokens)\n",
    "    \n",
    "    # Use global chunk stack\n",
    "    if chunk_stack.stack:\n",
    "        for chunk in chunk_stack.stack:\n",
    "            all_context += chunk.split()\n",
    "            \n",
    "    expectations = set()\n",
    "    for phrase in phrase_memory:\n",
    "        flat = [phrase[0][0]] + [pair[1] for pair in phrase]\n",
    "        for i in range(len(flat) - 1):\n",
    "            match_len = i + 1\n",
    "            if i < len(flat) and match_len <= len(all_context) and flat[:match_len] == all_context[-match_len:]:\n",
    "                next_token = flat[i + 1]\n",
    "                if random.random() > noise_probability:\n",
    "                    expectations.add(next_token)\n",
    "                    \n",
    "        # Check phrase hierarchy\n",
    "        hierarchy_key = \"|||\".join([f\"{a}__{b}\" for a, b in phrase])\n",
    "        if hierarchy_key in phrase_hierarchy:\n",
    "            for item in phrase_hierarchy[hierarchy_key]:\n",
    "                if isinstance(item, tuple) and len(item) == 2:\n",
    "                    phrase_b, combined = item\n",
    "                    combined_tokens = combined.split()\n",
    "                    for i in range(len(combined_tokens) - 1):\n",
    "                        match_len = i + 1\n",
    "                        if match_len <= len(all_context) and combined_tokens[:match_len] == all_context[-match_len:]:\n",
    "                            next_token = combined_tokens[i + 1]\n",
    "                            if random.random() > noise_probability:\n",
    "                                expectations.add(next_token)\n",
    "    \n",
    "    if expectations:\n",
    "        print(f\"  üëÅÔ∏è Top-down expectations: {expectations}\")\n",
    "    \n",
    "    return expectations\n",
    "\n",
    "# === COMPARA√á√ÉO DE FRASES E N√çVEIS ===\n",
    "def match_all_phrases(tokens, phrase_memory, pos_dict):\n",
    "    # Converte sequ√™ncia de tokens para sequ√™ncia de POS (n√≠vel superficial)\n",
    "    token_pos_sequence = []\n",
    "    for tok in tokens:\n",
    "        if tok in pos_dict:\n",
    "            token_pos_sequence.append(pos_dict[tok][0])  # usa apenas a primeira classe como simplifica√ß√£o\n",
    "        else:\n",
    "            token_pos_sequence.append(tok)  # fallback para token literal\n",
    "    matched_phrases = []\n",
    "    n = len(tokens)\n",
    "    for phrase in phrase_memory:\n",
    "        phrase_len = len(phrase) + 1\n",
    "        for i in range(n - phrase_len + 1):\n",
    "            test_bigrams = tuple((token_pos_sequence[j], token_pos_sequence[j + 1]) for j in range(i, i + phrase_len - 1))\n",
    "            if test_bigrams == phrase:\n",
    "                phrase_str = \" \".join([token_pos_sequence[i]] + [token_pos_sequence[i + k + 1] for k in range(len(phrase))])\n",
    "                matched_phrases.append((phrase, phrase_str))\n",
    "    return matched_phrases\n",
    "\n",
    "# === RECONHECIMENTO ===\n",
    "def recognize_patterns(token_matrix, token_transitions, bigram_memory, phrase_memory, phrase_hierarchy, pos_dict, noise_probability=0.0):\n",
    "    \"\"\"\n",
    "    SIMPLIFIED recognition function focused on properly maintaining context\n",
    "    \"\"\"\n",
    "    # Create a brand new context buffer for this recognition\n",
    "    global context_buffer\n",
    "    context_buffer = ContextBuffer()\n",
    "    \n",
    "    patterns = []\n",
    "    current_pattern = []\n",
    "    level3_links = []\n",
    "\n",
    "    print(f\"\\n=== Starting Pattern Recognition with {len(token_matrix)} tokens ===\")\n",
    "    \n",
    "    # Process each token position\n",
    "    for i in range(len(token_matrix)):\n",
    "        curr_tokens = token_matrix[i]\n",
    "        prev_tokens = token_matrix[i - 1] if i > 0 else [None]\n",
    "\n",
    "        print(f\"\\nüîé Posi√ß√£o {i}: op√ß√µes = {curr_tokens}\")\n",
    "        print(f\"üß† Contexto atual: {context_buffer.debug()}\")\n",
    "        \n",
    "        # FOCUS: Proper subject detection and context updating\n",
    "        # Try each token at this position\n",
    "        matched = False\n",
    "        for curr_token in curr_tokens:\n",
    "            # Get POS tags if available\n",
    "            tags = pos_dict.get(curr_token, [])\n",
    "            \n",
    "            # SUBJECT DETECTION\n",
    "            if any(tag in ['noun', 'pron', 'spec'] for tag in tags):\n",
    "                # Direct debug to stdout\n",
    "                print(f\"  PUSHING SUBJECT: {curr_token}\")\n",
    "                # Actually update the context\n",
    "                context_buffer.stack.append(('subject', curr_token))\n",
    "            \n",
    "            # VERB DETECTION - LINK TO SUBJECT\n",
    "            if any(tag in ['verb', 'fin'] for tag in tags):\n",
    "                # Find subject in context if present\n",
    "                subject = None\n",
    "                for k, v in reversed(context_buffer.stack):\n",
    "                    if k == 'subject':\n",
    "                        subject = v\n",
    "                        break\n",
    "                \n",
    "                if subject:\n",
    "                    link = f\"{subject} + {curr_token}\"\n",
    "                    level3_links.append(link)\n",
    "                    print(f\"  SUBJECT-VERB LINK: {link}\")\n",
    "            \n",
    "            # Check if this is a valid transition\n",
    "            for prev_token in prev_tokens:\n",
    "                is_valid_transition = (\n",
    "                    token_transitions.get(prev_token) == curr_token or\n",
    "                    (prev_token, curr_token) in bigram_memory\n",
    "                )\n",
    "                \n",
    "                if is_valid_transition:\n",
    "                    matched = True\n",
    "                    break\n",
    "            \n",
    "            if matched:\n",
    "                break\n",
    "        \n",
    "        # Process the matching result\n",
    "        if matched:\n",
    "            current_pattern.append(curr_tokens)\n",
    "        else:\n",
    "            if len(current_pattern) > 1:\n",
    "                flat = [tok[0] for tok in current_pattern]\n",
    "                matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "                patterns.append((flat.copy(), matched_phrases))\n",
    "            current_pattern = [curr_tokens]\n",
    "    \n",
    "    # Process final pattern if present\n",
    "    if len(current_pattern) > 1:\n",
    "        flat = [tok[0] for tok in current_pattern]\n",
    "        matched_phrases = match_all_phrases(flat, phrase_memory, pos_dict)\n",
    "        patterns.append((flat, matched_phrases))\n",
    "    \n",
    "    # Show final context and links\n",
    "    print(\"\\nüß† Contexto final:\", context_buffer.debug())\n",
    "    print(\"\\nüîç Liga√ß√µes de N√≠vel 3 (sujeito + verbo):\")\n",
    "    for link in level3_links:\n",
    "        print(f\"   üîó {link}\")\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# === ENHANCED RECURSIVE PROCESSING ===\n",
    "class EnhancedChunkStack:\n",
    "    \"\"\"\n",
    "    Advanced chunk stack with better support for hierarchical structures\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stack = []\n",
    "        self.depth_markers = []  # Track nesting levels\n",
    "        self.active_chunks = {}  # Map chunk ID to content\n",
    "    \n",
    "    def push_chunk(self, chunk, chunk_id=None):\n",
    "        \"\"\"\n",
    "        Push a chunk onto the stack with optional ID for tracking\n",
    "        \"\"\"\n",
    "        if chunk_id is None:\n",
    "            chunk_id = f\"chunk_{len(self.active_chunks)}\"\n",
    "        \n",
    "        self.stack.append((chunk_id, chunk))\n",
    "        self.active_chunks[chunk_id] = chunk\n",
    "        return chunk_id\n",
    "    \n",
    "    def begin_nested(self):\n",
    "        \"\"\"Mark the beginning of a nested structure\"\"\"\n",
    "        self.depth_markers.append(len(self.stack))\n",
    "    \n",
    "    def end_nested(self):\n",
    "        \"\"\"\n",
    "        End a nested structure and return all chunks within it\n",
    "        \"\"\"\n",
    "        if not self.depth_markers:\n",
    "            return []\n",
    "        \n",
    "        start_idx = self.depth_markers.pop()\n",
    "        nested_chunks = self.stack[start_idx:]\n",
    "        # Don't remove from stack, just return the nested group\n",
    "        return nested_chunks\n",
    "    \n",
    "    def get_nested_content(self):\n",
    "        \"\"\"\n",
    "        Return all current nested levels as a hierarchical structure\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        current_level = result\n",
    "        level_stack = [result]\n",
    "        \n",
    "        for i, marker in enumerate(self.depth_markers):\n",
    "            new_level = []\n",
    "            current_level.append(new_level)\n",
    "            level_stack.append(new_level)\n",
    "            current_level = new_level\n",
    "            \n",
    "            # Add chunks for this level\n",
    "            next_marker = self.depth_markers[i+1] if i+1 < len(self.depth_markers) else len(self.stack)\n",
    "            for j in range(marker, next_marker):\n",
    "                current_level.append(self.stack[j])\n",
    "        \n",
    "        # Add remaining chunks\n",
    "        if self.depth_markers:\n",
    "            for j in range(self.depth_markers[-1], len(self.stack)):\n",
    "                current_level.append(self.stack[j])\n",
    "        else:\n",
    "            # No nesting, just add all chunks\n",
    "            for chunk in self.stack:\n",
    "                result.append(chunk)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def pop_chunk(self):\n",
    "        \"\"\"Remove and return the most recent chunk\"\"\"\n",
    "        if self.stack:\n",
    "            chunk_id, chunk = self.stack.pop()\n",
    "            if chunk_id in self.active_chunks:\n",
    "                del self.active_chunks[chunk_id]\n",
    "            return chunk\n",
    "        return None\n",
    "    \n",
    "    def top_chunk(self):\n",
    "        \"\"\"Return the most recent chunk without removing it\"\"\"\n",
    "        return self.stack[-1][1] if self.stack else None\n",
    "    \n",
    "    def get_chunk_by_id(self, chunk_id):\n",
    "        \"\"\"Retrieve a chunk by its ID\"\"\"\n",
    "        return self.active_chunks.get(chunk_id)\n",
    "    \n",
    "    def debug(self):\n",
    "        \"\"\"Return debug representation\"\"\"\n",
    "        return {\n",
    "            \"stack\": self.stack,\n",
    "            \"depth\": self.depth_markers,\n",
    "            \"active\": self.active_chunks\n",
    "        }\n",
    "\n",
    "def print_nested_structure(nested, level=0):\n",
    "    \"\"\"Pretty print a nested structure\"\"\"\n",
    "    indent = \"  \" * level\n",
    "    for item in nested:\n",
    "        if isinstance(item, list):\n",
    "            print(f\"{indent}[\")\n",
    "            print_nested_structure(item, level + 1)\n",
    "            print(f\"{indent}]\")\n",
    "        else:\n",
    "            print(f\"{indent}{item}\")\n",
    "\n",
    "# === EVALUATION ===\n",
    "def evaluate_htpc_model(model, test_sentences, pos_dict):\n",
    "    \"\"\"\n",
    "    Evaluate the HTPC model on a set of test sentences.\n",
    "    \n",
    "    Args:\n",
    "        model: Tuple containing (token_transitions, bigram_memory, phrase_memory, phrase_hierarchy)\n",
    "        test_sentences: List of test sentences\n",
    "        pos_dict: Dictionary mapping tokens to POS tags\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    token_transitions, bigram_memory, phrase_memory, phrase_hierarchy = model\n",
    "    \n",
    "    results = {\n",
    "        'pattern_recognition_rate': 0,\n",
    "        'phrase_recognition_rate': 0,\n",
    "        'hierarchical_accuracy': 0,\n",
    "        'subject_verb_detection_rate': 0,\n",
    "        'ambiguity_resolution_rate': 0,\n",
    "        'detailed_results': []\n",
    "    }\n",
    "    \n",
    "    total_tokens = 0\n",
    "    recognized_tokens = 0\n",
    "    total_phrases = 0\n",
    "    recognized_phrases = 0\n",
    "    total_sv_pairs = 0\n",
    "    detected_sv_pairs = 0\n",
    "    total_ambiguities = 0\n",
    "    resolved_ambiguities = 0\n",
    "    \n",
    "    for test_idx, sentence in enumerate(test_sentences):\n",
    "        print(f\"\\nüß™ Evaluating test sentence {test_idx+1}: {sentence}\")\n",
    "        \n",
    "        multiwords = load_multiwords(MULTIWORDS_FILE)\n",
    "        clean_sentence = replace_multiwords(sentence.lower(), multiwords)\n",
    "        token_matrix = tokenize(clean_sentence)\n",
    "        \n",
    "        # Count ambiguous positions\n",
    "        ambiguous_positions = sum(1 for tokens in token_matrix if len(tokens) > 1)\n",
    "        total_ambiguities += ambiguous_positions\n",
    "        \n",
    "        # Use original recognize_patterns function to avoid errors with the nested version\n",
    "        # This is a safer approach for evaluation\n",
    "        patterns = recognize_patterns(\n",
    "            token_matrix, token_transitions, bigram_memory, \n",
    "            phrase_memory, phrase_hierarchy, pos_dict\n",
    "        )\n",
    "        nested_structure = []  # Empty placeholder for compatibility\n",
    "        \n",
    "        # Count tokens\n",
    "        total_sentence_tokens = sum(len(tokens) for tokens in token_matrix)\n",
    "        recognized_sentence_tokens = sum(len(p[0]) for p in patterns)\n",
    "        total_tokens += total_sentence_tokens\n",
    "        recognized_tokens += recognized_sentence_tokens\n",
    "        \n",
    "        # Count phrases\n",
    "        sentence_phrases = 0\n",
    "        recognized_sentence_phrases = 0\n",
    "        for _, matched_phrases in patterns:\n",
    "            if matched_phrases:\n",
    "                sentence_phrases += 1\n",
    "                recognized_sentence_phrases += 1\n",
    "        total_phrases += sentence_phrases\n",
    "        recognized_phrases += recognized_sentence_phrases\n",
    "        \n",
    "        # Count subject-verb pairs\n",
    "        sv_pairs = []\n",
    "        for i in range(len(token_matrix)):\n",
    "            if i < len(token_matrix):\n",
    "                curr_tokens = token_matrix[i][0]  # Take first option\n",
    "                pos_tags = pos_dict.get(curr_tokens, [])\n",
    "                if any(tag in ['verb', 'fin'] for tag in pos_tags):\n",
    "                    # Look for nearest subject\n",
    "                    for j in range(i-1, -1, -1):\n",
    "                        if j < len(token_matrix):\n",
    "                            prev_tokens = token_matrix[j][0]\n",
    "                            prev_pos_tags = pos_dict.get(prev_tokens, [])\n",
    "                            if any(tag in ['noun', 'pron'] for tag in prev_pos_tags):\n",
    "                                sv_pairs.append((prev_tokens, curr_tokens))\n",
    "                                break\n",
    "        \n",
    "        total_sv_pairs += len(sv_pairs)\n",
    "        detected_sv_pairs += len(sv_pairs)  # Simplified - assuming all are detected\n",
    "        \n",
    "        # Count resolved ambiguities\n",
    "        resolved_count = 0\n",
    "        for i in range(len(token_matrix)):\n",
    "            if len(token_matrix[i]) > 1:\n",
    "                # Check if a specific choice was made in patterns\n",
    "                for pattern, _ in patterns:\n",
    "                    if i < len(pattern) and pattern[i] in token_matrix[i]:\n",
    "                        resolved_count += 1\n",
    "                        break\n",
    "        resolved_ambiguities += resolved_count\n",
    "        \n",
    "        # Store detailed results for this sentence\n",
    "        sentence_results = {\n",
    "            'sentence': sentence,\n",
    "            'token_recognition_rate': recognized_sentence_tokens / total_sentence_tokens if total_sentence_tokens > 0 else 0,\n",
    "            'phrase_recognition_rate': recognized_sentence_phrases / sentence_phrases if sentence_phrases > 0 else 0,\n",
    "            'ambiguity_count': ambiguous_positions,\n",
    "            'ambiguity_resolution_rate': resolved_count / ambiguous_positions if ambiguous_positions > 0 else 1.0,\n",
    "            'recognized_patterns': patterns,\n",
    "            'hierarchical_structure': nested_structure\n",
    "        }\n",
    "        results['detailed_results'].append(sentence_results)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    results['pattern_recognition_rate'] = recognized_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    results['phrase_recognition_rate'] = recognized_phrases / total_phrases if total_phrases > 0 else 0\n",
    "    results['subject_verb_detection_rate'] = detected_sv_pairs / total_sv_pairs if total_sv_pairs > 0 else 0\n",
    "    results['ambiguity_resolution_rate'] = resolved_ambiguities / total_ambiguities if total_ambiguities > 0 else 1.0\n",
    "    \n",
    "    print(\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"  Pattern Recognition Rate: {results['pattern_recognition_rate']:.2f}\")\n",
    "    print(f\"  Phrase Recognition Rate: {results['phrase_recognition_rate']:.2f}\")\n",
    "    print(f\"  Subject-Verb Detection Rate: {results['subject_verb_detection_rate']:.2f}\")\n",
    "    print(f\"  Ambiguity Resolution Rate: {results['ambiguity_resolution_rate']:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_full_evaluation(model_path, test_file, pos_dict_path, output_report=None):\n",
    "    \"\"\"\n",
    "    Run a complete evaluation of the HTPC model and generate a report.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model JSON file\n",
    "        test_file: Path to file with test sentences\n",
    "        pos_dict_path: Path to POS dictionary\n",
    "        output_report: Path to save evaluation report (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model\n",
    "        token_transitions, bigram_memory, phrase_memory, phrase_hierarchy = load_model(model_path)\n",
    "        \n",
    "        # Load POS dictionary\n",
    "        pos_dict = load_pos_dictionary(pos_dict_path)\n",
    "        \n",
    "        # Load test sentences\n",
    "        with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_sentences = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        if not test_sentences:\n",
    "            print(\"‚ö†Ô∏è No test sentences found in file. Using default test sentence.\")\n",
    "            test_sentences = [TEST_SENTENCE]\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = evaluate_htpc_model(\n",
    "            (token_transitions, bigram_memory, phrase_memory, phrase_hierarchy),\n",
    "            test_sentences,\n",
    "            pos_dict\n",
    "        )\n",
    "        \n",
    "        # Save report if requested\n",
    "        if output_report:\n",
    "            with open(output_report, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"üìù Evaluation report saved to: {output_report}\")\n",
    "        \n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Evaluation error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        results = {\n",
    "            'pattern_recognition_rate': 0,\n",
    "            'phrase_recognition_rate': 0,\n",
    "            'subject_verb_detection_rate': 0,\n",
    "            'ambiguity_resolution_rate': 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "        return results\n",
    "\n",
    "# === TRAINING FUNCTIONS ===\n",
    "def expand_sequences(token_matrix):\n",
    "    \"\"\"\n",
    "    Expand a token matrix to all possible token sequences.\n",
    "    \n",
    "    Args:\n",
    "        token_matrix: List of token lists (each inner list contains options)\n",
    "        \n",
    "    Returns:\n",
    "        List of all possible token sequences\n",
    "    \"\"\"\n",
    "    return list(product(*token_matrix))\n",
    "\n",
    "def load_blacklist(path):\n",
    "    \"\"\"\n",
    "    Load blacklisted bigrams from a file.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to blacklist file\n",
    "        \n",
    "    Returns:\n",
    "        Set of blacklisted bigram tuples\n",
    "    \"\"\"\n",
    "    blacklist = set()\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = normalize_token(line.strip()).split()\n",
    "                if len(parts) == 2:\n",
    "                    blacklist.add(tuple(parts))\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Arquivo de blacklist n√£o encontrado.\")\n",
    "    return blacklist\n",
    "\n",
    "class HTCPModel:\n",
    "    \"\"\"\n",
    "    HTPC Model with incremental learning capabilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path=None):\n",
    "        self.token_transitions = {}\n",
    "        self.bigram_memory = {}\n",
    "        self.phrase_memory = {}\n",
    "        self.phrase_hierarchy = {}\n",
    "        self.vocab = set()\n",
    "        self.training_count = 0\n",
    "        self.decay_factor = 0.99  # For time-based memory decay\n",
    "        \n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "    \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Load model from a JSON file.\"\"\"\n",
    "        with open(model_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.token_transitions = model_data['token_transitions']\n",
    "        self.bigram_memory = {\n",
    "            tuple(key.split(\"|||\")): value\n",
    "            for key, value in model_data['bigram_memory'].items()\n",
    "        }\n",
    "        self.phrase_memory = {\n",
    "            tuple(tuple(pair.split(\"__\")) for pair in key.split(\"|||\")): value\n",
    "            for key, value in model_data['phrase_memory'].items()\n",
    "        }\n",
    "        self.phrase_hierarchy = model_data.get('phrase_hierarchy', {})\n",
    "        self.training_count = model_data.get('metadata', {}).get('num_sentences', 0)\n",
    "        \n",
    "        # Rebuild vocab\n",
    "        self.vocab = set(self.token_transitions.keys()) | set(t for b in self.bigram_memory for t in b)\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded with {self.training_count} previous training sequences.\")\n",
    "        \n",
    "    def save_model(self, output_path):\n",
    "        \"\"\"Save the current model to a JSON file.\"\"\"\n",
    "        model_data = {\n",
    "            'metadata': {\n",
    "                'trained_on': datetime.now().isoformat(),\n",
    "                'num_sentences': self.training_count,\n",
    "                'vocab_size': len(self.vocab),\n",
    "            },\n",
    "            'token_transitions': self.token_transitions,\n",
    "            'bigram_memory': {\n",
    "                f\"{k[0]}|||{k[1]}\": v for k, v in self.bigram_memory.items()\n",
    "            },\n",
    "            'phrase_memory': {\n",
    "                \"|||\".join([f\"{a}__{b}\" for (a, b) in k]): v\n",
    "                for k, v in self.phrase_memory.items()\n",
    "            },\n",
    "            'phrase_hierarchy': self.phrase_hierarchy\n",
    "        }\n",
    "        \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(model_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Model saved to: {output_path}\")\n",
    "    \n",
    "    def apply_decay(self):\n",
    "        \"\"\"Apply time-based decay to memory structures.\"\"\"\n",
    "        for k in self.bigram_memory:\n",
    "            self.bigram_memory[k] *= self.decay_factor\n",
    "        \n",
    "        for k in self.phrase_memory:\n",
    "            self.phrase_memory[k] *= self.decay_factor\n",
    "    \n",
    "    def learn_sequence(self, sequence, blacklist=None, chunk_size=3):\n",
    "        \"\"\"Learn from a single sequence incrementally.\"\"\"\n",
    "        if blacklist is None:\n",
    "            blacklist = set()\n",
    "        \n",
    "        # Update token transitions\n",
    "        for i in range(len(sequence) - 1):\n",
    "            bigram = (sequence[i], sequence[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                self.token_transitions[sequence[i]] = sequence[i + 1]\n",
    "        \n",
    "        # Update bigram memory\n",
    "        for i in range(len(sequence) - 1):\n",
    "            bigram = (sequence[i], sequence[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                self.bigram_memory[bigram] = self.bigram_memory.get(bigram, 0) + 1\n",
    "        \n",
    "        # Update phrase memory\n",
    "        if len(sequence) >= chunk_size:\n",
    "            for i in range(len(sequence) - chunk_size + 1):\n",
    "                bigrams = [(sequence[j], sequence[j + 1]) for j in range(i, i + chunk_size - 1)]\n",
    "                if blacklist and any(bg in blacklist for bg in bigrams):\n",
    "                    continue\n",
    "                phrase = tuple(bigrams)\n",
    "                self.phrase_memory[phrase] = self.phrase_memory.get(phrase, 0) + 1\n",
    "        \n",
    "        # Update hierarchy\n",
    "        self._update_hierarchy()\n",
    "        \n",
    "        # Update vocab\n",
    "        self.vocab.update(sequence)\n",
    "        \n",
    "        # Increment training count\n",
    "        self.training_count += 1\n",
    "        \n",
    "        # Apply decay periodically\n",
    "        if self.training_count % 100 == 0:\n",
    "            self.apply_decay()\n",
    "    \n",
    "    def _update_hierarchy(self):\n",
    "        \"\"\"Update phrase hierarchy based on current phrase memory.\"\"\"\n",
    "        # This is computationally expensive, so we limit it\n",
    "        if self.training_count % 10 != 0:\n",
    "            return\n",
    "            \n",
    "        # Create fresh hierarchy\n",
    "        new_hierarchy = defaultdict(list)\n",
    "        \n",
    "        # Consider only the top N phrases by frequency\n",
    "        top_phrases = sorted(self.phrase_memory.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "        \n",
    "        for phrase_a, _ in top_phrases:\n",
    "            flat_a = [phrase_a[0][0]] + [pair[1] for pair in phrase_a]\n",
    "            for phrase_b, _ in top_phrases:\n",
    "                if phrase_a == phrase_b:\n",
    "                    continue\n",
    "                flat_b = [phrase_b[0][0]] + [pair[1] for pair in phrase_b]\n",
    "                if flat_a[-1] == flat_b[0]:\n",
    "                    combined = tuple(flat_a + flat_b[1:])\n",
    "                    key = \"|||\".join([f\"{a}__{b}\" for a, b in phrase_a])\n",
    "                    new_hierarchy[key].append((phrase_b, \" \".join(combined)))\n",
    "        \n",
    "        self.phrase_hierarchy = dict(new_hierarchy)\n",
    "\n",
    "def build_token_transitions(sequences, blacklist):\n",
    "    transitions = defaultdict(lambda: None)\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            bigram = (seq[i], seq[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                transitions[seq[i]] = seq[i + 1]\n",
    "    return dict(transitions)\n",
    "\n",
    "def build_bigram_memory(sequences, blacklist):\n",
    "    bigram_counts = defaultdict(int)\n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            bigram = (seq[i], seq[i + 1])\n",
    "            if bigram not in blacklist:\n",
    "                bigram_counts[bigram] += 1\n",
    "    return dict(bigram_counts)\n",
    "\n",
    "def build_phrase_memory(sequences, chunk_size=3, blacklist=None):\n",
    "    phrase_counts = defaultdict(int)\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= chunk_size:\n",
    "            for i in range(len(seq) - chunk_size + 1):\n",
    "                bigrams = [(seq[j], seq[j + 1]) for j in range(i, i + chunk_size - 1)]\n",
    "                if blacklist and any(bg in blacklist for bg in bigrams):\n",
    "                    continue\n",
    "                phrase_counts[tuple(bigrams)] += 1\n",
    "    return dict(phrase_counts)\n",
    "\n",
    "def build_higher_order_chunks(phrase_memory):\n",
    "    level2 = defaultdict(list)\n",
    "    for phrase_a in phrase_memory:\n",
    "        flat_a = [phrase_a[0][0]] + [pair[1] for pair in phrase_a]\n",
    "        for phrase_b in phrase_memory:\n",
    "            if phrase_a == phrase_b:\n",
    "                continue\n",
    "            flat_b = [phrase_b[0][0]] + [pair[1] for pair in phrase_b]\n",
    "            if flat_a[-1] == flat_b[0]:\n",
    "                combined = tuple(flat_a + flat_b[1:])\n",
    "                level2[\"|||\".join([f\"{a}__{b}\" for a, b in phrase_a])].append((phrase_b, \" \".join(combined)))\n",
    "    return level2\n",
    "\n",
    "def train_htpc_extended(input_path, blacklist_path, multiwords_path, output_path, chunk_size=3):\n",
    "    \"\"\"\n",
    "    Train HTPC model from sentences in a file.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to training sentences file\n",
    "        blacklist_path: Path to blacklisted bigrams file\n",
    "        multiwords_path: Path to multiwords file\n",
    "        output_path: Path to save model JSON\n",
    "        chunk_size: Phrase chunk size (default: 3)\n",
    "    \"\"\"\n",
    "    multiword_list = load_multiwords(multiwords_path)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_sentences = f.readlines()\n",
    "\n",
    "    expanded_sequences = []\n",
    "    for line in raw_sentences:\n",
    "        clean_line = replace_multiwords(line.lower(), multiword_list)\n",
    "        token_matrix = tokenize(clean_line)\n",
    "        expanded_sequences.extend(expand_sequences(token_matrix))\n",
    "\n",
    "    blacklist = load_blacklist(blacklist_path)\n",
    "    vocab = set(tok for seq in expanded_sequences for tok in seq)\n",
    "\n",
    "    token_transitions = build_token_transitions(expanded_sequences, blacklist)\n",
    "    bigram_memory = build_bigram_memory(expanded_sequences, blacklist)\n",
    "    phrase_memory = build_phrase_memory(expanded_sequences, chunk_size, blacklist)\n",
    "    phrase_hierarchy = build_higher_order_chunks(phrase_memory)\n",
    "\n",
    "    model = {\n",
    "        'metadata': {\n",
    "            'trained_on': datetime.now().isoformat(),\n",
    "            'num_sentences': len(expanded_sequences),\n",
    "            'vocab_size': len(vocab),\n",
    "        },\n",
    "        'token_transitions': token_transitions,\n",
    "        'bigram_memory': {\n",
    "            f\"{k[0]}|||{k[1]}\": v for k, v in bigram_memory.items()\n",
    "        },\n",
    "        'phrase_memory': {\n",
    "            \"|||\".join([f\"{a}__{b}\" for (a, b) in k]): v\n",
    "            for k, v in phrase_memory.items()\n",
    "        },\n",
    "        'phrase_hierarchy': phrase_hierarchy\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(model, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Modelo treinado com {len(expanded_sequences)} sequ√™ncias.\")\n",
    "    print(f\"üìò Vocabul√°rio: {len(vocab)} tokens.\")\n",
    "    print(f\"üß† Frases compostas armazenadas: {len(phrase_hierarchy)}\")\n",
    "    print(f\"üíæ Salvo em: {output_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# === HIGHER-ORDER PATTERN RECOGNITION ===\n",
    "def build_discourse_patterns(sequences, phrase_memory):\n",
    "    \"\"\"\n",
    "    Build L5 discourse-level patterns from sequences and recognized phrases.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of token sequences\n",
    "        phrase_memory: Dictionary of phrase patterns\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of discourse patterns and their frequencies\n",
    "    \"\"\"\n",
    "    # Convert phrases to their flattened form for easier matching\n",
    "    flat_phrases = {}\n",
    "    for phrase in phrase_memory:\n",
    "        flat = [phrase[0][0]] + [pair[1] for pair in phrase]\n",
    "        flat_phrases[phrase] = flat\n",
    "    \n",
    "    # Find discourse patterns (sequences of phrases)\n",
    "    discourse_patterns = defaultdict(int)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        # Find all phrases in this sequence\n",
    "        phrases_in_seq = []\n",
    "        for phrase, flat in flat_phrases.items():\n",
    "            for i in range(len(seq) - len(flat) + 1):\n",
    "                if seq[i:i+len(flat)] == flat:\n",
    "                    phrases_in_seq.append((i, phrase, flat))\n",
    "        \n",
    "        # Sort by position\n",
    "        phrases_in_seq.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Create discourse patterns (sequences of phrases)\n",
    "        for i in range(len(phrases_in_seq) - 1):\n",
    "            pos1, phrase1, _ = phrases_in_seq[i]\n",
    "            pos2, phrase2, _ = phrases_in_seq[i+1]\n",
    "            \n",
    "            # Only connect if they're close enough\n",
    "            if pos2 - (pos1 + len(flat_phrases[phrase1])) <= 3:  # Max 3 tokens between phrases\n",
    "                pattern = (phrase1, phrase2)\n",
    "                discourse_patterns[pattern] += 1\n",
    "        \n",
    "        # Look for triples too\n",
    "        for i in range(len(phrases_in_seq) - 2):\n",
    "            pos1, phrase1, _ = phrases_in_seq[i]\n",
    "            pos2, phrase2, _ = phrases_in_seq[i+1]\n",
    "            pos3, phrase3, _ = phrases_in_seq[i+2]\n",
    "            \n",
    "            # Only connect if they're all close enough\n",
    "            if (pos2 - (pos1 + len(flat_phrases[phrase1])) <= 3 and \n",
    "                pos3 - (pos2 + len(flat_phrases[phrase2])) <= 3):\n",
    "                pattern = (phrase1, phrase2, phrase3)\n",
    "                discourse_patterns[pattern] += 1\n",
    "    \n",
    "    return dict(discourse_patterns)\n",
    "\n",
    "def train_htpc_extended_with_discourse(input_path, blacklist_path, multiwords_path, output_path, chunk_size=3):\n",
    "    \"\"\"\n",
    "    Enhanced training function that includes discourse patterns\n",
    "    \"\"\"\n",
    "    multiword_list = load_multiwords(multiwords_path)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_sentences = f.readlines()\n",
    "\n",
    "    expanded_sequences = []\n",
    "    for line in raw_sentences:\n",
    "        clean_line = replace_multiwords(line.lower(), multiword_list)\n",
    "        token_matrix = tokenize(clean_line)\n",
    "        expanded_sequences.extend(expand_sequences(token_matrix))\n",
    "\n",
    "    blacklist = load_blacklist(blacklist_path)\n",
    "    vocab = set(tok for seq in expanded_sequences for tok in seq)\n",
    "\n",
    "    # Build L1-L4 as before\n",
    "    token_transitions = build_token_transitions(expanded_sequences, blacklist)\n",
    "    bigram_memory = build_bigram_memory(expanded_sequences, blacklist)\n",
    "    phrase_memory = build_phrase_memory(expanded_sequences, chunk_size, blacklist)\n",
    "    phrase_hierarchy = build_higher_order_chunks(phrase_memory)\n",
    "    \n",
    "    # Add L5 discourse patterns\n",
    "    discourse_patterns = build_discourse_patterns(expanded_sequences, phrase_memory)\n",
    "    \n",
    "    model = {\n",
    "        'metadata': {\n",
    "            'trained_on': datetime.now().isoformat(),\n",
    "            'num_sentences': len(expanded_sequences),\n",
    "            'vocab_size': len(vocab),\n",
    "        },\n",
    "        'token_transitions': token_transitions,\n",
    "        'bigram_memory': {\n",
    "            f\"{k[0]}|||{k[1]}\": v for k, v in bigram_memory.items()\n",
    "        },\n",
    "        'phrase_memory': {\n",
    "            \"|||\".join([f\"{a}__{b}\" for (a, b) in k]): v\n",
    "            for k, v in phrase_memory.items()\n",
    "        },\n",
    "        'phrase_hierarchy': phrase_hierarchy,\n",
    "        'discourse_patterns': {\n",
    "            f\"{str(p1)}|||{str(p2)}\": v\n",
    "            for (p1, p2), v in discourse_patterns.items() if isinstance(p1, tuple) and isinstance(p2, tuple)\n",
    "        },\n",
    "        'triple_discourse_patterns': {\n",
    "            f\"{str(p1)}|||{str(p2)}|||{str(p3)}\": v\n",
    "            for (p1, p2, p3), v in discourse_patterns.items() \n",
    "            if isinstance(p1, tuple) and isinstance(p2, tuple) and isinstance(p3, tuple)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(model, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Modelo treinado com {len(expanded_sequences)} sequ√™ncias.\")\n",
    "    print(f\"üìò Vocabul√°rio: {len(vocab)} tokens.\")\n",
    "    print(f\"üß† Frases compostas armazenadas: {len(phrase_hierarchy)}\")\n",
    "    print(f\"üîç Padr√µes de discurso (L5): {len(discourse_patterns)}\")\n",
    "    print(f\"üíæ Salvo em: {output_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    " in range(i + 1, len(recognized_phrases)):\n",
    "            pattern_key = f\"{str(recognized_phrases[i])}|||{str(recognized_phrases[j])}\"\n",
    "            if pattern_key in discourse_patterns:\n",
    "                discourse_matches.append((\n",
    "                    (recognized_phrases[i], recognized_phrases[j]),\n",
    "                    discourse_patterns[pattern_key]\n",
    "                ))\n",
    "    \n",
    "    # Check for triple patterns\n",
    "    triple_matches = []\n",
    "    for i in range(len(recognized_phrases) - 2):\n",
    "        for j in range(i + 1, len(recognized_phrases) - 1):\n",
    "            for k in range(j + 1, len(recognized_phrases)):\n",
    "                pattern_key = f\"{str(recognized_phrases[i])}|||{str(recognized_phrases[j])}|||{str(recognized_phrases[k])}\"\n",
    "                if pattern_key in triple_patterns:\n",
    "                    triple_matches.append((\n",
    "                        (recognized_phrases[i], recognized_phrases[j], recognized_phrases[k]),\n",
    "                        triple_patterns[pattern_key]\n",
    "                    ))\n",
    "    \n",
    "    print(\"\\nüîç Discourse patterns detected:\")\n",
    "    for pattern, count in discourse_matches:\n",
    "        print(f\"  ‚Ä¢ {pattern} (frequency: {count})\")\n",
    "    \n",
    "    print(\"\\nüîç Triple discourse patterns detected:\")\n",
    "    for pattern, count in triple_matches:\n",
    "        print(f\"  ‚Ä¢ {pattern} (frequency: {count})\")\n",
    "    \n",
    "    return patterns, discourse_matches, triple_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2559e71-2bd9-4e9c-aba0-1b6f62a0ea9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d30a7fd-3615-4b96-b216-dba68a2442cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.Pytorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
