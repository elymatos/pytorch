{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894b615-468d-4d12-822f-9cff764abb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-GAT Semantic Relation Inference for New Word\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.nn import GATConv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Load Vocabulary and Relations ---\n",
    "with open(\"words.txt\") as f:\n",
    "    words = [line.strip() for line in f if line.strip()]\n",
    "relations_df = pd.read_csv(\"relations.csv\")\n",
    "relations = list(relations_df.itertuples(index=False, name=None))\n",
    "\n",
    "# --- Rebuild word2idx and label encoder ---\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "rel_encoder = LabelEncoder()\n",
    "rel_encoder.fit([r[1] for r in relations])\n",
    "num_rels = len(rel_encoder.classes_)\n",
    "\n",
    "# --- Reload XLM-R model ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "xlmr = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "xlmr.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_word_embedding(word):\n",
    "    input_ids = tokenizer.encode(word, return_tensors=\"pt\")\n",
    "    outputs = xlmr(input_ids)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "\n",
    "# --- Re-encode all known word embeddings ---\n",
    "embeddings = torch.stack([get_word_embedding(w) for w in words])\n",
    "\n",
    "# --- Define RGAT Model (same as before) ---\n",
    "class RGAT(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, num_rels, dropout=0.2, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.ln = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.gats = torch.nn.ModuleList([\n",
    "            torch.nn.ModuleList([\n",
    "                GATConv(in_dim if l == 0 else hidden_dim, hidden_dim, heads=1, concat=False)\n",
    "                for _ in range(num_rels)\n",
    "            ]) for l in range(num_layers)\n",
    "        ])\n",
    "        self.out_proj = torch.nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        for layer in self.gats:\n",
    "            out = torch.zeros(x.size(0), layer[0].out_channels, device=x.device)\n",
    "            for rel_id, conv in enumerate(layer):\n",
    "                mask = edge_type == rel_id\n",
    "                if mask.sum() == 0:\n",
    "                    continue\n",
    "                rel_edges = edge_index[:, mask]\n",
    "                out += conv(x, rel_edges)\n",
    "            x = self.ln(self.dropout(F.relu(out)))\n",
    "        return self.out_proj(x)\n",
    "\n",
    "# --- Define Edge Classifier ---\n",
    "class EdgeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(2 * in_dim, num_classes)\n",
    "\n",
    "    def forward(self, src, dst):\n",
    "        return self.fc(torch.cat([src, dst], dim=1))\n",
    "\n",
    "# --- Load saved models ---\n",
    "input_dim = embeddings.size(1)\n",
    "encoder = RGAT(input_dim, 256, input_dim, num_rels)\n",
    "classifier = EdgeClassifier(input_dim, num_rels + 1)\n",
    "encoder.load_state_dict(torch.load(\"rgat_encoder.pt\"))\n",
    "classifier.load_state_dict(torch.load(\"rgat_classifier.pt\"))\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "# --- Inference Function ---\n",
    "def predict_relations(new_word, k=3):\n",
    "    new_emb = get_word_embedding(new_word).unsqueeze(0)\n",
    "    all_emb = torch.cat([embeddings, new_emb], dim=0)\n",
    "    with torch.no_grad():\n",
    "        h = encoder(all_emb, edge_index, edge_type)\n",
    "        new_h = h[-1]\n",
    "        sims = F.cosine_similarity(new_h.unsqueeze(0), h[:-1])\n",
    "        topk = torch.topk(sims, k)\n",
    "\n",
    "        print(f\"Top {k} semantic predictions for '{new_word}':\")\n",
    "        for idx in topk.indices:\n",
    "            rel_logits = classifier(new_h.unsqueeze(0), h[idx].unsqueeze(0))\n",
    "            rel_pred = rel_logits.argmax().item()\n",
    "            rel_label = rel_encoder.inverse_transform([rel_pred])[0] if rel_pred < num_rels else \"no-relation\"\n",
    "            print(f\"{new_word} --{rel_label}--> {words[idx]}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "predict_relations(\"wolf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.Pytorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
