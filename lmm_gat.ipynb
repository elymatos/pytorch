{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# üì¶ Install Required Libraries\n",
    "# =============================\n",
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install torch-geometric --quiet\n",
    "!pip install transformers --quiet\n",
    "\n",
    "# =============================\n",
    "# üìö Imports\n",
    "# =============================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GATConv\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================\n",
    "# üì• Load the Cora Dataset\n",
    "# =============================\n",
    "dataset = Planetoid(root='data/Cora', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "# =============================\n",
    "# üß† Simulated Texts per Node\n",
    "# =============================\n",
    "texts = [f\"paper about topic {int(label)}\" for label in data.y.tolist()]\n",
    "\n",
    "# =============================\n",
    "# üî° Encode Text with BERT\n",
    "# =============================\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert.eval()\n",
    "\n",
    "def embed_texts(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=32)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "bert_embeds = embed_texts(texts)\n",
    "bert_embeds = torch.tensor(normalize(bert_embeds), dtype=torch.float)\n",
    "\n",
    "# =============================\n",
    "# üß† Define GAT + BERT Fusion Model\n",
    "# =============================\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.6)\n",
    "        self.gat2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class BERT_GAT_Fusion(nn.Module):\n",
    "    def __init__(self, gnn_in, gnn_hidden, gnn_out, bert_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.gnn = GAT(gnn_in, gnn_hidden, gnn_out)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(gnn_out + bert_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, data, bert_embed):\n",
    "        gnn_out = self.gnn(data.x, data.edge_index)\n",
    "        combined = torch.cat([bert_embed, gnn_out], dim=1)\n",
    "        return self.mlp(combined)\n",
    "\n",
    "# =============================\n",
    "# üèãÔ∏è Train Model\n",
    "# =============================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERT_GAT_Fusion(\n",
    "    gnn_in=data.x.size(1),\n",
    "    gnn_hidden=64,\n",
    "    gnn_out=64,\n",
    "    bert_dim=bert_embeds.size(1),\n",
    "    num_classes=dataset.num_classes\n",
    ").to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "bert_embeds = bert_embeds.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data, bert_embeds)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# =============================\n",
    "# üìà Plot Training Loss\n",
    "# =============================\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss (BERT + GAT Fusion)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# =============================\n",
    "# ‚úÖ Evaluation\n",
    "# =============================\n",
    "model.eval()\n",
    "pred = model(data, bert_embeds).argmax(dim=1)\n",
    "correct = int((pred[data.test_mask] == data.y[data.test_mask]).sum())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
