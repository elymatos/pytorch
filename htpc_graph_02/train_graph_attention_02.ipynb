{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2238a2-861d-48eb-9448-ff9fdecf334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 15 sentences\n",
      "Training on 15 sequences for 3 epochs\n",
      "Epoch 1/3\n",
      "  Average surprisal: 64.6667\n",
      "Epoch 2/3\n",
      "  Average surprisal: 7.8583\n",
      "Epoch 3/3\n",
      "  Average surprisal: 7.8583\n",
      "Found 83 potential chunks\n",
      "27 chunks met frequency criteria, 16 met cohesion criteria\n",
      "Training complete.\n",
      "POS graph has 10 nodes and 27 edges\n",
      "Chunk graph has 16 nodes and 139 edges\n",
      "\n",
      "Top 5 POS tags by attention:\n",
      "  <END>: 1.000\n",
      "  CONJ: 0.941\n",
      "  PRON: 0.922\n",
      "  PREP: 0.890\n",
      "  ADV: 0.859\n",
      "\n",
      "Bottom 5 POS tags by attention:\n",
      "  ADJ: 0.845\n",
      "  <START>: 0.576\n",
      "  VERB: 0.516\n",
      "  DET: 0.409\n",
      "  NOUN: 0.369\n",
      "\n",
      "Top 5 chunks by attention:\n",
      "  ('CONJ', 'VERB'): 0.971\n",
      "  ('PRON', 'VERB'): 0.961\n",
      "  ('PREP', 'DET'): 0.945\n",
      "  ('ADJ', 'NOUN'): 0.923\n",
      "  ('PRON', 'VERB', 'PREP'): 0.728\n",
      "\n",
      "Test sentence: ['DET', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN']\n",
      "\n",
      "Recognized chunks:\n",
      "  ('ADJ', 'NOUN') (Position 1-3, Activation: 0.716)\n",
      "  ('PREP', 'DET') (Position 4-6, Activation: 0.747)\n",
      "\n",
      "Final segmentation: [['DET'], ['ADJ', 'NOUN'], ['VERB'], ['PREP', 'DET'], ['NOUN']]\n",
      "\n",
      "Top predictions after ['DET', 'ADJ']:\n",
      "  NOUN: 0.74 (Attention: 0.37)\n",
      "  CONJ: 0.13 (Attention: 0.94)\n",
      "  ADJ: 0.13 (Attention: 0.85)\n",
      "POS graph visualization saved to pos_graph_attention.png\n",
      "Chunk graph visualization saved to chunk_graph_attention.png\n",
      "Learning progress plot saved to learning_progress.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class POSGraphWithAttention:\n",
    "    \"\"\"\n",
    "    Implements a graph-based structure for POS sequence processing with predictive coding\n",
    "    and attention mechanisms for dynamic precision weighting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predefined_boundaries: Optional[Dict[Tuple[str, str], float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the POS graph with attention mechanisms.\n",
    "        \n",
    "        Args:\n",
    "            predefined_boundaries: Optional dictionary of predefined boundary probabilities\n",
    "        \"\"\"\n",
    "        # Main transition graph\n",
    "        self.graph = nx.DiGraph()\n",
    "        \n",
    "        # Higher-order chunk graph for learned patterns\n",
    "        self.chunk_graph = nx.DiGraph()\n",
    "        \n",
    "        # Track n-gram counts for training\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "        \n",
    "        # Boundary probabilities\n",
    "        self.boundary_probs = defaultdict(float)\n",
    "        \n",
    "        # Predefined linguistic rules\n",
    "        self.predefined_boundaries = predefined_boundaries or {\n",
    "            ('NOUN', 'VERB'): 0.9,      # NP to VP transition\n",
    "            ('VERB', 'DET'): 0.8,       # VP to NP transition\n",
    "            ('PUNCT', 'DET'): 0.95,     # Punctuation followed by determiner\n",
    "            ('NOUN', 'PREP'): 0.7,      # NP to PP transition\n",
    "            ('VERB', 'PREP'): 0.6,      # VP to PP transition\n",
    "            ('ADJ', 'NOUN'): 0.2,       # Within NP (low boundary probability)\n",
    "            ('DET', 'ADJ'): 0.1,        # Within NP (very low boundary probability)\n",
    "        }\n",
    "        \n",
    "        # Thresholds\n",
    "        self.hard_boundary_threshold = 0.75\n",
    "        self.soft_boundary_threshold = 0.4\n",
    "        \n",
    "        # Discovered chunks\n",
    "        self.common_chunks = {}\n",
    "        \n",
    "        # Add special start and end nodes\n",
    "        self.graph.add_node(\"<START>\", pos_type=\"special\")\n",
    "        self.graph.add_node(\"<END>\", pos_type=\"special\")\n",
    "        \n",
    "        # Attention mechanisms\n",
    "        self.attention_weights = {}  # POS tag attention weights\n",
    "        self.chunk_attention_weights = {}  # Chunk attention weights\n",
    "        self.surprisal_history = []  # Track surprisal for adaptive attention\n",
    "        self.learning_rate = 0.1  # Base learning rate\n",
    "        self.attention_learning_rate = 0.05  # For updating attention weights\n",
    "        \n",
    "        # Precision weighting parameters\n",
    "        self.base_precision = 1.0\n",
    "        self.max_precision = 5.0\n",
    "        self.min_precision = 0.2\n",
    "        \n",
    "        # Context tracking\n",
    "        self.context_history = []  # Track recent contexts for attention modulation\n",
    "\n",
    "    def train(self, pos_sequences: List[List[str]], epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Train the POS graph on a corpus of POS tag sequences with attention.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequences: List of POS tag sequences, each representing a sentence\n",
    "            epochs: Number of training epochs\n",
    "        \"\"\"\n",
    "        print(f\"Training on {len(pos_sequences)} sequences for {epochs} epochs\")\n",
    "        \n",
    "        # 1. Initialize attention weights uniformly\n",
    "        self._initialize_attention_weights(pos_sequences)\n",
    "        \n",
    "        # 2. Build initial graph and collect n-gram statistics\n",
    "        self._build_initial_graph(pos_sequences)\n",
    "        \n",
    "        # 3. Iterative training with attention modulation\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Shuffle sequences for stochastic training\n",
    "            random.shuffle(pos_sequences)\n",
    "            \n",
    "            epoch_surprisal = 0.0\n",
    "            \n",
    "            # Process each sequence with attention\n",
    "            for sequence in pos_sequences:\n",
    "                # Forward pass - calculate predictions and errors\n",
    "                sequence_surprisal, prediction_errors = self._forward_pass(sequence)\n",
    "                epoch_surprisal += sequence_surprisal\n",
    "                \n",
    "                # Update attention weights based on prediction errors\n",
    "                self._update_attention_weights(sequence, prediction_errors)\n",
    "                \n",
    "                # Update graph weights with attention-modulated learning\n",
    "                self._update_graph_weights(sequence, prediction_errors)\n",
    "            \n",
    "            avg_surprisal = epoch_surprisal / len(pos_sequences)\n",
    "            self.surprisal_history.append(avg_surprisal)\n",
    "            print(f\"  Average surprisal: {avg_surprisal:.4f}\")\n",
    "            \n",
    "            # Recalculate edge weights based on updated counts\n",
    "            self._calculate_edge_weights()\n",
    "            \n",
    "            # After each epoch, update boundary probabilities\n",
    "            self._calculate_boundary_probabilities()\n",
    "        \n",
    "        # 4. Identify common chunks with attention influence\n",
    "        self._identify_common_chunks(pos_sequences)\n",
    "        \n",
    "        # 5. Build chunk graph with attention-weighted connections\n",
    "        self._build_chunk_graph()\n",
    "        \n",
    "        print(f\"Training complete.\")\n",
    "        print(f\"POS graph has {len(self.graph.nodes)} nodes and {len(self.graph.edges)} edges\")\n",
    "        print(f\"Chunk graph has {len(self.chunk_graph.nodes)} nodes and {len(self.chunk_graph.edges)} edges\")\n",
    "        \n",
    "        # Report top attention weights\n",
    "        self._report_attention_weights()\n",
    "\n",
    "    def _initialize_attention_weights(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Initialize attention weights for all POS tags.\"\"\"\n",
    "        # Extract all unique POS tags from sequences\n",
    "        unique_pos = set()\n",
    "        for sequence in pos_sequences:\n",
    "            unique_pos.update(sequence)\n",
    "        \n",
    "        # Initialize with uniform weights\n",
    "        for pos in unique_pos:\n",
    "            self.attention_weights[pos] = 1.0\n",
    "            \n",
    "        # Special start and end nodes\n",
    "        self.attention_weights[\"<START>\"] = 1.0\n",
    "        self.attention_weights[\"<END>\"] = 1.0\n",
    "\n",
    "    def _build_initial_graph(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Build the initial graph structure and collect statistics.\"\"\"\n",
    "        # First pass - add all nodes and count statistics\n",
    "        for sequence in pos_sequences:\n",
    "            # Add nodes for each unique POS tag\n",
    "            for pos in sequence:\n",
    "                if not self.graph.has_node(pos):\n",
    "                    self.graph.add_node(pos, pos_type=\"basic\", precision=self.base_precision)\n",
    "                self.unigram_counts[pos] += 1\n",
    "            \n",
    "            # Count bigrams and add edges\n",
    "            for i in range(len(sequence) - 1):\n",
    "                pos1, pos2 = sequence[i], sequence[i+1]\n",
    "                self.bigram_counts[pos1][pos2] += 1\n",
    "                \n",
    "                # Ensure edge exists (weight will be calculated later)\n",
    "                if not self.graph.has_edge(pos1, pos2):\n",
    "                    self.graph.add_edge(pos1, pos2, weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                \n",
    "                # Increment edge count\n",
    "                self.graph[pos1][pos2][\"count\"] += 1\n",
    "            \n",
    "            # Add connections from start and to end\n",
    "            if sequence:\n",
    "                if not self.graph.has_edge(\"<START>\", sequence[0]):\n",
    "                    self.graph.add_edge(\"<START>\", sequence[0], weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                self.graph[\"<START>\"][sequence[0]][\"count\"] += 1\n",
    "                \n",
    "                if not self.graph.has_edge(sequence[-1], \"<END>\"):\n",
    "                    self.graph.add_edge(sequence[-1], \"<END>\", weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                self.graph[sequence[-1]][\"<END>\"][\"count\"] += 1\n",
    "            \n",
    "            # Count trigrams\n",
    "            for i in range(len(sequence) - 2):\n",
    "                pos1, pos2, pos3 = sequence[i], sequence[i+1], sequence[i+2]\n",
    "                self.trigram_counts[pos1][pos2][pos3] += 1\n",
    "\n",
    "    def _forward_pass(self, sequence: List[str]) -> Tuple[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Process a sequence and calculate prediction errors with current model.\n",
    "        \n",
    "        Args:\n",
    "            sequence: A sequence of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (total_surprisal, list_of_prediction_errors)\n",
    "        \"\"\"\n",
    "        total_surprisal = 0.0\n",
    "        prediction_errors = []\n",
    "        \n",
    "        # Start with START node\n",
    "        current_pos = \"<START>\"\n",
    "        \n",
    "        # Process each position in the sequence\n",
    "        for pos in sequence:\n",
    "            # Calculate prediction probability for this position\n",
    "            prediction_prob = 0.0\n",
    "            if self.graph.has_edge(current_pos, pos):\n",
    "                prediction_prob = self.graph[current_pos][pos].get(\"weight\", 0.0)\n",
    "            \n",
    "            # Calculate surprisal (-log probability)\n",
    "            if prediction_prob > 0:\n",
    "                surprisal = -math.log2(prediction_prob)\n",
    "            else:\n",
    "                surprisal = 10.0  # High surprisal for unseen transitions\n",
    "                \n",
    "            # Get current precision for this transition\n",
    "            precision = self.base_precision\n",
    "            if self.graph.has_edge(current_pos, pos):\n",
    "                precision = self.graph[current_pos][pos].get(\"precision\", self.base_precision)\n",
    "            \n",
    "            # Calculate precision-weighted prediction error\n",
    "            prediction_error = surprisal * precision\n",
    "            \n",
    "            prediction_errors.append(prediction_error)\n",
    "            total_surprisal += surprisal\n",
    "            \n",
    "            # Update current position\n",
    "            current_pos = pos\n",
    "        \n",
    "        return total_surprisal, prediction_errors\n",
    "\n",
    "    def _update_attention_weights(self, sequence: List[str], prediction_errors: List[float]):\n",
    "        \"\"\"\n",
    "        Update attention weights based on prediction errors.\n",
    "        Higher errors lead to increased attention.\n",
    "        \n",
    "        Args:\n",
    "            sequence: The POS sequence\n",
    "            prediction_errors: Corresponding prediction errors for transitions\n",
    "        \"\"\"\n",
    "        # Normalize prediction errors to [0,1] range for attention updates\n",
    "        if prediction_errors:\n",
    "            max_error = max(prediction_errors)\n",
    "            if max_error > 0:\n",
    "                normalized_errors = [error / max_error for error in prediction_errors]\n",
    "            else:\n",
    "                normalized_errors = [0.0] * len(prediction_errors)\n",
    "            \n",
    "            # Update attention for START node and first token\n",
    "            self.attention_weights[\"<START>\"] = (1 - self.attention_learning_rate) * self.attention_weights[\"<START>\"] + \\\n",
    "                                      self.attention_learning_rate * normalized_errors[0]\n",
    "            \n",
    "            # Update attention weights for each POS tag based on prediction errors\n",
    "            for i, pos in enumerate(sequence):\n",
    "                # Current position's error influences its attention\n",
    "                if i < len(normalized_errors):\n",
    "                    error_weight = normalized_errors[i]\n",
    "                    \n",
    "                    # Update attention weight with learning rate\n",
    "                    self.attention_weights[pos] = (1 - self.attention_learning_rate) * self.attention_weights.get(pos, 1.0) + \\\n",
    "                                          self.attention_learning_rate * error_weight\n",
    "                    \n",
    "                    # Ensure attention weights stay in reasonable range\n",
    "                    self.attention_weights[pos] = max(0.2, min(3.0, self.attention_weights[pos]))\n",
    "\n",
    "    def _update_graph_weights(self, sequence: List[str], prediction_errors: List[float]):\n",
    "        \"\"\"\n",
    "        Update graph edge weights based on attention-modulated learning.\n",
    "        \n",
    "        Args:\n",
    "            sequence: The POS sequence\n",
    "            prediction_errors: Corresponding prediction errors\n",
    "        \"\"\"\n",
    "        current_pos = \"<START>\"\n",
    "        \n",
    "        for i, pos in enumerate(sequence):\n",
    "            # Get attention weights for current and next position\n",
    "            current_attention = self.attention_weights.get(current_pos, 1.0)\n",
    "            target_attention = self.attention_weights.get(pos, 1.0)\n",
    "            \n",
    "            # Combined attention effect (geometric mean)\n",
    "            combined_attention = math.sqrt(current_attention * target_attention)\n",
    "            \n",
    "            # Attention-modulated learning rate\n",
    "            effective_lr = self.learning_rate * combined_attention\n",
    "            \n",
    "            # Update precision for this transition based on prediction error\n",
    "            if i < len(prediction_errors):\n",
    "                error = prediction_errors[i]\n",
    "                new_precision = self.base_precision * (1 + 0.1 * error)\n",
    "                new_precision = max(self.min_precision, min(self.max_precision, new_precision))\n",
    "                \n",
    "                if self.graph.has_edge(current_pos, pos):\n",
    "                    # Slowly update precision for this edge\n",
    "                    old_precision = self.graph[current_pos][pos].get(\"precision\", self.base_precision)\n",
    "                    updated_precision = 0.9 * old_precision + 0.1 * new_precision\n",
    "                    self.graph[current_pos][pos][\"precision\"] = updated_precision\n",
    "            \n",
    "            # Update current position\n",
    "            current_pos = pos\n",
    "\n",
    "    def _calculate_edge_weights(self):\n",
    "        \"\"\"Calculate edge weights (transition probabilities) based on counts.\"\"\"\n",
    "        # For each node, calculate outgoing transition probabilities\n",
    "        for node in self.graph.nodes():\n",
    "            if node == \"<END>\":\n",
    "                continue  # End node has no outgoing edges\n",
    "                \n",
    "            # Get total count of outgoing transitions\n",
    "            outgoing_edges = list(self.graph.out_edges(node, data=True))\n",
    "            total_count = sum(data[\"count\"] for _, _, data in outgoing_edges)\n",
    "            \n",
    "            if total_count > 0:\n",
    "                # Calculate probability for each outgoing edge\n",
    "                for _, target, data in outgoing_edges:\n",
    "                    prob = data[\"count\"] / total_count\n",
    "                    self.graph[node][target][\"weight\"] = prob\n",
    "\n",
    "    def _calculate_boundary_probabilities(self):\n",
    "        \"\"\"Calculate boundary probabilities with attention influence.\"\"\"\n",
    "        for source, target, data in self.graph.edges(data=True):\n",
    "            if source in (\"<START>\", \"<END>\") or target in (\"<START>\", \"<END>\"):\n",
    "                continue  # Skip special nodes\n",
    "                \n",
    "            # Calculate surprisal for this transition\n",
    "            prob = data.get(\"weight\", 0)\n",
    "            if prob > 0:\n",
    "                surprisal = -math.log2(prob)\n",
    "                \n",
    "                # Get attention weights for source and target\n",
    "                source_attention = self.attention_weights.get(source, 1.0)\n",
    "                target_attention = self.attention_weights.get(target, 1.0)\n",
    "                \n",
    "                # Attention-modulated boundary probability\n",
    "                # Higher attention at either end means more salient boundary\n",
    "                attention_factor = (source_attention + target_attention) / 2\n",
    "                \n",
    "                # Normalize surprisal to a boundary probability between 0 and 1\n",
    "                raw_boundary_prob = 1 / (1 + math.exp(-(surprisal - 1)))\n",
    "                \n",
    "                # Adjust boundary probability based on attention\n",
    "                boundary_prob = raw_boundary_prob * attention_factor\n",
    "                \n",
    "                # Consider predefined boundaries if available\n",
    "                if (source, target) in self.predefined_boundaries:\n",
    "                    predefined_prob = self.predefined_boundaries[(source, target)]\n",
    "                    alpha = 0.3  # Weight for predefined rules\n",
    "                    boundary_prob = alpha * predefined_prob + (1 - alpha) * boundary_prob\n",
    "                \n",
    "                # Store in graph and in lookup dictionary\n",
    "                boundary_prob = max(0.0, min(1.0, boundary_prob))  # Ensure [0,1] range\n",
    "                self.graph[source][target][\"boundary_prob\"] = boundary_prob\n",
    "                self.boundary_probs[(source, target)] = boundary_prob\n",
    "\n",
    "    def _identify_common_chunks(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Identify common chunks using attention-weighted statistics.\"\"\"\n",
    "        # Use a sliding window approach to find potential chunks\n",
    "        chunk_candidates = Counter()\n",
    "        \n",
    "        # Try different chunk sizes\n",
    "        for size in range(2, 5):  # 2-grams to 4-grams\n",
    "            for sequence in pos_sequences:\n",
    "                if len(sequence) < size:\n",
    "                    continue\n",
    "                    \n",
    "                for i in range(len(sequence) - size + 1):\n",
    "                    chunk = tuple(sequence[i:i+size])\n",
    "                    chunk_candidates[chunk] += 1\n",
    "        \n",
    "        print(f\"Found {len(chunk_candidates)} potential chunks\")\n",
    "        \n",
    "        # For small training sets, lower the threshold\n",
    "        total_sentences = len(pos_sequences)\n",
    "        min_occurrences = max(2, int(total_sentences * 0.05))\n",
    "        \n",
    "        # Lower the cohesion threshold for small datasets\n",
    "        cohesion_threshold = 0.6 if total_sentences < 20 else 0.7\n",
    "        \n",
    "        # Count qualifying chunks\n",
    "        qualifying_chunks = 0\n",
    "        for chunk, count in chunk_candidates.items():\n",
    "            if count >= min_occurrences:\n",
    "                qualifying_chunks += 1\n",
    "                \n",
    "                # Calculate internal cohesion with attention weighting\n",
    "                internal_boundaries = 0\n",
    "                chunk_attention = 1.0  # Start with neutral attention\n",
    "                \n",
    "                for i in range(len(chunk) - 1):\n",
    "                    pos1, pos2 = chunk[i], chunk[i+1]\n",
    "                    \n",
    "                    # Get boundary probability\n",
    "                    boundary_prob = self.boundary_probs.get((pos1, pos2), 0.5)\n",
    "                    \n",
    "                    # Apply attention weighting - average of the two positions\n",
    "                    pos1_attention = self.attention_weights.get(pos1, 1.0)\n",
    "                    pos2_attention = self.attention_weights.get(pos2, 1.0)\n",
    "                    avg_attention = (pos1_attention + pos2_attention) / 2\n",
    "                    \n",
    "                    # Accumulate attention-weighted boundary probability\n",
    "                    internal_boundaries += boundary_prob\n",
    "                    \n",
    "                    # Calculate overall chunk attention (product of position attentions)\n",
    "                    chunk_attention *= (pos1_attention * 0.5 + 0.5)  # Dampen the effect\n",
    "                \n",
    "                avg_internal_boundary = internal_boundaries / (len(chunk) - 1)\n",
    "                cohesion = 1 - avg_internal_boundary\n",
    "                \n",
    "                # Boost cohesion for chunks with high attention\n",
    "                attention_adjusted_cohesion = cohesion * (0.8 + 0.2 * chunk_attention)\n",
    "                \n",
    "                # Only keep reasonably cohesive chunks\n",
    "                if attention_adjusted_cohesion > cohesion_threshold:\n",
    "                    chunk_name = f\"{'_'.join(chunk)}\"\n",
    "                    self.common_chunks[chunk] = {\n",
    "                        \"name\": chunk_name,\n",
    "                        \"elements\": chunk,\n",
    "                        \"count\": count,\n",
    "                        \"cohesion\": attention_adjusted_cohesion,\n",
    "                        \"attention\": chunk_attention,\n",
    "                        \"activation\": 0.0  # Initial activation level\n",
    "                    }\n",
    "                    \n",
    "                    # Initialize chunk attention weight\n",
    "                    self.chunk_attention_weights[chunk] = chunk_attention\n",
    "                    \n",
    "        print(f\"{qualifying_chunks} chunks met frequency criteria, {len(self.common_chunks)} met cohesion criteria\")\n",
    "\n",
    "    def _build_chunk_graph(self):\n",
    "        \"\"\"Build higher-order graph representing transitions between chunks.\"\"\"\n",
    "        # Add nodes for each chunk\n",
    "        for chunk_tuple, chunk_info in self.common_chunks.items():\n",
    "            chunk_name = chunk_info[\"name\"]\n",
    "            self.chunk_graph.add_node(\n",
    "                chunk_name, \n",
    "                pos_type=\"chunk\", \n",
    "                elements=chunk_info[\"elements\"],\n",
    "                cohesion=chunk_info[\"cohesion\"],\n",
    "                attention=chunk_info[\"attention\"]\n",
    "            )\n",
    "        \n",
    "        # Connect chunks that can follow each other\n",
    "        for chunk1_tuple, chunk1_info in self.common_chunks.items():\n",
    "            for chunk2_tuple, chunk2_info in self.common_chunks.items():\n",
    "                # Check if chunk2 can follow chunk1 (overlap or adjacency)\n",
    "                if self._can_follow(chunk1_tuple, chunk2_tuple):\n",
    "                    # Calculate transition probability\n",
    "                    # This is simplified - would need corpus analysis for accurate probabilities\n",
    "                    transition_prob = 0.1  # Default low probability\n",
    "                    \n",
    "                    # If we have trigram data, use it to estimate transition probability\n",
    "                    if len(chunk1_tuple) >= 2 and len(chunk2_tuple) >= 1:\n",
    "                        last1, last2 = chunk1_tuple[-2], chunk1_tuple[-1]\n",
    "                        first = chunk2_tuple[0]\n",
    "                        \n",
    "                        if last2 in self.trigram_counts.get(last1, {}):\n",
    "                            total = sum(self.trigram_counts[last1][last2].values())\n",
    "                            if total > 0:\n",
    "                                count = self.trigram_counts[last1][last2].get(first, 0)\n",
    "                                transition_prob = count / total\n",
    "                    \n",
    "                    # Apply attention weighting to transition\n",
    "                    chunk1_attention = self.chunk_attention_weights.get(chunk1_tuple, 1.0)\n",
    "                    chunk2_attention = self.chunk_attention_weights.get(chunk2_tuple, 1.0)\n",
    "                    \n",
    "                    # Higher attention on both chunks strengthens their connection\n",
    "                    attention_factor = (chunk1_attention + chunk2_attention) / 2\n",
    "                    weighted_prob = transition_prob * attention_factor\n",
    "                    \n",
    "                    # Add edge with weight\n",
    "                    chunk1_name = chunk1_info[\"name\"]\n",
    "                    chunk2_name = chunk2_info[\"name\"]\n",
    "                    self.chunk_graph.add_edge(\n",
    "                        chunk1_name, \n",
    "                        chunk2_name, \n",
    "                        weight=weighted_prob,\n",
    "                        attention=attention_factor\n",
    "                    )\n",
    "\n",
    "    def _can_follow(self, chunk1: Tuple[str, ...], chunk2: Tuple[str, ...]) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if chunk2 can follow chunk1 in a sequence.\n",
    "        Either through overlap or adjacency.\n",
    "        \"\"\"\n",
    "        # Check if there's an overlap\n",
    "        for overlap_size in range(1, min(len(chunk1), len(chunk2))):\n",
    "            if chunk1[-overlap_size:] == chunk2[:overlap_size]:\n",
    "                return True\n",
    "        \n",
    "        # Check if there's an edge from the last element of chunk1 to the first of chunk2\n",
    "        last_of_chunk1 = chunk1[-1]\n",
    "        first_of_chunk2 = chunk2[0]\n",
    "        \n",
    "        return self.graph.has_edge(last_of_chunk1, first_of_chunk2)\n",
    "\n",
    "    def _report_attention_weights(self):\n",
    "        \"\"\"Report the top and bottom attention weights.\"\"\"\n",
    "        # Sort attention weights\n",
    "        sorted_pos = sorted(self.attention_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Report top attention weights\n",
    "        print(\"\\nTop 5 POS tags by attention:\")\n",
    "        for pos, weight in sorted_pos[:5]:\n",
    "            print(f\"  {pos}: {weight:.3f}\")\n",
    "            \n",
    "        # Report bottom attention weights if we have enough\n",
    "        if len(sorted_pos) > 5:\n",
    "            print(\"\\nBottom 5 POS tags by attention:\")\n",
    "            for pos, weight in sorted_pos[-5:]:\n",
    "                print(f\"  {pos}: {weight:.3f}\")\n",
    "                \n",
    "        # Report chunk attention if available\n",
    "        if self.chunk_attention_weights:\n",
    "            sorted_chunks = sorted(self.chunk_attention_weights.items(), \n",
    "                                   key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(\"\\nTop 5 chunks by attention:\")\n",
    "            for chunk, weight in sorted_chunks[:min(5, len(sorted_chunks))]:\n",
    "                print(f\"  {chunk}: {weight:.3f}\")\n",
    "\n",
    "    def segment(self, pos_sequence: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Segment a POS sequence into chunks based on boundary probabilities.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: List of POS tags for a sentence\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks, where each chunk is a list of POS tags\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = [pos_sequence[0]]\n",
    "        \n",
    "        for i in range(1, len(pos_sequence)):\n",
    "            pos1, pos2 = pos_sequence[i-1], pos_sequence[i]\n",
    "            \n",
    "            # Get boundary probability\n",
    "            boundary_prob = self.boundary_probs.get((pos1, pos2), 0.2)  # Default if unseen\n",
    "            \n",
    "            # Apply attention weighting\n",
    "            pos1_attention = self.attention_weights.get(pos1, 1.0)\n",
    "            pos2_attention = self.attention_weights.get(pos2, 1.0)\n",
    "            attention_factor = (pos1_attention + pos2_attention) / 2\n",
    "            \n",
    "            # Attention-modulated boundary decision\n",
    "            effective_boundary = boundary_prob * attention_factor\n",
    "            \n",
    "            if effective_boundary > self.hard_boundary_threshold:\n",
    "                # Hard boundary - create a new chunk\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = [pos2]\n",
    "            else:\n",
    "                # Continue current chunk\n",
    "                current_chunk.append(pos2)\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def predict_next_pos(self, context: List[str], top_n: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Predict the next POS tag with attention-modulated probabilities.\n",
    "        \n",
    "        Args:\n",
    "            context: List of preceding POS tags\n",
    "            top_n: Number of top predictions to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (pos_tag, probability) pairs, sorted by probability\n",
    "        \"\"\"\n",
    "        # Update context history\n",
    "        self.context_history.append(context)\n",
    "        if len(self.context_history) > 5:  # Keep only recent history\n",
    "            self.context_history = self.context_history[-5:]\n",
    "        \n",
    "        if not context:\n",
    "            # No context, use connections from start node\n",
    "            predictions = []\n",
    "            for target, data in self.graph.out_edges(\"<START>\", data=True):\n",
    "                if target != \"<END>\":\n",
    "                    # Apply attention weighting\n",
    "                    base_prob = data.get(\"weight\", 0.0)\n",
    "                    target_attention = self.attention_weights.get(target, 1.0)\n",
    "                    adjusted_prob = base_prob * target_attention\n",
    "                    predictions.append((target, adjusted_prob))\n",
    "            \n",
    "            # Normalize probabilities\n",
    "            total_prob = sum(prob for _, prob in predictions)\n",
    "            if total_prob > 0:\n",
    "                predictions = [(tag, prob/total_prob) for tag, prob in predictions]\n",
    "                \n",
    "            return sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        # Use the last tag for prediction\n",
    "        last_pos = context[-1]\n",
    "        \n",
    "        if self.graph.has_node(last_pos):\n",
    "            # Apply attention to outgoing predictions\n",
    "            predictions = []\n",
    "            \n",
    "            # Get attention for the source position\n",
    "            source_attention = self.attention_weights.get(last_pos, 1.0)\n",
    "            \n",
    "            # First try to use chunk-based prediction if we have matching chunks\n",
    "            chunk_predictions = self._predict_from_chunks(context)\n",
    "            \n",
    "            if chunk_predictions:\n",
    "                # If we have chunk-based predictions, give them more weight\n",
    "                # but also include some direct edge predictions\n",
    "                direct_predictions = []\n",
    "                for _, target, data in self.graph.out_edges(last_pos, data=True):\n",
    "                    if target != \"<END>\":\n",
    "                        base_prob = data.get(\"weight\", 0.0)\n",
    "                        target_attention = self.attention_weights.get(target, 1.0)\n",
    "                        # Combined attention effect (geometric mean)\n",
    "                        combined_attention = math.sqrt(source_attention * target_attention)\n",
    "                        adjusted_prob = base_prob * combined_attention\n",
    "                        direct_predictions.append((target, adjusted_prob))\n",
    "                \n",
    "                # Normalize direct predictions\n",
    "                total_direct = sum(prob for _, prob in direct_predictions)\n",
    "                if total_direct > 0:\n",
    "                    direct_predictions = [(tag, prob/total_direct) for tag, prob in direct_predictions]\n",
    "                \n",
    "                # Combine chunk-based and direct predictions with 3:1 weighting\n",
    "                combined = {}\n",
    "                for tag, prob in chunk_predictions:\n",
    "                    combined[tag] = prob * 0.75\n",
    "                \n",
    "                for tag, prob in direct_predictions:\n",
    "                    if tag in combined:\n",
    "                        combined[tag] += prob * 0.25\n",
    "                    else:\n",
    "                        combined[tag] = prob * 0.25\n",
    "                \n",
    "                predictions = [(tag, prob) for tag, prob in combined.items()]\n",
    "            else:\n",
    "                # No chunk predictions, use direct edge predictions\n",
    "                for _, target, data in self.graph.out_edges(last_pos, data=True):\n",
    "                    if target != \"<END>\":\n",
    "                        base_prob = data.get(\"weight\", 0.0)\n",
    "                        target_attention = self.attention_weights.get(target, 1.0)\n",
    "                        # Combined attention effect\n",
    "                        combined_attention = math.sqrt(source_attention * target_attention)\n",
    "                        adjusted_prob = base_prob * combined_attention\n",
    "                        predictions.append((target, adjusted_prob))\n",
    "            \n",
    "            # Normalize probabilities\n",
    "            total_prob = sum(prob for _, prob in predictions)\n",
    "            if total_prob > 0:\n",
    "                predictions = [(tag, prob/total_prob) for tag, prob in predictions]\n",
    "                \n",
    "            return sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        else:\n",
    "            # Unseen POS tag\n",
    "            return [(\"<UNK>\", 1.0)]  # Return unknown with full probability\n",
    "\n",
    "    def _predict_from_chunks(self, context: List[str]) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Generate predictions based on chunk matching.\n",
    "        \n",
    "        Args:\n",
    "            context: The context sequence\n",
    "            \n",
    "        Returns:\n",
    "            List of (pos_tag, probability) tuples\n",
    "        \"\"\"\n",
    "        if len(context) < 1:\n",
    "            return []\n",
    "            \n",
    "        # Try to match the end of the context with the beginning of chunks\n",
    "        matches = []\n",
    "        max_match_length = 0\n",
    "        \n",
    "        for chunk_tuple, chunk_info in self.common_chunks.items():\n",
    "            for match_length in range(min(len(context), len(chunk_tuple)), 0, -1):\n",
    "                if context[-match_length:] == chunk_tuple[:match_length]:\n",
    "                    if match_length > max_match_length:\n",
    "                        max_match_length = match_length\n",
    "                        matches = [(chunk_tuple, chunk_info, match_length)]\n",
    "                    elif match_length == max_match_length:\n",
    "                        matches.append((chunk_tuple, chunk_info, match_length))\n",
    "        \n",
    "        if not matches:\n",
    "            return []\n",
    "            \n",
    "        # Generate predictions based on matched chunks\n",
    "        predictions = Counter()\n",
    "        total_weight = 0\n",
    "        \n",
    "        for chunk_tuple, chunk_info, match_length in matches:\n",
    "            # If match is complete, this chunk can't help with prediction\n",
    "            if match_length >= len(chunk_tuple):\n",
    "                continue\n",
    "                \n",
    "            # The next element in the chunk is the prediction\n",
    "            next_pos = chunk_tuple[match_length]\n",
    "            \n",
    "            # Weight by chunk cohesion and attention\n",
    "            weight = chunk_info[\"cohesion\"] * chunk_info.get(\"attention\", 1.0)\n",
    "            predictions[next_pos] += weight\n",
    "            total_weight += weight\n",
    "        \n",
    "        # Normalize predictions\n",
    "        if total_weight > 0:\n",
    "            return [(pos, weight/total_weight) for pos, weight in predictions.items()]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def predictive_processing(self, pos_sequence: List[str]) -> Tuple[List[Dict[str, Any]], List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Process a sequence using predictive coding principles with attention modulation.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: List of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (recognized chunks, segmented sequence)\n",
    "        \"\"\"\n",
    "        # First pass: recognize chunks\n",
    "        recognized_chunks = self.recognize_chunks(pos_sequence)\n",
    "        \n",
    "        # Second pass: resolve overlaps with attention-weighted resolution\n",
    "        non_overlapping = self._resolve_chunk_overlaps(recognized_chunks, len(pos_sequence))\n",
    "        \n",
    "        # Third pass: final segmentation based on chunks and boundaries\n",
    "        segmentation = self._create_final_segmentation(pos_sequence, non_overlapping)\n",
    "        \n",
    "        return non_overlapping, segmentation\n",
    "    \n",
    "    def recognize_chunks(self, pos_sequence: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Recognize known chunks in a POS sequence with attention modulation.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: List of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            List of recognized chunks with their properties\n",
    "        \"\"\"\n",
    "        recognized = []\n",
    "        \n",
    "        # Try to match chunks of different sizes\n",
    "        for i in range(len(pos_sequence)):\n",
    "            for size in range(4, 1, -1):  # Try larger chunks first (4, 3, 2)\n",
    "                if i + size <= len(pos_sequence):\n",
    "                    chunk_tuple = tuple(pos_sequence[i:i+size])\n",
    "                    if chunk_tuple in self.common_chunks:\n",
    "                        # Calculate activation based on cohesion and attention\n",
    "                        chunk_info = self.common_chunks[chunk_tuple]\n",
    "                        base_activation = chunk_info[\"cohesion\"]\n",
    "                        chunk_attention = self.chunk_attention_weights.get(chunk_tuple, 1.0)\n",
    "                        \n",
    "                        # Attention-modulated activation\n",
    "                        activation = base_activation * chunk_attention\n",
    "                        \n",
    "                        recognized.append({\n",
    "                            \"chunk\": chunk_info,\n",
    "                            \"start\": i,\n",
    "                            \"end\": i + size,\n",
    "                            \"activation\": activation\n",
    "                        })\n",
    "        \n",
    "        # Sort by start position\n",
    "        recognized.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        return recognized\n",
    "        \n",
    "    def _resolve_chunk_overlaps(self, chunks: List[Dict[str, Any]], seq_length: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resolve overlapping chunks by selecting the most activated ones with attention influence.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of recognized chunks\n",
    "            seq_length: Length of the original sequence\n",
    "            \n",
    "        Returns:\n",
    "            List of non-overlapping chunks\n",
    "        \"\"\"\n",
    "        # If no chunks, return empty list\n",
    "        if not chunks:\n",
    "            return []\n",
    "            \n",
    "        # Sort by activation (influenced by attention) to prioritize strongest chunks\n",
    "        sorted_chunks = sorted(chunks, key=lambda x: x[\"activation\"], reverse=True)\n",
    "        \n",
    "        # Track which positions are covered\n",
    "        covered = [False] * seq_length\n",
    "        \n",
    "        # Select non-overlapping chunks\n",
    "        selected = []\n",
    "        \n",
    "        for chunk in sorted_chunks:\n",
    "            start, end = chunk[\"start\"], chunk[\"end\"]\n",
    "            \n",
    "            # Check if this chunk overlaps with already selected ones\n",
    "            overlap = False\n",
    "            for i in range(start, end):\n",
    "                if covered[i]:\n",
    "                    overlap = True\n",
    "                    break\n",
    "            \n",
    "            if not overlap:\n",
    "                # Add chunk and mark positions as covered\n",
    "                selected.append(chunk)\n",
    "                for i in range(start, end):\n",
    "                    covered[i] = True\n",
    "        \n",
    "        # Sort by start position\n",
    "        selected.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        return selected\n",
    "\n",
    "    def visualize_pos_graph(self, filename: str = \"pos_graph_attention.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the POS transition graph with attention weighting.\n",
    "        \n",
    "        Args:\n",
    "            filename: Output file name\n",
    "        \"\"\"\n",
    "        # Check if graph is empty\n",
    "        if len(self.graph) <= 2:  # Only START and END nodes\n",
    "            print(\"POS graph is empty or contains only special nodes - no visualization created\")\n",
    "            return\n",
    "            \n",
    "        # Create a copy without special nodes for cleaner visualization\n",
    "        g = self.graph.copy()\n",
    "        \n",
    "        # Only remove special nodes if they exist\n",
    "        if \"<START>\" in g:\n",
    "            g.remove_node(\"<START>\")\n",
    "        if \"<END>\" in g:\n",
    "            g.remove_node(\"<END>\")\n",
    "            \n",
    "        if len(g.edges()) == 0:\n",
    "            print(\"POS graph has no edges - no visualization created\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Define node positions using spring layout\n",
    "        pos = nx.spring_layout(g, seed=42)\n",
    "        \n",
    "        # Node sizes based on attention weights\n",
    "        node_sizes = []\n",
    "        node_colors = []\n",
    "        for node in g.nodes():\n",
    "            attention = self.attention_weights.get(node, 1.0)\n",
    "            node_sizes.append(400 + 300 * attention)  # Base size + attention effect\n",
    "            \n",
    "            # Color gradient from cool to warm based on attention\n",
    "            # Low attention: blue (0.0), High attention: red (1.0)\n",
    "            node_colors.append((min(1.0, attention/2), 0.2, max(0.0, 1.0-attention/2)))\n",
    "        \n",
    "        # Draw nodes with size and color based on attention\n",
    "        nx.draw_networkx_nodes(g, pos, node_size=node_sizes, node_color=node_colors)\n",
    "        \n",
    "        # Prepare edge attributes\n",
    "        edge_width = []\n",
    "        edge_color = []\n",
    "        \n",
    "        for source, target, data in g.edges(data=True):\n",
    "            # Default to 0.1 if weight is missing or zero\n",
    "            weight = data.get(\"weight\", 0.1)\n",
    "            if weight == 0:\n",
    "                weight = 0.1\n",
    "                \n",
    "            # Adjust width by precision weighting\n",
    "            precision = data.get(\"precision\", self.base_precision)\n",
    "            precision_factor = precision / self.base_precision\n",
    "            \n",
    "            edge_width.append(weight * 5 * precision_factor)\n",
    "            \n",
    "            # Edge color based on boundary probability\n",
    "            edge_color.append(data.get(\"boundary_prob\", 0.5))\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(\n",
    "            g, pos, width=edge_width, \n",
    "            edge_color=edge_color, edge_cmap=plt.cm.Reds,\n",
    "            connectionstyle=\"arc3,rad=0.1\"\n",
    "        )\n",
    "        \n",
    "        # Add labels\n",
    "        nx.draw_networkx_labels(g, pos, font_size=10)\n",
    "        \n",
    "        # Edge labels (probability + precision)\n",
    "        edge_labels = {}\n",
    "        for u, v, d in g.edges(data=True):\n",
    "            weight = d.get(\"weight\", 0.0)\n",
    "            precision = d.get(\"precision\", self.base_precision)\n",
    "            edge_labels[(u, v)] = f\"{weight:.2f}\\n(p:{precision:.1f})\"\n",
    "                \n",
    "        nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels, font_size=8)\n",
    "        \n",
    "        # Add a color bar for boundary probabilities\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds)\n",
    "        sm.set_array([])\n",
    "        fig.colorbar(sm, ax=ax, label=\"Boundary Probability\")\n",
    "        \n",
    "        plt.title(\"POS Transition Graph with Attention\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        print(f\"POS graph visualization saved to {filename}\")\n",
    "        plt.close()\n",
    "        \n",
    "    def visualize_chunk_graph(self, filename: str = \"chunk_graph_attention.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the chunk transition graph with attention weighting.\n",
    "        \n",
    "        Args:\n",
    "            filename: Output file name\n",
    "        \"\"\"\n",
    "        if len(self.chunk_graph) == 0:\n",
    "            print(\"Chunk graph is empty - no visualization created\")\n",
    "            return\n",
    "            \n",
    "        if len(self.chunk_graph.edges()) == 0:\n",
    "            print(\"Chunk graph has no edges - adding artificial edges for visualization\")\n",
    "            # Create some artificial edges just for visualization\n",
    "            nodes = list(self.chunk_graph.nodes())\n",
    "            if len(nodes) > 1:\n",
    "                for i in range(len(nodes)-1):\n",
    "                    self.chunk_graph.add_edge(nodes[i], nodes[i+1], weight=0.1, attention=1.0)\n",
    "            \n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Define node positions using spring layout\n",
    "        pos = nx.spring_layout(self.chunk_graph, seed=42)\n",
    "        \n",
    "        # Draw nodes with size and color based on attention and cohesion\n",
    "        node_sizes = []\n",
    "        node_colors = []\n",
    "        for node in self.chunk_graph.nodes():\n",
    "            cohesion = self.chunk_graph.nodes[node].get(\"cohesion\", 0.5)\n",
    "            attention = self.chunk_graph.nodes[node].get(\"attention\", 1.0)\n",
    "            \n",
    "            if cohesion <= 0:\n",
    "                cohesion = 0.5  # Ensure minimum size\n",
    "                \n",
    "            # Size based on cohesion and attention\n",
    "            node_sizes.append(cohesion * attention * 1000)\n",
    "            \n",
    "            # Color based on attention (from cool to warm)\n",
    "            node_colors.append((min(1.0, attention/2), 0.4, max(0.0, 1.0-attention/2)))\n",
    "        \n",
    "        nx.draw_networkx_nodes(\n",
    "            self.chunk_graph, pos, \n",
    "            node_size=node_sizes, \n",
    "            node_color=node_colors\n",
    "        )\n",
    "        \n",
    "        # Draw edges with width and color based on weight and attention\n",
    "        if len(self.chunk_graph.edges()) > 0:\n",
    "            edge_width = []\n",
    "            edge_color = []\n",
    "            \n",
    "            for _, _, data in self.chunk_graph.edges(data=True):\n",
    "                weight = data.get(\"weight\", 0.1)\n",
    "                attention = data.get(\"attention\", 1.0)\n",
    "                \n",
    "                if weight <= 0:\n",
    "                    weight = 0.1  # Ensure minimum width\n",
    "                    \n",
    "                # Width affected by both weight and attention\n",
    "                edge_width.append(weight * attention * 10)\n",
    "                \n",
    "                # Edge color based on attention\n",
    "                edge_color.append(attention)\n",
    "            \n",
    "            nx.draw_networkx_edges(\n",
    "                self.chunk_graph, pos, width=edge_width, \n",
    "                edge_color=edge_color, edge_cmap=plt.cm.YlOrRd, alpha=0.7,\n",
    "                connectionstyle=\"arc3,rad=0.1\"\n",
    "            )\n",
    "        \n",
    "        # Add labels\n",
    "        nx.draw_networkx_labels(self.chunk_graph, pos, font_size=9)\n",
    "        \n",
    "        # Add a color bar for edge attention\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.YlOrRd)\n",
    "        sm.set_array([])\n",
    "        fig.colorbar(sm, ax=ax, label=\"Attention Weight\")\n",
    "        \n",
    "        plt.title(\"Chunk Transition Graph with Attention\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Chunk graph visualization saved to {filename}\")\n",
    "        plt.close()\n",
    "        \n",
    "    def _create_final_segmentation(self, pos_sequence: List[str], chunks: List[Dict[str, Any]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Create final segmentation based on recognized chunks and boundary probabilities\n",
    "        with attention modulation.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: Original POS sequence\n",
    "            chunks: Non-overlapping chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of segments (chunks)\n",
    "        \"\"\"\n",
    "        # If no chunks recognized, fall back to boundary-based segmentation\n",
    "        if not chunks:\n",
    "            return self.segment(pos_sequence)\n",
    "        \n",
    "        # Create segmentation based on recognized chunks and boundaries\n",
    "        segmentation = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            start, end = chunk[\"start\"], chunk[\"end\"]\n",
    "            \n",
    "            # If there's a gap before this chunk, segment it using boundaries\n",
    "            if start > current_pos:\n",
    "                gap_sequence = pos_sequence[current_pos:start]\n",
    "                gap_segments = self.segment(gap_sequence)\n",
    "                \n",
    "                # Adjust segment positions\n",
    "                adjusted_segments = []\n",
    "                for segment in gap_segments:\n",
    "                    adjusted_segments.append(segment)\n",
    "                \n",
    "                segmentation.extend(adjusted_segments)\n",
    "            \n",
    "            # Add the recognized chunk\n",
    "            segmentation.append(pos_sequence[start:end])\n",
    "            current_pos = end\n",
    "        \n",
    "        # Handle any remaining sequence after the last chunk\n",
    "        if current_pos < len(pos_sequence):\n",
    "            remaining = pos_sequence[current_pos:]\n",
    "            remaining_segments = self.segment(remaining)\n",
    "            segmentation.extend(remaining_segments)\n",
    "        \n",
    "        return segmentation\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample POS sequences for training - Create more examples with repeating patterns\n",
    "    # to increase likelihood of chunk detection\n",
    "    training_data = [\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"],\n",
    "        [\"PRON\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],\n",
    "        [\"DET\", \"NOUN\", \"VERB\", \"ADV\", \"ADJ\"],\n",
    "        [\"DET\", \"ADJ\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],\n",
    "        [\"PRON\", \"VERB\", \"DET\", \"NOUN\", \"CONJ\", \"VERB\", \"ADV\"],\n",
    "        # More examples with repeating patterns to help chunk detection\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],  # Repeat\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"],          # Repeat\n",
    "        [\"DET\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"ADJ\", \"NOUN\"],\n",
    "        [\"PRON\", \"VERB\", \"ADV\", \"CONJ\", \"VERB\", \"DET\", \"NOUN\"],\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"ADV\", \"PREP\", \"PRON\"],\n",
    "        [\"NOUN\", \"VERB\", \"DET\", \"ADJ\", \"NOUN\", \"PREP\", \"DET\", \"NOUN\"],\n",
    "        [\"DET\", \"NOUN\", \"VERB\", \"ADJ\", \"CONJ\", \"ADV\"],\n",
    "        # Even more repetition of common patterns\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"],          # Repeat\n",
    "        [\"PRON\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],                # Repeat\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],  # Repeat\n",
    "    ]\n",
    "    \n",
    "    print(f\"Training on {len(training_data)} sentences\")\n",
    "    \n",
    "    # Initialize and train the graph with attention mechanisms\n",
    "    pos_graph = POSGraphWithAttention()\n",
    "    pos_graph.train(training_data, epochs=3)  # Multiple epochs to allow attention to develop\n",
    "    \n",
    "    # Test on a new sentence\n",
    "    test_sentence = [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"]\n",
    "    \n",
    "    print(\"\\nTest sentence:\", test_sentence)\n",
    "    \n",
    "    # Recognition and segmentation\n",
    "    chunks, segments = pos_graph.predictive_processing(test_sentence)\n",
    "    \n",
    "    print(\"\\nRecognized chunks:\")\n",
    "    if chunks:\n",
    "        for chunk in chunks:\n",
    "            print(f\"  {chunk['chunk']['elements']} (Position {chunk['start']}-{chunk['end']}, \" +\n",
    "                  f\"Activation: {chunk['activation']:.3f})\")\n",
    "    else:\n",
    "        print(\"  No chunks recognized\")\n",
    "    \n",
    "    print(\"\\nFinal segmentation:\", segments)\n",
    "    \n",
    "    # Predictions\n",
    "    context = [\"DET\", \"ADJ\"]\n",
    "    predictions = pos_graph.predict_next_pos(context)\n",
    "    print(f\"\\nTop predictions after {context}:\")\n",
    "    for pos, prob in predictions:\n",
    "        print(f\"  {pos}: {prob:.2f} (Attention: {pos_graph.attention_weights.get(pos, 1.0):.2f})\")\n",
    "    \n",
    "    # Visualize graphs with attention\n",
    "    pos_graph.visualize_pos_graph()\n",
    "    pos_graph.visualize_chunk_graph()\n",
    "    \n",
    "    # Plot surprisal history to show learning progress\n",
    "    if pos_graph.surprisal_history:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(pos_graph.surprisal_history, 'b-o')\n",
    "        plt.title('Learning Progress - Average Surprisal per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Surprisal')\n",
    "        plt.grid(True)\n",
    "        plt.savefig('learning_progress.png')\n",
    "        print(\"Learning progress plot saved to learning_progress.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537afbe-676f-40e3-9743-de61b1500781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.Pytorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
