{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1fd612-a848-4b6d-bdff-29a73f57c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graphs(self, prefix=\"bidirectional_\"):\n",
    "        \"\"\"Visualize all graphs (forward, backward, and chunk).\"\"\"\n",
    "        self.visualize_pos_graph(self.forward_graph, f\"{prefix}forward_graph.png\", \"Forward\")\n",
    "        self.visualize_pos_graph(self.backward_graph, f\"{prefix}backward_graph.png\", \"Backward\")\n",
    "        self.visualize_chunk_graph(f\"{prefix}chunk_graph.png\")\n",
    "        \n",
    "        # Plot learning progress\n",
    "        if self.surprisal_history:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.surprisal_history, 'b-o')\n",
    "            plt.title('Bidirectional Learning Progress - Average Surprisal per Epoch')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Average Surprisal')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f'{prefix}learning_progress.png')\n",
    "            print(f\"Learning progress plot saved to {prefix}learning_progress.png\")\n",
    "            plt.close()\n",
    "\n",
    "    def visualize_pos_graph(self, graph, filename: str = \"pos_graph_attention.png\", direction: str = \"Forward\"):\n",
    "        \"\"\"\n",
    "        Visualize a POS transition graph with attention weighting.\n",
    "        \n",
    "        Args:\n",
    "            graph: The graph to visualize\n",
    "            filename: Output file name\n",
    "            direction: Label for the graph direction\n",
    "        \"\"\"\n",
    "        # Check if graph is empty\n",
    "        if len(graph) <= 2:  # Only START and END nodes\n",
    "            print(f\"{direction} graph is empty or contains only special nodes - no visualization created\")\n",
    "            return\n",
    "            \n",
    "        # Create a copy without special nodes for cleaner visualization\n",
    "        g = graph.copy()\n",
    "        \n",
    "        # Only remove special nodes if they exist\n",
    "        if \"<START>\" in g:\n",
    "            g.remove_node(\"<START>\")\n",
    "        if \"<END>\" in g:\n",
    "            g.remove_node(\"<END>\")\n",
    "            \n",
    "        if len(g.edges()) == 0:\n",
    "            print(f\"{direction} graph has no edges - no visualization created\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Define node positions using spring layout\n",
    "        pos = nx.spring_layout(g, seed=42)\n",
    "        \n",
    "        # Node sizes based on attention weights\n",
    "        node_sizes = []\n",
    "        node_colors = []\n",
    "        for node in g.nodes():\n",
    "            attention = self.attention_weights.get(node, 1.0)\n",
    "            node_sizes.append(400 + 300 * attention)  # Base size + attention effect\n",
    "            \n",
    "            # Color gradient from cool to warm based on attention\n",
    "            # Low attention: blue (0.0), High attention: red (1.0)\n",
    "            node_colors.append((min(1.0, attention/2), 0.2, max(0.0, 1.0-attention/2)))\n",
    "        \n",
    "        # Draw nodes with size and color based on attention\n",
    "        nx.draw_networkx_nodes(g, pos, node_size=node_sizes, node_color=node_colors)\n",
    "        \n",
    "        # Prepare edge attributes\n",
    "        edge_width = []\n",
    "        edge_color = []\n",
    "        \n",
    "        for source, target, data in g.edges(data=True):\n",
    "            # Default to 0.1 if weight is missing or zero\n",
    "            weight = data.get(\"weight\", 0.1)\n",
    "            if weight == 0:\n",
    "                weight = 0.1\n",
    "                \n",
    "            # Adjust width by precision weighting\n",
    "            precision = data.get(\"precision\", self.base_precision)\n",
    "            precision_factor = precision / self.base_precision\n",
    "            \n",
    "            edge_width.append(weight * 5 * precision_factor)\n",
    "            \n",
    "            # Edge color based on boundary probability\n",
    "            edge_color.append(data.get(\"boundary_prob\", 0.5))\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(\n",
    "            g, pos, width=edge_width, \n",
    "            edge_color=edge_color, edge_cmap=plt.cm.Reds,\n",
    "            connectionstyle=\"arc3,rad=0.1\"\n",
    "        )\n",
    "        \n",
    "        # Add labels\n",
    "        nx.draw_networkx_labels(g, pos, font_size=10)\n",
    "        \n",
    "        # Edge labels (probability + precision)\n",
    "        edge_labels = {}\n",
    "        for u, v, d in g.edges(data=True):\n",
    "            weight = d.get(\"weight\", 0.0)\n",
    "            precision = d.get(\"precision\", self.base_precision)\n",
    "            edge_labels[(u, v)] = f\"{weight:.2f}\\n(p:{precision:.1f})\"\n",
    "                \n",
    "        nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels, font_size=8)\n",
    "        \n",
    "        # Add a color bar for boundary probabilities\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds)\n",
    "        sm.set_array([])\n",
    "        fig.colorbar(sm, ax=ax, label=\"Boundary Probability\")\n",
    "        \n",
    "        plt.title(f\"{direction} POS Transition Graph with Attention\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        print(f\"{direction} graph visualization saved to {filename}\")\n",
    "        plt.close()import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "class POSGraphBidirectional:\n",
    "    \"\"\"\n",
    "    Implements a graph-based structure for POS sequence processing with predictive coding,\n",
    "    attention mechanisms, and bidirectional processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predefined_boundaries: Optional[Dict[Tuple[str, str], float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the POS graph with attention mechanisms and bidirectional processing.\n",
    "        \n",
    "        Args:\n",
    "            predefined_boundaries: Optional dictionary of predefined boundary probabilities\n",
    "        \"\"\"\n",
    "        # Main transition graph (forward direction)\n",
    "        self.forward_graph = nx.DiGraph()\n",
    "        \n",
    "        # Backward transition graph (for bidirectional processing)\n",
    "        self.backward_graph = nx.DiGraph()\n",
    "        \n",
    "        # Higher-order chunk graph for learned patterns\n",
    "        self.chunk_graph = nx.DiGraph()\n",
    "        \n",
    "        # Track n-gram counts for training\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.backward_bigram_counts = defaultdict(Counter)  # For backward transitions\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "        \n",
    "        # Boundary probabilities\n",
    "        self.forward_boundary_probs = defaultdict(float)  # Forward direction\n",
    "        self.backward_boundary_probs = defaultdict(float)  # Backward direction\n",
    "        self.combined_boundary_probs = defaultdict(float)  # Combined\n",
    "        \n",
    "        # Predefined linguistic rules\n",
    "        self.predefined_boundaries = predefined_boundaries or {\n",
    "            ('NOUN', 'VERB'): 0.9,      # NP to VP transition\n",
    "            ('VERB', 'DET'): 0.8,       # VP to NP transition\n",
    "            ('PUNCT', 'DET'): 0.95,     # Punctuation followed by determiner\n",
    "            ('NOUN', 'PREP'): 0.7,      # NP to PP transition\n",
    "            ('VERB', 'PREP'): 0.6,      # VP to PP transition\n",
    "            ('ADJ', 'NOUN'): 0.2,       # Within NP (low boundary probability)\n",
    "            ('DET', 'ADJ'): 0.1,        # Within NP (very low boundary probability)\n",
    "        }\n",
    "        \n",
    "        # Thresholds\n",
    "        self.hard_boundary_threshold = 0.75\n",
    "        self.soft_boundary_threshold = 0.4\n",
    "        \n",
    "        # Discovered chunks\n",
    "        self.common_chunks = {}\n",
    "        \n",
    "        # Add special start and end nodes\n",
    "        self.forward_graph.add_node(\"<START>\", pos_type=\"special\")\n",
    "        self.forward_graph.add_node(\"<END>\", pos_type=\"special\")\n",
    "        self.backward_graph.add_node(\"<START>\", pos_type=\"special\")\n",
    "        self.backward_graph.add_node(\"<END>\", pos_type=\"special\")\n",
    "        \n",
    "        # Attention mechanisms\n",
    "        self.attention_weights = {}  # POS tag attention weights\n",
    "        self.chunk_attention_weights = {}  # Chunk attention weights\n",
    "        self.surprisal_history = []  # Track surprisal for adaptive attention\n",
    "        self.learning_rate = 0.1  # Base learning rate\n",
    "        self.attention_learning_rate = 0.05  # For updating attention weights\n",
    "        \n",
    "        # Precision weighting parameters\n",
    "        self.base_precision = 1.0\n",
    "        self.max_precision = 5.0\n",
    "        self.min_precision = 0.2\n",
    "        \n",
    "        # Context tracking\n",
    "        self.context_history = []  # Track recent contexts for attention modulation\n",
    "        \n",
    "        # Bidirectional weights\n",
    "        self.forward_weight = 0.6  # Weight for forward processing (typically higher)\n",
    "        self.backward_weight = 0.4  # Weight for backward processing\n",
    "\n",
    "    def train(self, pos_sequences: List[List[str]], epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Train the POS graph on a corpus of POS tag sequences with attention\n",
    "        and bidirectional processing.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequences: List of POS tag sequences, each representing a sentence\n",
    "            epochs: Number of training epochs\n",
    "        \"\"\"\n",
    "        print(f\"Training on {len(pos_sequences)} sequences for {epochs} epochs\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. Initialize attention weights uniformly\n",
    "        self._initialize_attention_weights(pos_sequences)\n",
    "        \n",
    "        # 2. Build initial graphs (forward and backward) and collect statistics\n",
    "        self._build_initial_graphs(pos_sequences)\n",
    "        \n",
    "        # 3. Iterative training with attention modulation and bidirectional processing\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Shuffle sequences for stochastic training\n",
    "            random.shuffle(pos_sequences)\n",
    "            \n",
    "            epoch_surprisal = 0.0\n",
    "            \n",
    "            # Process each sequence with attention\n",
    "            for sequence in pos_sequences:\n",
    "                # Forward pass - calculate predictions and errors\n",
    "                sequence_surprisal, prediction_errors = self._forward_pass(sequence)\n",
    "                epoch_surprisal += sequence_surprisal\n",
    "                \n",
    "                # Backward pass - process the sequence in reverse\n",
    "                reversed_sequence = list(reversed(sequence))\n",
    "                backward_surprisal, backward_errors = self._backward_pass(reversed_sequence)\n",
    "                epoch_surprisal += backward_surprisal\n",
    "                \n",
    "                # Update attention weights based on both forward and backward errors\n",
    "                self._update_attention_weights(sequence, prediction_errors, reversed_sequence, backward_errors)\n",
    "                \n",
    "                # Update graph weights with attention-modulated learning\n",
    "                self._update_graph_weights(sequence, prediction_errors, reversed_sequence, backward_errors)\n",
    "            \n",
    "            avg_surprisal = epoch_surprisal / (len(pos_sequences) * 2)  # Both directions\n",
    "            self.surprisal_history.append(avg_surprisal)\n",
    "            print(f\"  Average surprisal: {avg_surprisal:.4f}\")\n",
    "            \n",
    "            # Recalculate edge weights based on updated counts\n",
    "            self._calculate_edge_weights()\n",
    "            \n",
    "            # After each epoch, update boundary probabilities (both directions)\n",
    "            self._calculate_boundary_probabilities()\n",
    "            \n",
    "            # Combine boundary probabilities from both directions\n",
    "            self._combine_boundary_probabilities()\n",
    "        \n",
    "        # 4. Identify common chunks with attention influence\n",
    "        self._identify_common_chunks(pos_sequences)\n",
    "        \n",
    "        # 5. Build chunk graph with attention-weighted connections\n",
    "        self._build_chunk_graph()\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training complete in {training_time:.2f} seconds.\")\n",
    "        print(f\"Forward graph has {len(self.forward_graph.nodes)} nodes and {len(self.forward_graph.edges)} edges\")\n",
    "        print(f\"Backward graph has {len(self.backward_graph.nodes)} nodes and {len(self.backward_graph.edges)} edges\")\n",
    "        print(f\"Chunk graph has {len(self.chunk_graph.nodes)} nodes and {len(self.chunk_graph.edges)} edges\")\n",
    "        \n",
    "        # Report top attention weights\n",
    "        self._report_attention_weights()\n",
    "\n",
    "    def _initialize_attention_weights(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Initialize attention weights for all POS tags.\"\"\"\n",
    "        # Extract all unique POS tags from sequences\n",
    "        unique_pos = set()\n",
    "        for sequence in pos_sequences:\n",
    "            unique_pos.update(sequence)\n",
    "        \n",
    "        # Initialize with uniform weights\n",
    "        for pos in unique_pos:\n",
    "            self.attention_weights[pos] = 1.0\n",
    "            \n",
    "        # Special start and end nodes\n",
    "        self.attention_weights[\"<START>\"] = 1.0\n",
    "        self.attention_weights[\"<END>\"] = 1.0\n",
    "\n",
    "    def _build_initial_graphs(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Build the initial forward and backward graphs and collect statistics.\"\"\"\n",
    "        # Forward graph\n",
    "        for sequence in pos_sequences:\n",
    "            # Add nodes for each unique POS tag\n",
    "            for pos in sequence:\n",
    "                if not self.forward_graph.has_node(pos):\n",
    "                    self.forward_graph.add_node(pos, pos_type=\"basic\", precision=self.base_precision)\n",
    "                self.unigram_counts[pos] += 1\n",
    "            \n",
    "            # Count bigrams and add edges (forward)\n",
    "            for i in range(len(sequence) - 1):\n",
    "                pos1, pos2 = sequence[i], sequence[i+1]\n",
    "                self.bigram_counts[pos1][pos2] += 1\n",
    "                \n",
    "                # Ensure edge exists (weight will be calculated later)\n",
    "                if not self.forward_graph.has_edge(pos1, pos2):\n",
    "                    self.forward_graph.add_edge(pos1, pos2, weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                \n",
    "                # Increment edge count\n",
    "                self.forward_graph[pos1][pos2][\"count\"] += 1\n",
    "            \n",
    "            # Add connections from start and to end (forward)\n",
    "            if sequence:\n",
    "                if not self.forward_graph.has_edge(\"<START>\", sequence[0]):\n",
    "                    self.forward_graph.add_edge(\"<START>\", sequence[0], weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                self.forward_graph[\"<START>\"][sequence[0]][\"count\"] += 1\n",
    "                \n",
    "                if not self.forward_graph.has_edge(sequence[-1], \"<END>\"):\n",
    "                    self.forward_graph.add_edge(sequence[-1], \"<END>\", weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                self.forward_graph[sequence[-1]][\"<END>\"][\"count\"] += 1\n",
    "            \n",
    "            # Count trigrams\n",
    "            for i in range(len(sequence) - 2):\n",
    "                pos1, pos2, pos3 = sequence[i], sequence[i+1], sequence[i+2]\n",
    "                self.trigram_counts[pos1][pos2][pos3] += 1\n",
    "        \n",
    "        # Backward graph (reversed sequences)\n",
    "        for sequence in pos_sequences:\n",
    "            reversed_seq = list(reversed(sequence))\n",
    "            \n",
    "            # Add nodes for each unique POS tag in backward graph\n",
    "            for pos in reversed_seq:\n",
    "                if not self.backward_graph.has_node(pos):\n",
    "                    self.backward_graph.add_node(pos, pos_type=\"basic\", precision=self.base_precision)\n",
    "            \n",
    "            # Count bigrams and add edges (backward)\n",
    "            for i in range(len(reversed_seq) - 1):\n",
    "                pos1, pos2 = reversed_seq[i], reversed_seq[i+1]\n",
    "                self.backward_bigram_counts[pos1][pos2] += 1\n",
    "                \n",
    "                # Ensure edge exists in backward graph\n",
    "                if not self.backward_graph.has_edge(pos1, pos2):\n",
    "                    self.backward_graph.add_edge(pos1, pos2, weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                \n",
    "                # Increment edge count\n",
    "                self.backward_graph[pos1][pos2][\"count\"] += 1\n",
    "            \n",
    "            # Add connections from start and to end (backward)\n",
    "            if reversed_seq:\n",
    "                if not self.backward_graph.has_edge(\"<START>\", reversed_seq[0]):\n",
    "                    self.backward_graph.add_edge(\"<START>\", reversed_seq[0], weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                self.backward_graph[\"<START>\"][reversed_seq[0]][\"count\"] += 1\n",
    "                \n",
    "                if not self.backward_graph.has_edge(reversed_seq[-1], \"<END>\"):\n",
    "                    self.backward_graph.add_edge(reversed_seq[-1], \"<END>\", weight=0, count=0, boundary_prob=0, precision=self.base_precision)\n",
    "                self.backward_graph[reversed_seq[-1]][\"<END>\"][\"count\"] += 1\n",
    "\n",
    "    def _forward_pass(self, sequence: List[str]) -> Tuple[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Process a sequence in forward direction and calculate prediction errors.\n",
    "        \n",
    "        Args:\n",
    "            sequence: A sequence of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (total_surprisal, list_of_prediction_errors)\n",
    "        \"\"\"\n",
    "        total_surprisal = 0.0\n",
    "        prediction_errors = []\n",
    "        \n",
    "        # Start with START node\n",
    "        current_pos = \"<START>\"\n",
    "        \n",
    "        # Process each position in the sequence\n",
    "        for pos in sequence:\n",
    "            # Calculate prediction probability for this position\n",
    "            prediction_prob = 0.0\n",
    "            if self.forward_graph.has_edge(current_pos, pos):\n",
    "                prediction_prob = self.forward_graph[current_pos][pos].get(\"weight\", 0.0)\n",
    "            \n",
    "            # Calculate surprisal (-log probability)\n",
    "            if prediction_prob > 0:\n",
    "                surprisal = -math.log2(prediction_prob)\n",
    "            else:\n",
    "                surprisal = 10.0  # High surprisal for unseen transitions\n",
    "                \n",
    "            # Get current precision for this transition\n",
    "            precision = self.base_precision\n",
    "            if self.forward_graph.has_edge(current_pos, pos):\n",
    "                precision = self.forward_graph[current_pos][pos].get(\"precision\", self.base_precision)\n",
    "            \n",
    "            # Calculate precision-weighted prediction error\n",
    "            prediction_error = surprisal * precision\n",
    "            \n",
    "            prediction_errors.append(prediction_error)\n",
    "            total_surprisal += surprisal\n",
    "            \n",
    "            # Update current position\n",
    "            current_pos = pos\n",
    "        \n",
    "        return total_surprisal, prediction_errors\n",
    "\n",
    "    def _backward_pass(self, reversed_sequence: List[str]) -> Tuple[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Process a sequence in backward direction and calculate prediction errors.\n",
    "        \n",
    "        Args:\n",
    "            reversed_sequence: A reversed sequence of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (total_surprisal, list_of_prediction_errors)\n",
    "        \"\"\"\n",
    "        total_surprisal = 0.0\n",
    "        prediction_errors = []\n",
    "        \n",
    "        # Start with START node\n",
    "        current_pos = \"<START>\"\n",
    "        \n",
    "        # Process each position in the sequence\n",
    "        for pos in reversed_sequence:\n",
    "            # Calculate prediction probability for this position\n",
    "            prediction_prob = 0.0\n",
    "            if self.backward_graph.has_edge(current_pos, pos):\n",
    "                prediction_prob = self.backward_graph[current_pos][pos].get(\"weight\", 0.0)\n",
    "            \n",
    "            # Calculate surprisal (-log probability)\n",
    "            if prediction_prob > 0:\n",
    "                surprisal = -math.log2(prediction_prob)\n",
    "            else:\n",
    "                surprisal = 10.0  # High surprisal for unseen transitions\n",
    "                \n",
    "            # Get current precision for this transition\n",
    "            precision = self.base_precision\n",
    "            if self.backward_graph.has_edge(current_pos, pos):\n",
    "                precision = self.backward_graph[current_pos][pos].get(\"precision\", self.base_precision)\n",
    "            \n",
    "            # Calculate precision-weighted prediction error\n",
    "            prediction_error = surprisal * precision\n",
    "            \n",
    "            prediction_errors.append(prediction_error)\n",
    "            total_surprisal += surprisal\n",
    "            \n",
    "            # Update current position\n",
    "            current_pos = pos\n",
    "        \n",
    "        return total_surprisal, prediction_errors\n",
    "\n",
    "    def _update_attention_weights(self, sequence: List[str], forward_errors: List[float], \n",
    "                                 reversed_sequence: List[str], backward_errors: List[float]):\n",
    "        \"\"\"\n",
    "        Update attention weights based on prediction errors from both directions.\n",
    "        \n",
    "        Args:\n",
    "            sequence: The forward POS sequence\n",
    "            forward_errors: Prediction errors for forward transitions\n",
    "            reversed_sequence: The reversed POS sequence\n",
    "            backward_errors: Prediction errors for backward transitions\n",
    "        \"\"\"\n",
    "        # Combine forward and backward errors for each position\n",
    "        combined_errors = {}\n",
    "        \n",
    "        # Process forward errors\n",
    "        if forward_errors:\n",
    "            max_forward = max(forward_errors)\n",
    "            if max_forward > 0:\n",
    "                for i, pos in enumerate(sequence):\n",
    "                    if i < len(forward_errors):\n",
    "                        error = forward_errors[i] / max_forward\n",
    "                        combined_errors[pos] = combined_errors.get(pos, 0) + error * self.forward_weight\n",
    "        \n",
    "        # Process backward errors (need to re-reverse to align with original positions)\n",
    "        if backward_errors:\n",
    "            max_backward = max(backward_errors)\n",
    "            if max_backward > 0:\n",
    "                backward_positions = list(reversed(reversed_sequence))  # Re-reverse to match original\n",
    "                for i, pos in enumerate(backward_positions):\n",
    "                    if i < len(backward_errors):\n",
    "                        error_idx = len(backward_errors) - i - 1  # Map to correct backward error\n",
    "                        if error_idx >= 0 and error_idx < len(backward_errors):\n",
    "                            error = backward_errors[error_idx] / max_backward\n",
    "                            combined_errors[pos] = combined_errors.get(pos, 0) + error * self.backward_weight\n",
    "        \n",
    "        # Update attention weights based on combined errors\n",
    "        for pos, error in combined_errors.items():\n",
    "            current_attention = self.attention_weights.get(pos, 1.0)\n",
    "            # Update with learning rate\n",
    "            self.attention_weights[pos] = (1 - self.attention_learning_rate) * current_attention + \\\n",
    "                                      self.attention_learning_rate * error\n",
    "            \n",
    "            # Ensure attention weights stay in reasonable range\n",
    "            self.attention_weights[pos] = max(0.2, min(3.0, self.attention_weights[pos]))\n",
    "        \n",
    "        # Update special nodes\n",
    "        if \"<START>\" not in combined_errors:\n",
    "            # Keep START node's attention stable\n",
    "            self.attention_weights[\"<START>\"] = self.attention_weights.get(\"<START>\", 1.0)\n",
    "        \n",
    "        if \"<END>\" not in combined_errors:\n",
    "            # Keep END node's attention stable\n",
    "            self.attention_weights[\"<END>\"] = self.attention_weights.get(\"<END>\", 1.0)\n",
    "\n",
    "    def _update_graph_weights(self, sequence: List[str], forward_errors: List[float],\n",
    "                            reversed_sequence: List[str], backward_errors: List[float]):\n",
    "        \"\"\"\n",
    "        Update graph edge weights based on attention-modulated learning in both directions.\n",
    "        \n",
    "        Args:\n",
    "            sequence: The forward POS sequence\n",
    "            forward_errors: Prediction errors for forward transitions\n",
    "            reversed_sequence: The reversed POS sequence\n",
    "            backward_errors: Prediction errors for backward transitions\n",
    "        \"\"\"\n",
    "        # Update forward graph\n",
    "        current_pos = \"<START>\"\n",
    "        for i, pos in enumerate(sequence):\n",
    "            # Get attention weights for current and next position\n",
    "            current_attention = self.attention_weights.get(current_pos, 1.0)\n",
    "            target_attention = self.attention_weights.get(pos, 1.0)\n",
    "            \n",
    "            # Combined attention effect (geometric mean)\n",
    "            combined_attention = math.sqrt(current_attention * target_attention)\n",
    "            \n",
    "            # Attention-modulated learning rate\n",
    "            effective_lr = self.learning_rate * combined_attention\n",
    "            \n",
    "            # Update precision for this transition based on prediction error\n",
    "            if i < len(forward_errors):\n",
    "                error = forward_errors[i]\n",
    "                new_precision = self.base_precision * (1 + 0.1 * error)\n",
    "                new_precision = max(self.min_precision, min(self.max_precision, new_precision))\n",
    "                \n",
    "                if self.forward_graph.has_edge(current_pos, pos):\n",
    "                    # Slowly update precision for this edge\n",
    "                    old_precision = self.forward_graph[current_pos][pos].get(\"precision\", self.base_precision)\n",
    "                    updated_precision = 0.9 * old_precision + 0.1 * new_precision\n",
    "                    self.forward_graph[current_pos][pos][\"precision\"] = updated_precision\n",
    "            \n",
    "            # Update current position\n",
    "            current_pos = pos\n",
    "        \n",
    "        # Update backward graph\n",
    "        current_pos = \"<START>\"\n",
    "        for i, pos in enumerate(reversed_sequence):\n",
    "            # Get attention weights\n",
    "            current_attention = self.attention_weights.get(current_pos, 1.0)\n",
    "            target_attention = self.attention_weights.get(pos, 1.0)\n",
    "            \n",
    "            # Combined attention effect (geometric mean)\n",
    "            combined_attention = math.sqrt(current_attention * target_attention)\n",
    "            \n",
    "            # Attention-modulated learning rate\n",
    "            effective_lr = self.learning_rate * combined_attention\n",
    "            \n",
    "            # Update precision for this transition based on backward prediction error\n",
    "            if i < len(backward_errors):\n",
    "                error = backward_errors[i]\n",
    "                new_precision = self.base_precision * (1 + 0.1 * error)\n",
    "                new_precision = max(self.min_precision, min(self.max_precision, new_precision))\n",
    "                \n",
    "                if self.backward_graph.has_edge(current_pos, pos):\n",
    "                    # Slowly update precision for this edge\n",
    "                    old_precision = self.backward_graph[current_pos][pos].get(\"precision\", self.base_precision)\n",
    "                    updated_precision = 0.9 * old_precision + 0.1 * new_precision\n",
    "                    self.backward_graph[current_pos][pos][\"precision\"] = updated_precision\n",
    "            \n",
    "            # Update current position\n",
    "            current_pos = pos\n",
    "\n",
    "    def _calculate_edge_weights(self):\n",
    "        \"\"\"Calculate edge weights (transition probabilities) based on counts for both graphs.\"\"\"\n",
    "        # Forward graph\n",
    "        for node in self.forward_graph.nodes():\n",
    "            if node == \"<END>\":\n",
    "                continue  # End node has no outgoing edges\n",
    "                \n",
    "            # Get total count of outgoing transitions\n",
    "            outgoing_edges = list(self.forward_graph.out_edges(node, data=True))\n",
    "            total_count = sum(data[\"count\"] for _, _, data in outgoing_edges)\n",
    "            \n",
    "            if total_count > 0:\n",
    "                # Calculate probability for each outgoing edge\n",
    "                for _, target, data in outgoing_edges:\n",
    "                    prob = data[\"count\"] / total_count\n",
    "                    self.forward_graph[node][target][\"weight\"] = prob\n",
    "        \n",
    "        # Backward graph\n",
    "        for node in self.backward_graph.nodes():\n",
    "            if node == \"<END>\":\n",
    "                continue  # End node has no outgoing edges\n",
    "                \n",
    "            # Get total count of outgoing transitions\n",
    "            outgoing_edges = list(self.backward_graph.out_edges(node, data=True))\n",
    "            total_count = sum(data[\"count\"] for _, _, data in outgoing_edges)\n",
    "            \n",
    "            if total_count > 0:\n",
    "                # Calculate probability for each outgoing edge\n",
    "                for _, target, data in outgoing_edges:\n",
    "                    prob = data[\"count\"] / total_count\n",
    "                    self.backward_graph[node][target][\"weight\"] = prob\n",
    "\n",
    "    def _calculate_boundary_probabilities(self):\n",
    "        \"\"\"Calculate boundary probabilities with attention influence for both directions.\"\"\"\n",
    "        # Forward direction\n",
    "        for source, target, data in self.forward_graph.edges(data=True):\n",
    "            if source in (\"<START>\", \"<END>\") or target in (\"<START>\", \"<END>\"):\n",
    "                continue  # Skip special nodes\n",
    "                \n",
    "            # Calculate surprisal for this transition\n",
    "            prob = data.get(\"weight\", 0)\n",
    "            if prob > 0:\n",
    "                surprisal = -math.log2(prob)\n",
    "                \n",
    "                # Get attention weights for source and target\n",
    "                source_attention = self.attention_weights.get(source, 1.0)\n",
    "                target_attention = self.attention_weights.get(target, 1.0)\n",
    "                \n",
    "                # Attention-modulated boundary probability\n",
    "                # Higher attention at either end means more salient boundary\n",
    "                attention_factor = (source_attention + target_attention) / 2\n",
    "                \n",
    "                # Normalize surprisal to a boundary probability between 0 and 1\n",
    "                raw_boundary_prob = 1 / (1 + math.exp(-(surprisal - 1)))\n",
    "                \n",
    "                # Adjust boundary probability based on attention\n",
    "                boundary_prob = raw_boundary_prob * attention_factor\n",
    "                \n",
    "                # Consider predefined boundaries if available\n",
    "                if (source, target) in self.predefined_boundaries:\n",
    "                    predefined_prob = self.predefined_boundaries[(source, target)]\n",
    "                    alpha = 0.3  # Weight for predefined rules\n",
    "                    boundary_prob = alpha * predefined_prob + (1 - alpha) * boundary_prob\n",
    "                \n",
    "                # Store in graph and in lookup dictionary\n",
    "                boundary_prob = max(0.0, min(1.0, boundary_prob))  # Ensure [0,1] range\n",
    "                self.forward_graph[source][target][\"boundary_prob\"] = boundary_prob\n",
    "                self.forward_boundary_probs[(source, target)] = boundary_prob\n",
    "        \n",
    "        # Backward direction\n",
    "        for source, target, data in self.backward_graph.edges(data=True):\n",
    "            if source in (\"<START>\", \"<END>\") or target in (\"<START>\", \"<END>\"):\n",
    "                continue  # Skip special nodes\n",
    "                \n",
    "            # Calculate surprisal for this transition\n",
    "            prob = data.get(\"weight\", 0)\n",
    "            if prob > 0:\n",
    "                surprisal = -math.log2(prob)\n",
    "                \n",
    "                # Get attention weights for source and target\n",
    "                source_attention = self.attention_weights.get(source, 1.0)\n",
    "                target_attention = self.attention_weights.get(target, 1.0)\n",
    "                \n",
    "                # Attention-modulated boundary probability\n",
    "                attention_factor = (source_attention + target_attention) / 2\n",
    "                \n",
    "                # Normalize surprisal to a boundary probability between 0 and 1\n",
    "                raw_boundary_prob = 1 / (1 + math.exp(-(surprisal - 1)))\n",
    "                \n",
    "                # Adjust boundary probability based on attention\n",
    "                boundary_prob = raw_boundary_prob * attention_factor\n",
    "                \n",
    "                # Handle predefined boundaries - note reversed order for backward graph\n",
    "                if (target, source) in self.predefined_boundaries:  # Reversed for backward direction\n",
    "                    predefined_prob = self.predefined_boundaries[(target, source)]\n",
    "                    alpha = 0.3  # Weight for predefined rules\n",
    "                    boundary_prob = alpha * predefined_prob + (1 - alpha) * boundary_prob\n",
    "                \n",
    "                # Store in graph and in lookup dictionary\n",
    "                boundary_prob = max(0.0, min(1.0, boundary_prob))  # Ensure [0,1] range\n",
    "                self.backward_graph[source][target][\"boundary_prob\"] = boundary_prob\n",
    "                self.backward_boundary_probs[(source, target)] = boundary_prob\n",
    "\n",
    "    def _combine_boundary_probabilities(self):\n",
    "        \"\"\"Combine forward and backward boundary probabilities.\"\"\"\n",
    "        # First combine all the transitions found in either direction\n",
    "        all_transitions = set(self.forward_boundary_probs.keys()) | set(self.backward_boundary_probs.keys())\n",
    "        \n",
    "        # For each transition, create a combined boundary probability\n",
    "        for source, target in all_transitions:\n",
    "            # Get forward boundary probability\n",
    "            forward_prob = self.forward_boundary_probs.get((source, target), 0.0)\n",
    "            \n",
    "            # Get backward boundary probability (note: need to reverse direction)\n",
    "            backward_prob = self.backward_boundary_probs.get((target, source), 0.0)\n",
    "            \n",
    "            # Weighted combination (can adjust weights as needed)\n",
    "            combined_prob = self.forward_weight * forward_prob + self.backward_weight * backward_prob\n",
    "            \n",
    "            # Store the combined probability\n",
    "            self.combined_boundary_probs[(source, target)] = combined_prob\n",
    "\n",
    "    def _identify_common_chunks(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Identify common chunks using attention-weighted statistics with bidirectional information.\"\"\"\n",
    "        # Use a sliding window approach to find potential chunks\n",
    "        chunk_candidates = Counter()\n",
    "        \n",
    "        # Try different chunk sizes\n",
    "        for size in range(2, 5):  # 2-grams to 4-grams\n",
    "            for sequence in pos_sequences:\n",
    "                if len(sequence) < size:\n",
    "                    continue\n",
    "                    \n",
    "                for i in range(len(sequence) - size + 1):\n",
    "                    chunk = tuple(sequence[i:i+size])\n",
    "                    chunk_candidates[chunk] += 1\n",
    "        \n",
    "        print(f\"Found {len(chunk_candidates)} potential chunks\")\n",
    "        \n",
    "        # For small training sets, lower the threshold\n",
    "        total_sentences = len(pos_sequences)\n",
    "        min_occurrences = max(2, int(total_sentences * 0.05))\n",
    "        \n",
    "        # Lower the cohesion threshold for small datasets\n",
    "        cohesion_threshold = 0.6 if total_sentences < 20 else 0.7\n",
    "        \n",
    "        # Count qualifying chunks\n",
    "        qualifying_chunks = 0\n",
    "        for chunk, count in chunk_candidates.items():\n",
    "            if count >= min_occurrences:\n",
    "                qualifying_chunks += 1\n",
    "                \n",
    "                # Calculate internal cohesion with attention weighting\n",
    "                # Using combined boundary probabilities (bidirectional)\n",
    "                internal_boundaries = 0\n",
    "                chunk_attention = 1.0  # Start with neutral attention\n",
    "                \n",
    "                for i in range(len(chunk) - 1):\n",
    "                    pos1, pos2 = chunk[i], chunk[i+1]\n",
    "                    \n",
    "                    # Get combined boundary probability\n",
    "                    boundary_prob = self.combined_boundary_probs.get((pos1, pos2), 0.5)\n",
    "                    \n",
    "                    # Apply attention weighting - average of the two positions\n",
    "                    pos1_attention = self.attention_weights.get(pos1, 1.0)\n",
    "                    pos2_attention = self.attention_weights.get(pos2, 1.0)\n",
    "                    avg_attention = (pos1_attention + pos2_attention) / 2\n",
    "                    \n",
    "                    # Accumulate attention-weighted boundary probability\n",
    "                    internal_boundaries += boundary_prob\n",
    "                    \n",
    "                    # Calculate overall chunk attention (product of position attentions)\n",
    "                    chunk_attention *= (pos1_attention * 0.5 + 0.5)  # Dampen the effect\n",
    "                \n",
    "                avg_internal_boundary = internal_boundaries / (len(chunk) - 1)\n",
    "                cohesion = 1 - avg_internal_boundary\n",
    "                \n",
    "                # Boost cohesion for chunks with high attention\n",
    "                attention_adjusted_cohesion = cohesion * (0.8 + 0.2 * chunk_attention)\n",
    "                \n",
    "                # Only keep reasonably cohesive chunks\n",
    "                if attention_adjusted_cohesion > cohesion_threshold:\n",
    "                    chunk_name = f\"{'_'.join(chunk)}\"\n",
    "                    self.common_chunks[chunk] = {\n",
    "                        \"name\": chunk_name,\n",
    "                        \"elements\": chunk,\n",
    "                        \"count\": count,\n",
    "                        \"cohesion\": attention_adjusted_cohesion,\n",
    "                        \"attention\": chunk_attention,\n",
    "                        \"activation\": 0.0  # Initial activation level\n",
    "                    }\n",
    "                    \n",
    "                    # Initialize chunk attention weight\n",
    "                    self.chunk_attention_weights[chunk] = chunk_attention\n",
    "                    \n",
    "        print(f\"{qualifying_chunks} chunks met frequency criteria, {len(self.common_chunks)} met cohesion criteria\")\n",
    "\n",
    "    def _build_chunk_graph(self):\n",
    "        \"\"\"Build higher-order graph representing transitions between chunks.\"\"\"\n",
    "        # Add nodes for each chunk\n",
    "        for chunk_tuple, chunk_info in self.common_chunks.items():\n",
    "            chunk_name = chunk_info[\"name\"]\n",
    "            self.chunk_graph.add_node(\n",
    "                chunk_name, \n",
    "                pos_type=\"chunk\", \n",
    "                elements=chunk_info[\"elements\"],\n",
    "                cohesion=chunk_info[\"cohesion\"],\n",
    "                attention=chunk_info[\"attention\"]\n",
    "            )\n",
    "        \n",
    "        # Connect chunks that can follow each other\n",
    "        for chunk1_tuple, chunk1_info in self.common_chunks.items():\n",
    "            for chunk2_tuple, chunk2_info in self.common_chunks.items():\n",
    "                # Check if chunk2 can follow chunk1 (overlap or adjacency)\n",
    "                if self._can_follow(chunk1_tuple, chunk2_tuple):\n",
    "                    # Calculate transition probability\n",
    "                    # This is simplified - would need corpus analysis for accurate probabilities\n",
    "                    transition_prob = 0.1  # Default low probability\n",
    "                    \n",
    "                    # If we have trigram data, use it to estimate transition probability\n",
    "                    if len(chunk1_tuple) >= 2 and len(chunk2_tuple) >= 1:\n",
    "                        last1, last2 = chunk1_tuple[-2], chunk1_tuple[-1]\n",
    "                        first = chunk2_tuple[0]\n",
    "                        \n",
    "                        if last2 in self.trigram_counts.get(last1, {}):\n",
    "                            total = sum(self.trigram_counts[last1][last2].values())\n",
    "                            if total > 0:\n",
    "                                count = self.trigram_counts[last1][last2].get(first, 0)\n",
    "                                transition_prob = count / total\n",
    "                    \n",
    "                    # Apply attention weighting to transition\n",
    "                    chunk1_attention = self.chunk_attention_weights.get(chunk1_tuple, 1.0)\n",
    "                    chunk2_attention = self.chunk_attention_weights.get(chunk2_tuple, 1.0)\n",
    "                    \n",
    "                    # Higher attention on both chunks strengthens their connection\n",
    "                    attention_factor = (chunk1_attention + chunk2_attention) / 2\n",
    "                    weighted_prob = transition_prob * attention_factor\n",
    "                    \n",
    "                    # Add edge with weight\n",
    "                    chunk1_name = chunk1_info[\"name\"]\n",
    "                    chunk2_name = chunk2_info[\"name\"]\n",
    "                    self.chunk_graph.add_edge(\n",
    "                        chunk1_name, \n",
    "                        chunk2_name, \n",
    "                        weight=weighted_prob,\n",
    "                        attention=attention_factor\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.Pytorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
