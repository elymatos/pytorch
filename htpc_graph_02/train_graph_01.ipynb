{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2f64dd-618b-4184-a94c-15496e26c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 15 sentences\n",
      "Found 83 potential chunks\n",
      "27 chunks met frequency criteria, 6 met cohesion criteria\n",
      "Training complete on 15 sentences.\n",
      "POS graph has 10 nodes and 27 edges\n",
      "Chunk graph has 6 nodes and 19 edges\n",
      "\n",
      "Test sentence: ['DET', 'ADJ', 'NOUN', 'VERB', 'PREP', 'DET', 'NOUN']\n",
      "\n",
      "Recognized chunks:\n",
      "  ('ADJ', 'NOUN') (Position 1-3)\n",
      "  ('PREP', 'DET') (Position 4-6)\n",
      "\n",
      "Final segmentation: [['DET'], ['ADJ', 'NOUN'], ['VERB'], ['PREP', 'DET'], ['NOUN']]\n",
      "\n",
      "Top predictions after ['DET', 'ADJ']:\n",
      "  NOUN: 0.75\n",
      "  ADJ: 0.08\n",
      "  CONJ: 0.08\n",
      "POS graph visualization saved to pos_graph.png\n",
      "Chunk graph visualization saved to chunk_graph.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Set, Optional, Any\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class POSGraph:\n",
    "    \"\"\"\n",
    "    Implements a graph-based structure for POS sequence processing and predictive coding.\n",
    "    Each node represents a POS tag, and edges represent transitions with weights as probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, predefined_boundaries: Optional[Dict[Tuple[str, str], float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize the POS graph.\n",
    "        \n",
    "        Args:\n",
    "            predefined_boundaries: Optional dictionary of predefined boundary probabilities\n",
    "                                  for POS tag transitions (pos1, pos2) -> boundary_probability\n",
    "        \"\"\"\n",
    "        # Main transition graph\n",
    "        self.graph = nx.DiGraph()\n",
    "        \n",
    "        # Higher-order chunk graph for learned patterns\n",
    "        self.chunk_graph = nx.DiGraph()\n",
    "        \n",
    "        # Track n-gram counts for training\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.trigram_counts = defaultdict(lambda: defaultdict(Counter))\n",
    "        \n",
    "        # Boundary probabilities\n",
    "        self.boundary_probs = defaultdict(float)\n",
    "        \n",
    "        # Predefined linguistic rules\n",
    "        self.predefined_boundaries = predefined_boundaries or {\n",
    "            ('NOUN', 'VERB'): 0.9,      # NP to VP transition\n",
    "            ('VERB', 'DET'): 0.8,       # VP to NP transition\n",
    "            ('PUNCT', 'DET'): 0.95,     # Punctuation followed by determiner\n",
    "            ('NOUN', 'PREP'): 0.7,      # NP to PP transition\n",
    "            ('VERB', 'PREP'): 0.6,      # VP to PP transition\n",
    "            ('ADJ', 'NOUN'): 0.2,       # Within NP (low boundary probability)\n",
    "            ('DET', 'ADJ'): 0.1,        # Within NP (very low boundary probability)\n",
    "        }\n",
    "        \n",
    "        # Thresholds\n",
    "        self.hard_boundary_threshold = 0.75\n",
    "        self.soft_boundary_threshold = 0.4\n",
    "        \n",
    "        # Discovered chunks\n",
    "        self.common_chunks = {}\n",
    "        \n",
    "        # Add special start and end nodes\n",
    "        self.graph.add_node(\"<START>\", pos_type=\"special\")\n",
    "        self.graph.add_node(\"<END>\", pos_type=\"special\")\n",
    "\n",
    "    def train(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"\n",
    "        Train the POS graph on a corpus of POS tag sequences.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequences: List of POS tag sequences, each representing a sentence\n",
    "        \"\"\"\n",
    "        # 1. Collect n-gram statistics and build basic graph\n",
    "        self._build_initial_graph(pos_sequences)\n",
    "        \n",
    "        # 2. Calculate edge weights (transition probabilities)\n",
    "        self._calculate_edge_weights()\n",
    "        \n",
    "        # 3. Calculate boundary probabilities\n",
    "        self._calculate_boundary_probabilities()\n",
    "        \n",
    "        # 4. Identify common chunks\n",
    "        self._identify_common_chunks(pos_sequences)\n",
    "        \n",
    "        # 5. Build higher-order chunk graph\n",
    "        self._build_chunk_graph()\n",
    "        \n",
    "        print(f\"Training complete on {len(pos_sequences)} sentences.\")\n",
    "        print(f\"POS graph has {len(self.graph.nodes)} nodes and {len(self.graph.edges)} edges\")\n",
    "        print(f\"Chunk graph has {len(self.chunk_graph.nodes)} nodes and {len(self.chunk_graph.edges)} edges\")\n",
    "\n",
    "    def _build_initial_graph(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Build the initial graph structure and collect statistics.\"\"\"\n",
    "        # First pass - add all nodes and count statistics\n",
    "        for sequence in pos_sequences:\n",
    "            # Add nodes for each unique POS tag\n",
    "            for pos in sequence:\n",
    "                if not self.graph.has_node(pos):\n",
    "                    self.graph.add_node(pos, pos_type=\"basic\")\n",
    "                self.unigram_counts[pos] += 1\n",
    "            \n",
    "            # Count bigrams and add edges\n",
    "            for i in range(len(sequence) - 1):\n",
    "                pos1, pos2 = sequence[i], sequence[i+1]\n",
    "                self.bigram_counts[pos1][pos2] += 1\n",
    "                \n",
    "                # Ensure edge exists (weight will be calculated later)\n",
    "                if not self.graph.has_edge(pos1, pos2):\n",
    "                    self.graph.add_edge(pos1, pos2, weight=0, count=0, boundary_prob=0)\n",
    "                \n",
    "                # Increment edge count\n",
    "                self.graph[pos1][pos2][\"count\"] += 1\n",
    "            \n",
    "            # Add connections from start and to end\n",
    "            if sequence:\n",
    "                if not self.graph.has_edge(\"<START>\", sequence[0]):\n",
    "                    self.graph.add_edge(\"<START>\", sequence[0], weight=0, count=0, boundary_prob=0)\n",
    "                self.graph[\"<START>\"][sequence[0]][\"count\"] += 1\n",
    "                \n",
    "                if not self.graph.has_edge(sequence[-1], \"<END>\"):\n",
    "                    self.graph.add_edge(sequence[-1], \"<END>\", weight=0, count=0, boundary_prob=0)\n",
    "                self.graph[sequence[-1]][\"<END>\"][\"count\"] += 1\n",
    "            \n",
    "            # Count trigrams\n",
    "            for i in range(len(sequence) - 2):\n",
    "                pos1, pos2, pos3 = sequence[i], sequence[i+1], sequence[i+2]\n",
    "                self.trigram_counts[pos1][pos2][pos3] += 1\n",
    "\n",
    "    def _calculate_edge_weights(self):\n",
    "        \"\"\"Calculate edge weights (transition probabilities) based on counts.\"\"\"\n",
    "        # For each node, calculate outgoing transition probabilities\n",
    "        for node in self.graph.nodes():\n",
    "            if node == \"<END>\":\n",
    "                continue  # End node has no outgoing edges\n",
    "                \n",
    "            # Get total count of outgoing transitions\n",
    "            outgoing_edges = list(self.graph.out_edges(node, data=True))\n",
    "            total_count = sum(data[\"count\"] for _, _, data in outgoing_edges)\n",
    "            \n",
    "            if total_count > 0:\n",
    "                # Calculate probability for each outgoing edge\n",
    "                for _, target, data in outgoing_edges:\n",
    "                    prob = data[\"count\"] / total_count\n",
    "                    self.graph[node][target][\"weight\"] = prob\n",
    "\n",
    "    def _calculate_boundary_probabilities(self):\n",
    "        \"\"\"Calculate boundary probabilities for each edge based on transition statistics.\"\"\"\n",
    "        for source, target, data in self.graph.edges(data=True):\n",
    "            if source in (\"<START>\", \"<END>\") or target in (\"<START>\", \"<END>\"):\n",
    "                continue  # Skip special nodes\n",
    "                \n",
    "            # Calculate surprisal for this transition\n",
    "            prob = data[\"weight\"]\n",
    "            if prob > 0:\n",
    "                surprisal = -math.log2(prob)\n",
    "                \n",
    "                # Normalize surprisal to a boundary probability between 0 and 1\n",
    "                # Higher surprisal = higher boundary probability\n",
    "                boundary_prob = 1 / (1 + math.exp(-(surprisal - 1)))\n",
    "                \n",
    "                # Consider predefined boundaries if available\n",
    "                if (source, target) in self.predefined_boundaries:\n",
    "                    predefined_prob = self.predefined_boundaries[(source, target)]\n",
    "                    alpha = 0.3  # Weight for predefined rules\n",
    "                    boundary_prob = alpha * predefined_prob + (1 - alpha) * boundary_prob\n",
    "                \n",
    "                # Store in graph and in lookup dictionary\n",
    "                self.graph[source][target][\"boundary_prob\"] = boundary_prob\n",
    "                self.boundary_probs[(source, target)] = boundary_prob\n",
    "\n",
    "    def _identify_common_chunks(self, pos_sequences: List[List[str]]):\n",
    "        \"\"\"Identify common chunks based on frequency and boundary probabilities.\"\"\"\n",
    "        # Use a sliding window approach to find potential chunks\n",
    "        chunk_candidates = Counter()\n",
    "        \n",
    "        # Try different chunk sizes\n",
    "        for size in range(2, 5):  # 2-grams to 4-grams\n",
    "            for sequence in pos_sequences:\n",
    "                if len(sequence) < size:\n",
    "                    continue\n",
    "                    \n",
    "                for i in range(len(sequence) - size + 1):\n",
    "                    chunk = tuple(sequence[i:i+size])\n",
    "                    chunk_candidates[chunk] += 1\n",
    "        \n",
    "        print(f\"Found {len(chunk_candidates)} potential chunks\")\n",
    "        \n",
    "        # For small training sets, lower the threshold to ensure we find some chunks\n",
    "        total_sentences = len(pos_sequences)\n",
    "        min_occurrences = max(2, int(total_sentences * 0.05))  # At least 2 occurrences or 5% of sentences\n",
    "        \n",
    "        # Lower the cohesion threshold for small datasets\n",
    "        cohesion_threshold = 0.6 if total_sentences < 20 else 0.7\n",
    "        \n",
    "        # Count the chunks that meet our criteria\n",
    "        qualifying_chunks = 0\n",
    "        for chunk, count in chunk_candidates.items():\n",
    "            if count >= min_occurrences:\n",
    "                qualifying_chunks += 1\n",
    "                # Calculate internal cohesion (inverse of average boundary probability)\n",
    "                internal_boundaries = 0\n",
    "                for i in range(len(chunk) - 1):\n",
    "                    pos1, pos2 = chunk[i], chunk[i+1]\n",
    "                    if (pos1, pos2) in self.boundary_probs:\n",
    "                        internal_boundaries += self.boundary_probs[(pos1, pos2)]\n",
    "                    else:\n",
    "                        internal_boundaries += 0.5  # Default if unseen\n",
    "                \n",
    "                avg_internal_boundary = internal_boundaries / (len(chunk) - 1)\n",
    "                cohesion = 1 - avg_internal_boundary\n",
    "                \n",
    "                # Only keep reasonably cohesive chunks\n",
    "                if cohesion > cohesion_threshold:\n",
    "                    chunk_name = f\"{'_'.join(chunk)}\"\n",
    "                    self.common_chunks[chunk] = {\n",
    "                        \"name\": chunk_name,\n",
    "                        \"elements\": chunk,\n",
    "                        \"count\": count,\n",
    "                        \"cohesion\": cohesion,\n",
    "                        \"activation\": 0.0  # Initial activation level\n",
    "                    }\n",
    "                    \n",
    "        print(f\"{qualifying_chunks} chunks met frequency criteria, {len(self.common_chunks)} met cohesion criteria\")\n",
    "\n",
    "    def _build_chunk_graph(self):\n",
    "        \"\"\"Build higher-order graph representing transitions between chunks.\"\"\"\n",
    "        # Add nodes for each chunk\n",
    "        for chunk_tuple, chunk_info in self.common_chunks.items():\n",
    "            chunk_name = chunk_info[\"name\"]\n",
    "            self.chunk_graph.add_node(\n",
    "                chunk_name, \n",
    "                pos_type=\"chunk\", \n",
    "                elements=chunk_info[\"elements\"],\n",
    "                cohesion=chunk_info[\"cohesion\"]\n",
    "            )\n",
    "        \n",
    "        # Connect chunks that can follow each other\n",
    "        for chunk1_tuple, chunk1_info in self.common_chunks.items():\n",
    "            for chunk2_tuple, chunk2_info in self.common_chunks.items():\n",
    "                # Check if chunk2 can follow chunk1 (overlap or adjacency)\n",
    "                if self._can_follow(chunk1_tuple, chunk2_tuple):\n",
    "                    # Calculate transition probability\n",
    "                    # This is simplified - would need corpus analysis for accurate probabilities\n",
    "                    transition_prob = 0.1  # Default low probability\n",
    "                    \n",
    "                    # If we have trigram data, use it to estimate transition probability\n",
    "                    if len(chunk1_tuple) >= 2 and len(chunk2_tuple) >= 1:\n",
    "                        last1, last2 = chunk1_tuple[-2], chunk1_tuple[-1]\n",
    "                        first = chunk2_tuple[0]\n",
    "                        \n",
    "                        if last2 in self.trigram_counts.get(last1, {}):\n",
    "                            total = sum(self.trigram_counts[last1][last2].values())\n",
    "                            if total > 0:\n",
    "                                count = self.trigram_counts[last1][last2].get(first, 0)\n",
    "                                transition_prob = count / total\n",
    "                    \n",
    "                    # Add edge with weight\n",
    "                    chunk1_name = chunk1_info[\"name\"]\n",
    "                    chunk2_name = chunk2_info[\"name\"]\n",
    "                    self.chunk_graph.add_edge(\n",
    "                        chunk1_name, \n",
    "                        chunk2_name, \n",
    "                        weight=transition_prob\n",
    "                    )\n",
    "\n",
    "    def _can_follow(self, chunk1: Tuple[str, ...], chunk2: Tuple[str, ...]) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if chunk2 can follow chunk1 in a sequence.\n",
    "        Either through overlap or adjacency.\n",
    "        \"\"\"\n",
    "        # Check if there's an overlap\n",
    "        for overlap_size in range(1, min(len(chunk1), len(chunk2))):\n",
    "            if chunk1[-overlap_size:] == chunk2[:overlap_size]:\n",
    "                return True\n",
    "        \n",
    "        # Check if there's an edge from the last element of chunk1 to the first of chunk2\n",
    "        last_of_chunk1 = chunk1[-1]\n",
    "        first_of_chunk2 = chunk2[0]\n",
    "        \n",
    "        return self.graph.has_edge(last_of_chunk1, first_of_chunk2)\n",
    "\n",
    "    def segment(self, pos_sequence: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Segment a POS sequence into chunks based on boundary probabilities.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: List of POS tags for a sentence\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks, where each chunk is a list of POS tags\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = [pos_sequence[0]]\n",
    "        \n",
    "        for i in range(1, len(pos_sequence)):\n",
    "            pos1, pos2 = pos_sequence[i-1], pos_sequence[i]\n",
    "            \n",
    "            # Get boundary probability\n",
    "            boundary_prob = self.boundary_probs.get((pos1, pos2), 0.2)  # Default if unseen\n",
    "            \n",
    "            if boundary_prob > self.hard_boundary_threshold:\n",
    "                # Hard boundary - create a new chunk\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = [pos2]\n",
    "            else:\n",
    "                # Continue current chunk\n",
    "                current_chunk.append(pos2)\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def predict_next_pos(self, context: List[str], top_n: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Predict the next POS tag given a context sequence.\n",
    "        \n",
    "        Args:\n",
    "            context: List of preceding POS tags\n",
    "            top_n: Number of top predictions to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (pos_tag, probability) pairs, sorted by probability\n",
    "        \"\"\"\n",
    "        if not context:\n",
    "            # No context, use connections from start node\n",
    "            predictions = []\n",
    "            for target, data in self.graph.out_edges(\"<START>\", data=True):\n",
    "                predictions.append((target, data[\"weight\"]))\n",
    "            return sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        \n",
    "        # Use the last tag for prediction\n",
    "        last_pos = context[-1]\n",
    "        \n",
    "        if self.graph.has_node(last_pos):\n",
    "            # Get all outgoing edges\n",
    "            predictions = []\n",
    "            for _, target, data in self.graph.out_edges(last_pos, data=True):\n",
    "                if target != \"<END>\":  # Skip end node in predictions\n",
    "                    predictions.append((target, data[\"weight\"]))\n",
    "            \n",
    "            return sorted(predictions, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        else:\n",
    "            # Unseen POS tag\n",
    "            return [(\"<UNK>\", 1.0)]  # Return unknown with full probability\n",
    "\n",
    "    def recognize_chunks(self, pos_sequence: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Recognize known chunks in a POS sequence.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: List of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            List of recognized chunks with their properties\n",
    "        \"\"\"\n",
    "        recognized = []\n",
    "        \n",
    "        # Try to match chunks of different sizes\n",
    "        for i in range(len(pos_sequence)):\n",
    "            for size in range(4, 1, -1):  # Try larger chunks first (4, 3, 2)\n",
    "                if i + size <= len(pos_sequence):\n",
    "                    chunk_tuple = tuple(pos_sequence[i:i+size])\n",
    "                    if chunk_tuple in self.common_chunks:\n",
    "                        recognized.append({\n",
    "                            \"chunk\": self.common_chunks[chunk_tuple],\n",
    "                            \"start\": i,\n",
    "                            \"end\": i + size,\n",
    "                            \"activation\": self.common_chunks[chunk_tuple][\"cohesion\"]\n",
    "                        })\n",
    "        \n",
    "        # Sort by start position\n",
    "        recognized.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        return recognized\n",
    "\n",
    "    def predictive_processing(self, pos_sequence: List[str]) -> Tuple[List[Dict[str, Any]], List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Process a sequence using predictive coding principles.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: List of POS tags\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (recognized chunks, segmented sequence)\n",
    "        \"\"\"\n",
    "        # First pass: recognize chunks\n",
    "        recognized_chunks = self.recognize_chunks(pos_sequence)\n",
    "        \n",
    "        # Second pass: resolve overlaps and calculate activation\n",
    "        non_overlapping = self._resolve_chunk_overlaps(recognized_chunks, len(pos_sequence))\n",
    "        \n",
    "        # Third pass: final segmentation based on chunks and boundaries\n",
    "        segmentation = self._create_final_segmentation(pos_sequence, non_overlapping)\n",
    "        \n",
    "        return non_overlapping, segmentation\n",
    "\n",
    "    def _resolve_chunk_overlaps(self, chunks: List[Dict[str, Any]], seq_length: int) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Resolve overlapping chunks by selecting the most activated ones.\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of recognized chunks\n",
    "            seq_length: Length of the original sequence\n",
    "            \n",
    "        Returns:\n",
    "            List of non-overlapping chunks\n",
    "        \"\"\"\n",
    "        # If no chunks, return empty list\n",
    "        if not chunks:\n",
    "            return []\n",
    "            \n",
    "        # Sort by activation (cohesion) to prioritize strongest chunks\n",
    "        sorted_chunks = sorted(chunks, key=lambda x: x[\"activation\"], reverse=True)\n",
    "        \n",
    "        # Track which positions are covered\n",
    "        covered = [False] * seq_length\n",
    "        \n",
    "        # Select non-overlapping chunks\n",
    "        selected = []\n",
    "        \n",
    "        for chunk in sorted_chunks:\n",
    "            start, end = chunk[\"start\"], chunk[\"end\"]\n",
    "            \n",
    "            # Check if this chunk overlaps with already selected ones\n",
    "            overlap = False\n",
    "            for i in range(start, end):\n",
    "                if covered[i]:\n",
    "                    overlap = True\n",
    "                    break\n",
    "            \n",
    "            if not overlap:\n",
    "                # Add chunk and mark positions as covered\n",
    "                selected.append(chunk)\n",
    "                for i in range(start, end):\n",
    "                    covered[i] = True\n",
    "        \n",
    "        # Sort by start position\n",
    "        selected.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        return selected\n",
    "\n",
    "    def _create_final_segmentation(self, pos_sequence: List[str], chunks: List[Dict[str, Any]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Create final segmentation based on recognized chunks and boundary probabilities.\n",
    "        \n",
    "        Args:\n",
    "            pos_sequence: Original POS sequence\n",
    "            chunks: Non-overlapping chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of segments (chunks)\n",
    "        \"\"\"\n",
    "        # If no chunks recognized, fall back to boundary-based segmentation\n",
    "        if not chunks:\n",
    "            return self.segment(pos_sequence)\n",
    "        \n",
    "        # Create segmentation based on recognized chunks and boundaries\n",
    "        segmentation = []\n",
    "        current_pos = 0\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            start, end = chunk[\"start\"], chunk[\"end\"]\n",
    "            \n",
    "            # If there's a gap before this chunk, segment it using boundaries\n",
    "            if start > current_pos:\n",
    "                gap_sequence = pos_sequence[current_pos:start]\n",
    "                gap_segments = self.segment(gap_sequence)\n",
    "                segmentation.extend(gap_segments)\n",
    "            \n",
    "            # Add the recognized chunk\n",
    "            segmentation.append(pos_sequence[start:end])\n",
    "            current_pos = end\n",
    "        \n",
    "        # Handle any remaining sequence after the last chunk\n",
    "        if current_pos < len(pos_sequence):\n",
    "            remaining = pos_sequence[current_pos:]\n",
    "            remaining_segments = self.segment(remaining)\n",
    "            segmentation.extend(remaining_segments)\n",
    "        \n",
    "        return segmentation\n",
    "\n",
    "    def visualize_pos_graph(self, filename: str = \"pos_graph.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the POS transition graph.\n",
    "        \n",
    "        Args:\n",
    "            filename: Output file name\n",
    "        \"\"\"\n",
    "        # Check if graph is empty\n",
    "        if len(self.graph) <= 2:  # Only START and END nodes\n",
    "            print(\"POS graph is empty or contains only special nodes - no visualization created\")\n",
    "            return\n",
    "            \n",
    "        # Create a copy without special nodes for cleaner visualization\n",
    "        g = self.graph.copy()\n",
    "        \n",
    "        # Only remove special nodes if they exist\n",
    "        if \"<START>\" in g:\n",
    "            g.remove_node(\"<START>\")\n",
    "        if \"<END>\" in g:\n",
    "            g.remove_node(\"<END>\")\n",
    "            \n",
    "        if len(g.edges()) == 0:\n",
    "            print(\"POS graph has no edges - no visualization created\")\n",
    "            return\n",
    "        \n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Define node positions using spring layout\n",
    "        pos = nx.spring_layout(g, seed=42)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(g, pos, node_size=500, node_color=\"lightblue\")\n",
    "        \n",
    "        # Prepare edge attributes\n",
    "        edge_width = []\n",
    "        edge_color = []\n",
    "        \n",
    "        for _, _, data in g.edges(data=True):\n",
    "            # Default to 0.1 if weight is missing or zero\n",
    "            weight = data.get(\"weight\", 0.1)\n",
    "            if weight == 0:\n",
    "                weight = 0.1\n",
    "            edge_width.append(weight * 5)\n",
    "            \n",
    "            # Default to 0.5 if boundary_prob is missing\n",
    "            edge_color.append(data.get(\"boundary_prob\", 0.5))\n",
    "        \n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(\n",
    "            g, pos, width=edge_width, \n",
    "            edge_color=edge_color, edge_cmap=plt.cm.Reds,\n",
    "            connectionstyle=\"arc3,rad=0.1\"\n",
    "        )\n",
    "        \n",
    "        # Add labels\n",
    "        nx.draw_networkx_labels(g, pos, font_size=10)\n",
    "        \n",
    "        # Edge labels (probabilities)\n",
    "        edge_labels = {}\n",
    "        for u, v, d in g.edges(data=True):\n",
    "            if \"weight\" in d:\n",
    "                edge_labels[(u, v)] = f\"{d['weight']:.2f}\"\n",
    "            else:\n",
    "                edge_labels[(u, v)] = \"0.00\"\n",
    "                \n",
    "        nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels, font_size=8)\n",
    "        \n",
    "        # Add a color bar for boundary probabilities\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds)\n",
    "        sm.set_array([])\n",
    "        fig.colorbar(sm, ax=ax, label=\"Boundary Probability\")\n",
    "        \n",
    "        plt.title(\"POS Transition Graph\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        print(f\"POS graph visualization saved to {filename}\")\n",
    "        plt.close()\n",
    "\n",
    "    def visualize_chunk_graph(self, filename: str = \"chunk_graph.png\"):\n",
    "        \"\"\"\n",
    "        Visualize the chunk transition graph.\n",
    "        \n",
    "        Args:\n",
    "            filename: Output file name\n",
    "        \"\"\"\n",
    "        if len(self.chunk_graph) == 0:\n",
    "            print(\"Chunk graph is empty - no visualization created\")\n",
    "            return\n",
    "            \n",
    "        if len(self.chunk_graph.edges()) == 0:\n",
    "            print(\"Chunk graph has no edges - adding artificial edges for visualization\")\n",
    "            # Create some artificial edges just for visualization\n",
    "            nodes = list(self.chunk_graph.nodes())\n",
    "            if len(nodes) > 1:\n",
    "                for i in range(len(nodes)-1):\n",
    "                    self.chunk_graph.add_edge(nodes[i], nodes[i+1], weight=0.1)\n",
    "            \n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Define node positions using spring layout\n",
    "        pos = nx.spring_layout(self.chunk_graph, seed=42)\n",
    "        \n",
    "        # Draw nodes with size proportional to cohesion\n",
    "        node_sizes = []\n",
    "        for node in self.chunk_graph.nodes():\n",
    "            cohesion = self.chunk_graph.nodes[node].get(\"cohesion\", 0.5)\n",
    "            if cohesion <= 0:\n",
    "                cohesion = 0.5  # Ensure minimum size\n",
    "            node_sizes.append(cohesion * 800)\n",
    "        \n",
    "        nx.draw_networkx_nodes(\n",
    "            self.chunk_graph, pos, \n",
    "            node_size=node_sizes, \n",
    "            node_color=\"lightgreen\"\n",
    "        )\n",
    "        \n",
    "        # Draw edges with width proportional to weight\n",
    "        if len(self.chunk_graph.edges()) > 0:\n",
    "            edge_width = []\n",
    "            for _, _, data in self.chunk_graph.edges(data=True):\n",
    "                weight = data.get(\"weight\", 0.1)\n",
    "                if weight <= 0:\n",
    "                    weight = 0.1  # Ensure minimum width\n",
    "                edge_width.append(weight * 8)\n",
    "            \n",
    "            nx.draw_networkx_edges(\n",
    "                self.chunk_graph, pos, width=edge_width, \n",
    "                edge_color=\"gray\", alpha=0.6,\n",
    "                connectionstyle=\"arc3,rad=0.1\"\n",
    "            )\n",
    "        \n",
    "        # Add labels\n",
    "        nx.draw_networkx_labels(self.chunk_graph, pos, font_size=9)\n",
    "        \n",
    "        plt.title(\"Chunk Transition Graph\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Chunk graph visualization saved to {filename}\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample POS sequences for training - Create more examples with repeating patterns\n",
    "    # to increase likelihood of chunk detection\n",
    "    training_data = [\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"],\n",
    "        [\"PRON\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],\n",
    "        [\"DET\", \"NOUN\", \"VERB\", \"ADV\", \"ADJ\"],\n",
    "        [\"DET\", \"ADJ\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],\n",
    "        [\"PRON\", \"VERB\", \"DET\", \"NOUN\", \"CONJ\", \"VERB\", \"ADV\"],\n",
    "        # More examples with repeating patterns to help chunk detection\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],  # Repeat\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"],          # Repeat\n",
    "        [\"DET\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"ADJ\", \"NOUN\"],\n",
    "        [\"PRON\", \"VERB\", \"ADV\", \"CONJ\", \"VERB\", \"DET\", \"NOUN\"],\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"ADV\", \"PREP\", \"PRON\"],\n",
    "        [\"NOUN\", \"VERB\", \"DET\", \"ADJ\", \"NOUN\", \"PREP\", \"DET\", \"NOUN\"],\n",
    "        [\"DET\", \"NOUN\", \"VERB\", \"ADJ\", \"CONJ\", \"ADV\"],\n",
    "        # Even more repetition of common patterns\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"DET\", \"NOUN\"],          # Repeat\n",
    "        [\"PRON\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],                # Repeat\n",
    "        [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"],  # Repeat\n",
    "    ]\n",
    "    \n",
    "    print(f\"Training on {len(training_data)} sentences\")\n",
    "    \n",
    "    # Initialize and train the graph\n",
    "    pos_graph = POSGraph()\n",
    "    pos_graph.train(training_data)\n",
    "    \n",
    "    # Test on a new sentence\n",
    "    test_sentence = [\"DET\", \"ADJ\", \"NOUN\", \"VERB\", \"PREP\", \"DET\", \"NOUN\"]\n",
    "    \n",
    "    print(\"\\nTest sentence:\", test_sentence)\n",
    "    \n",
    "    # Recognition and segmentation\n",
    "    chunks, segments = pos_graph.predictive_processing(test_sentence)\n",
    "    \n",
    "    print(\"\\nRecognized chunks:\")\n",
    "    if chunks:\n",
    "        for chunk in chunks:\n",
    "            print(f\"  {chunk['chunk']['elements']} (Position {chunk['start']}-{chunk['end']})\")\n",
    "    else:\n",
    "        print(\"  No chunks recognized\")\n",
    "    \n",
    "    print(\"\\nFinal segmentation:\", segments)\n",
    "    \n",
    "    # Predictions\n",
    "    context = [\"DET\", \"ADJ\"]\n",
    "    predictions = pos_graph.predict_next_pos(context)\n",
    "    print(f\"\\nTop predictions after {context}:\")\n",
    "    for pos, prob in predictions:\n",
    "        print(f\"  {pos}: {prob:.2f}\")\n",
    "    \n",
    "    # Visualize graphs\n",
    "    pos_graph.visualize_pos_graph()\n",
    "    pos_graph.visualize_chunk_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5fc8a-717a-4cdc-9693-eb2f51f62a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FN4.Pytorch",
   "language": "python",
   "name": "fn4.pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
